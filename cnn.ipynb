{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\video-gamer\\projects\\segmentation_cnn\\venv\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\video-gamer\\projects\\segmentation_cnn\\venv\\lib\\site-packages (from opencv-python) (1.24.1)\n",
      "Requirement already satisfied: torch in c:\\users\\video-gamer\\projects\\segmentation_cnn\\venv\\lib\\site-packages (2.0.0+cu117)\n",
      "Requirement already satisfied: filelock in c:\\users\\video-gamer\\projects\\segmentation_cnn\\venv\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\video-gamer\\projects\\segmentation_cnn\\venv\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\video-gamer\\projects\\segmentation_cnn\\venv\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\video-gamer\\projects\\segmentation_cnn\\venv\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\video-gamer\\projects\\segmentation_cnn\\venv\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\video-gamer\\projects\\segmentation_cnn\\venv\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\video-gamer\\projects\\segmentation_cnn\\venv\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:05:31.333496Z",
     "end_time": "2023-04-16T22:05:33.714312Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchvision.models.segmentation\n",
    "import torch\n",
    "import torchvision.transforms as tf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:05:59.080842Z",
     "end_time": "2023-04-16T22:06:00.961943Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "Learning_Rate = 1e-5\n",
    "width = height = 800  # image width and height\n",
    "batchSize = 3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:06:02.717781Z",
     "end_time": "2023-04-16T22:06:02.733407Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "TrainFolder = \"LabPics/Simple/Train//\"\n",
    "ListImages = os.listdir(os.path.join(TrainFolder, \"Image\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:06:05.187772Z",
     "end_time": "2023-04-16T22:06:05.320216Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "transformImg = tf.Compose([tf.ToPILImage(), tf.Resize((height, width)), tf.ToTensor(),\n",
    "                           tf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "transformAnn = tf.Compose([tf.ToPILImage(), tf.Resize((height, width)), tf.ToTensor()])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:06:06.699744Z",
     "end_time": "2023-04-16T22:06:06.711782Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code snippet defines two transformation pipelines using PyTorch's `torchvision.transforms` module.\n",
    "\n",
    "The first transformation pipeline, transformImg, is used to preprocess images. It consists of four transformations:\n",
    "\n",
    "1. tf.ToPILImage(): This transformation converts the input image, which is in tensor format, to a PIL image.\n",
    "2. tf.Resize((height,width)): This transformation resizes the image to the specified height and width.\n",
    "3. tf.ToTensor(): This transformation converts the PIL image to a tensor.\n",
    "4. tf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)): This transformation normalizes the tensor by subtracting the mean (0.485, 0.456, 0.406) and dividing by the standard deviation (0.229, 0.224, 0.225) for each channel. This normalization is commonly used for pre-trained image classification models like VGG, ResNet, and MobileNet.\n",
    "\n",
    "The second transformation pipeline `transformAnn` is used to preprocess annotations. It consists of three transformations:\n",
    "\n",
    "1. tf.ToPILImage(): This transformation converts the input annotation, which is in tensor format, to a PIL image.\n",
    "2. tf.Resize((height,width)): This transformation resizes the annotation to the specified height and width.\n",
    "3. tf.ToTensor(): This transformation converts the PIL image to a tensor.\n",
    "\n",
    "Both pipelines can be applied to a batch of images and annotations using the `torch.utils.data.DataLoader` class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def ReadRandomImage():\n",
    "    idx = np.random.randint(0, len(ListImages))  # Pick random image\n",
    "    img = cv2.imread(os.path.join(TrainFolder, \"Image\", ListImages[idx]))\n",
    "\n",
    "    filled = cv2.imread(os.path.join(TrainFolder, \"Semantic/16_Filled\", ListImages[idx].replace(\"jpg\", \"png\")), 0)\n",
    "    vessel = cv2.imread(os.path.join(TrainFolder, \"Semantic/1_Vessel\", ListImages[idx].replace(\"jpg\", \"png\")), 0)\n",
    "\n",
    "    ann_map = np.zeros(img.shape[0:2], np.float32)  # Segmentation map\n",
    "\n",
    "    if vessel is not None:\n",
    "        ann_map[vessel == 1] = 1\n",
    "\n",
    "    if filled is not None:\n",
    "        ann_map[filled == 1] = 2\n",
    "\n",
    "    img = transformImg(img)\n",
    "    ann_map = transformAnn(ann_map)\n",
    "\n",
    "    return img, ann_map"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:06:11.952965Z",
     "end_time": "2023-04-16T22:06:11.964742Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def LoadBatch():  # Load batch of images\n",
    "    images = torch.zeros([batchSize, 3, height, width])\n",
    "    ann = torch.zeros([batchSize, height, width])\n",
    "\n",
    "    for i in range(batchSize):\n",
    "        images[i], ann[i] = ReadRandomImage()\n",
    "\n",
    "    return images, ann"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:06:14.153067Z",
     "end_time": "2023-04-16T22:06:14.160782Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Video-gamer\\Projects\\Segmentation_cnn\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Video-gamer\\Projects\\Segmentation_cnn\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "Net = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)  # Load net\n",
    "Net.classifier[4] = torch.nn.Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))  # Change final layer to 3 classes\n",
    "Net = Net.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=Net.parameters(), lr=Learning_Rate)  # Create adam optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T22:06:18.756089Z",
     "end_time": "2023-04-16T22:06:20.248315Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [   0/10000] ..........0 ) Loss= 1.1146412 - Saving Model0.torch\n",
      "Training [   1/10000] ..........1 ) Loss= 1.0547378\n",
      "Training [   2/10000] ..........2 ) Loss= 1.1281569\n",
      "Training [   3/10000] ..........3 ) Loss= 1.1044791\n",
      "Training [   4/10000] ..........4 ) Loss= 1.0876601\n",
      "Training [   5/10000] ..........5 ) Loss= 1.03734\n",
      "Training [   6/10000] ..........6 ) Loss= 1.0564982\n",
      "Training [   7/10000] ..........7 ) Loss= 1.0587741\n",
      "Training [   8/10000] ..........8 ) Loss= 1.1082567\n",
      "Training [   9/10000] ..........9 ) Loss= 1.0310553\n",
      "Training [  10/10000] ..........10 ) Loss= 1.0465515\n",
      "Training [  11/10000] ..........11 ) Loss= 1.0407495\n",
      "Training [  12/10000] ..........12 ) Loss= 1.0501089\n",
      "Training [  13/10000] ..........13 ) Loss= 1.0043029\n",
      "Training [  14/10000] ..........14 ) Loss= 1.0105424\n",
      "Training [  15/10000] ..........15 ) Loss= 1.0253922\n",
      "Training [  16/10000] ..........16 ) Loss= 1.0034077\n",
      "Training [  17/10000] ..........17 ) Loss= 1.0082889\n",
      "Training [  18/10000] ..........18 ) Loss= 1.0588678\n",
      "Training [  19/10000] ..........19 ) Loss= 0.94108593\n",
      "Training [  20/10000] ..........20 ) Loss= 1.0298012\n",
      "Training [  21/10000] ..........21 ) Loss= 0.9894092\n",
      "Training [  22/10000] ..........22 ) Loss= 0.99390876\n",
      "Training [  23/10000] ..........23 ) Loss= 0.9716176\n",
      "Training [  24/10000] ..........24 ) Loss= 1.0350516\n",
      "Training [  25/10000] ..........25 ) Loss= 1.0072323\n",
      "Training [  26/10000] ..........26 ) Loss= 0.974799\n",
      "Training [  27/10000] ..........27 ) Loss= 1.0294592\n",
      "Training [  28/10000] ..........28 ) Loss= 0.9496068\n",
      "Training [  29/10000] ..........29 ) Loss= 0.9505623\n",
      "Training [  30/10000] ..........30 ) Loss= 1.0033917\n",
      "Training [  31/10000] ..........31 ) Loss= 0.9466818\n",
      "Training [  32/10000] ..........32 ) Loss= 0.87644887\n",
      "Training [  33/10000] ..........33 ) Loss= 0.9014558\n",
      "Training [  34/10000] ..........34 ) Loss= 0.9427927\n",
      "Training [  35/10000] ..........35 ) Loss= 1.0002897\n",
      "Training [  36/10000] ..........36 ) Loss= 0.89997244\n",
      "Training [  37/10000] ..........37 ) Loss= 0.8988202\n",
      "Training [  38/10000] ..........38 ) Loss= 0.93699527\n",
      "Training [  39/10000] ..........39 ) Loss= 1.0076116\n",
      "Training [  40/10000] ..........40 ) Loss= 0.89153343\n",
      "Training [  41/10000] ..........41 ) Loss= 0.923535\n",
      "Training [  42/10000] ..........42 ) Loss= 0.8839742\n",
      "Training [  43/10000] ..........43 ) Loss= 0.8366107\n",
      "Training [  44/10000] ..........44 ) Loss= 0.98379266\n",
      "Training [  45/10000] ..........45 ) Loss= 0.8918578\n",
      "Training [  46/10000] ..........46 ) Loss= 1.0076183\n",
      "Training [  47/10000] ..........47 ) Loss= 0.8692804\n",
      "Training [  48/10000] ..........48 ) Loss= 0.8822712\n",
      "Training [  49/10000] ..........49 ) Loss= 0.9682676\n",
      "Training [  50/10000] ..........50 ) Loss= 0.8802766\n",
      "Training [  51/10000] ..........51 ) Loss= 0.96120274\n",
      "Training [  52/10000] ..........52 ) Loss= 0.8973768\n",
      "Training [  53/10000] ..........53 ) Loss= 0.8976075\n",
      "Training [  54/10000] ..........54 ) Loss= 0.9812717\n",
      "Training [  55/10000] ..........55 ) Loss= 0.8663679\n",
      "Training [  56/10000] ..........56 ) Loss= 0.943446\n",
      "Training [  57/10000] ..........57 ) Loss= 0.8711791\n",
      "Training [  58/10000] ..........58 ) Loss= 0.9247016\n",
      "Training [  59/10000] ..........59 ) Loss= 0.8697864\n",
      "Training [  60/10000] ..........60 ) Loss= 0.93103635\n",
      "Training [  61/10000] ..........61 ) Loss= 0.7956793\n",
      "Training [  62/10000] ..........62 ) Loss= 0.97063375\n",
      "Training [  63/10000] ..........63 ) Loss= 0.82067186\n",
      "Training [  64/10000] ..........64 ) Loss= 0.8891898\n",
      "Training [  65/10000] ..........65 ) Loss= 0.78817415\n",
      "Training [  66/10000] ..........66 ) Loss= 0.899728\n",
      "Training [  67/10000] ..........67 ) Loss= 0.7865487\n",
      "Training [  68/10000] ..........68 ) Loss= 0.89999324\n",
      "Training [  69/10000] ..........69 ) Loss= 0.8730194\n",
      "Training [  70/10000] ..........70 ) Loss= 0.9279882\n",
      "Training [  71/10000] ..........71 ) Loss= 0.7766183\n",
      "Training [  72/10000] ..........72 ) Loss= 0.887793\n",
      "Training [  73/10000] ..........73 ) Loss= 0.9422236\n",
      "Training [  74/10000] ..........74 ) Loss= 0.93767715\n",
      "Training [  75/10000] ..........75 ) Loss= 0.81219333\n",
      "Training [  76/10000] ..........76 ) Loss= 0.8266874\n",
      "Training [  77/10000] ..........77 ) Loss= 0.89817697\n",
      "Training [  78/10000] ..........78 ) Loss= 0.82055706\n",
      "Training [  79/10000] ..........79 ) Loss= 0.93203527\n",
      "Training [  80/10000] ..........80 ) Loss= 0.89371014\n",
      "Training [  81/10000] ..........81 ) Loss= 0.8033974\n",
      "Training [  82/10000] ..........82 ) Loss= 0.91507125\n",
      "Training [  83/10000] ..........83 ) Loss= 0.78742635\n",
      "Training [  84/10000] ..........84 ) Loss= 0.7012521\n",
      "Training [  85/10000] ..........85 ) Loss= 0.89931065\n",
      "Training [  86/10000] ..........86 ) Loss= 0.85083205\n",
      "Training [  87/10000] ..........87 ) Loss= 0.8274276\n",
      "Training [  88/10000] ..........88 ) Loss= 0.7146428\n",
      "Training [  89/10000] ..........89 ) Loss= 0.80652165\n",
      "Training [  90/10000] ..........90 ) Loss= 0.7720895\n",
      "Training [  91/10000] ..........91 ) Loss= 0.8329534\n",
      "Training [  92/10000] ..........92 ) Loss= 0.7866107\n",
      "Training [  93/10000] ..........93 ) Loss= 0.8769848\n",
      "Training [  94/10000] ..........94 ) Loss= 0.883511\n",
      "Training [  95/10000] ..........95 ) Loss= 0.7960556\n",
      "Training [  96/10000] ..........96 ) Loss= 0.757544\n",
      "Training [  97/10000] ..........97 ) Loss= 0.7293278\n",
      "Training [  98/10000] ..........98 ) Loss= 0.7562469\n",
      "Training [  99/10000] ..........99 ) Loss= 0.82682055\n",
      "Training [ 100/10000] ..........100 ) Loss= 0.7958801\n",
      "Training [ 101/10000] ..........101 ) Loss= 0.87177974\n",
      "Training [ 102/10000] ..........102 ) Loss= 0.7281607\n",
      "Training [ 103/10000] ..........103 ) Loss= 0.820085\n",
      "Training [ 104/10000] ..........104 ) Loss= 0.8089689\n",
      "Training [ 105/10000] ..........105 ) Loss= 0.66353875\n",
      "Training [ 106/10000] ..........106 ) Loss= 0.7927694\n",
      "Training [ 107/10000] ..........107 ) Loss= 0.8284735\n",
      "Training [ 108/10000] ..........108 ) Loss= 0.80798537\n",
      "Training [ 109/10000] ..........109 ) Loss= 0.7806035\n",
      "Training [ 110/10000] ..........110 ) Loss= 0.7932422\n",
      "Training [ 111/10000] ..........111 ) Loss= 0.8424288\n",
      "Training [ 112/10000] ..........112 ) Loss= 0.7862862\n",
      "Training [ 113/10000] ..........113 ) Loss= 0.6983956\n",
      "Training [ 114/10000] ..........114 ) Loss= 0.73211396\n",
      "Training [ 115/10000] ..........115 ) Loss= 0.8400267\n",
      "Training [ 116/10000] ..........116 ) Loss= 0.83999336\n",
      "Training [ 117/10000] ..........117 ) Loss= 0.730898\n",
      "Training [ 118/10000] ..........118 ) Loss= 0.8987078\n",
      "Training [ 119/10000] ..........119 ) Loss= 0.8746205\n",
      "Training [ 120/10000] ..........120 ) Loss= 0.7430652\n",
      "Training [ 121/10000] ..........121 ) Loss= 0.95596695\n",
      "Training [ 122/10000] ..........122 ) Loss= 0.78138435\n",
      "Training [ 123/10000] ..........123 ) Loss= 0.76324266\n",
      "Training [ 124/10000] ..........124 ) Loss= 0.74258727\n",
      "Training [ 125/10000] ..........125 ) Loss= 0.7351087\n",
      "Training [ 126/10000] ..........126 ) Loss= 0.8573384\n",
      "Training [ 127/10000] ..........127 ) Loss= 0.6924838\n",
      "Training [ 128/10000] ..........128 ) Loss= 0.6985065\n",
      "Training [ 129/10000] ..........129 ) Loss= 0.79784584\n",
      "Training [ 130/10000] ..........130 ) Loss= 0.8381102\n",
      "Training [ 131/10000] ..........131 ) Loss= 0.70023835\n",
      "Training [ 132/10000] ..........132 ) Loss= 0.69269884\n",
      "Training [ 133/10000] ..........133 ) Loss= 0.79240304\n",
      "Training [ 134/10000] ..........134 ) Loss= 0.86183816\n",
      "Training [ 135/10000] ..........135 ) Loss= 0.89973295\n",
      "Training [ 136/10000] ..........136 ) Loss= 0.7055403\n",
      "Training [ 137/10000] ..........137 ) Loss= 0.68242097\n",
      "Training [ 138/10000] ..........138 ) Loss= 0.70300835\n",
      "Training [ 139/10000] ..........139 ) Loss= 0.7590427\n",
      "Training [ 140/10000] ..........140 ) Loss= 0.6843101\n",
      "Training [ 141/10000] ..........141 ) Loss= 0.68971485\n",
      "Training [ 142/10000] ..........142 ) Loss= 0.6087414\n",
      "Training [ 143/10000] ..........143 ) Loss= 0.71588445\n",
      "Training [ 144/10000] ..........144 ) Loss= 0.65168124\n",
      "Training [ 145/10000] ..........145 ) Loss= 0.65608114\n",
      "Training [ 146/10000] ..........146 ) Loss= 0.66573673\n",
      "Training [ 147/10000] ..........147 ) Loss= 0.75961673\n",
      "Training [ 148/10000] ..........148 ) Loss= 0.7027958\n",
      "Training [ 149/10000] ..........149 ) Loss= 0.64139503\n",
      "Training [ 150/10000] ..........150 ) Loss= 0.66452014\n",
      "Training [ 151/10000] ..........151 ) Loss= 0.86327404\n",
      "Training [ 152/10000] ..........152 ) Loss= 0.8406157\n",
      "Training [ 153/10000] ..........153 ) Loss= 0.7675829\n",
      "Training [ 154/10000] ..........154 ) Loss= 0.73119557\n",
      "Training [ 155/10000] ..........155 ) Loss= 0.8084295\n",
      "Training [ 156/10000] ..........156 ) Loss= 0.59847903\n",
      "Training [ 157/10000] ..........157 ) Loss= 0.7150128\n",
      "Training [ 158/10000] ..........158 ) Loss= 0.66870874\n",
      "Training [ 159/10000] ..........159 ) Loss= 0.90694064\n",
      "Training [ 160/10000] ..........160 ) Loss= 0.6404005\n",
      "Training [ 161/10000] ..........161 ) Loss= 0.594873\n",
      "Training [ 162/10000] ..........162 ) Loss= 0.58584183\n",
      "Training [ 163/10000] ..........163 ) Loss= 0.6196767\n",
      "Training [ 164/10000] ..........164 ) Loss= 0.6178842\n",
      "Training [ 165/10000] ..........165 ) Loss= 0.5980439\n",
      "Training [ 166/10000] ..........166 ) Loss= 0.5864414\n",
      "Training [ 167/10000] ..........167 ) Loss= 0.7696175\n",
      "Training [ 168/10000] ..........168 ) Loss= 0.7465184\n",
      "Training [ 169/10000] ..........169 ) Loss= 0.6944201\n",
      "Training [ 170/10000] ..........170 ) Loss= 0.72434807\n",
      "Training [ 171/10000] ..........171 ) Loss= 0.6079138\n",
      "Training [ 172/10000] ..........172 ) Loss= 0.6178364\n",
      "Training [ 173/10000] ..........173 ) Loss= 0.5958307\n",
      "Training [ 174/10000] ..........174 ) Loss= 0.86233413\n",
      "Training [ 175/10000] ..........175 ) Loss= 0.75875336\n",
      "Training [ 176/10000] ..........176 ) Loss= 0.7031735\n",
      "Training [ 177/10000] ..........177 ) Loss= 0.6628659\n",
      "Training [ 178/10000] ..........178 ) Loss= 0.72102904\n",
      "Training [ 179/10000] ..........179 ) Loss= 0.76418453\n",
      "Training [ 180/10000] ..........180 ) Loss= 0.6704582\n",
      "Training [ 181/10000] ..........181 ) Loss= 0.6351619\n",
      "Training [ 182/10000] ..........182 ) Loss= 0.6971776\n",
      "Training [ 183/10000] ..........183 ) Loss= 0.66427773\n",
      "Training [ 184/10000] ..........184 ) Loss= 0.62471354\n",
      "Training [ 185/10000] ..........185 ) Loss= 0.5721729\n",
      "Training [ 186/10000] ..........186 ) Loss= 0.69226575\n",
      "Training [ 187/10000] ..........187 ) Loss= 0.7845461\n",
      "Training [ 188/10000] ..........188 ) Loss= 0.595297\n",
      "Training [ 189/10000] ..........189 ) Loss= 0.5186446\n",
      "Training [ 190/10000] ..........190 ) Loss= 0.5349199\n",
      "Training [ 191/10000] ..........191 ) Loss= 0.5600706\n",
      "Training [ 192/10000] ..........192 ) Loss= 0.5119363\n",
      "Training [ 193/10000] ..........193 ) Loss= 0.57049245\n",
      "Training [ 194/10000] ..........194 ) Loss= 0.64661264\n",
      "Training [ 195/10000] ..........195 ) Loss= 0.695876\n",
      "Training [ 196/10000] ..........196 ) Loss= 0.5504413\n",
      "Training [ 197/10000] ..........197 ) Loss= 0.67409366\n",
      "Training [ 198/10000] ..........198 ) Loss= 0.65960824\n",
      "Training [ 199/10000] ..........199 ) Loss= 0.4939512\n",
      "Training [ 200/10000] ..........200 ) Loss= 0.74382675\n",
      "Training [ 201/10000] ..........201 ) Loss= 0.8692991\n",
      "Training [ 202/10000] ..........202 ) Loss= 0.6018581\n",
      "Training [ 203/10000] ..........203 ) Loss= 0.7814381\n",
      "Training [ 204/10000] ..........204 ) Loss= 0.61513126\n",
      "Training [ 205/10000] ..........205 ) Loss= 0.5907767\n",
      "Training [ 206/10000] ..........206 ) Loss= 0.6822771\n",
      "Training [ 207/10000] ..........207 ) Loss= 0.6745305\n",
      "Training [ 208/10000] ..........208 ) Loss= 0.7257264\n",
      "Training [ 209/10000] ..........209 ) Loss= 0.61340404\n",
      "Training [ 210/10000] ..........210 ) Loss= 0.69274956\n",
      "Training [ 211/10000] ..........211 ) Loss= 0.56124544\n",
      "Training [ 212/10000] ..........212 ) Loss= 0.60968256\n",
      "Training [ 213/10000] ..........213 ) Loss= 0.65547705\n",
      "Training [ 214/10000] ..........214 ) Loss= 0.831504\n",
      "Training [ 215/10000] ..........215 ) Loss= 0.5051369\n",
      "Training [ 216/10000] ..........216 ) Loss= 0.82727724\n",
      "Training [ 217/10000] ..........217 ) Loss= 0.65747476\n",
      "Training [ 218/10000] ..........218 ) Loss= 0.6612605\n",
      "Training [ 219/10000] ..........219 ) Loss= 0.56811106\n",
      "Training [ 220/10000] ..........220 ) Loss= 0.67857105\n",
      "Training [ 221/10000] ..........221 ) Loss= 0.6065994\n",
      "Training [ 222/10000] ..........222 ) Loss= 0.52511746\n",
      "Training [ 223/10000] ..........223 ) Loss= 0.5150779\n",
      "Training [ 224/10000] ..........224 ) Loss= 0.6612897\n",
      "Training [ 225/10000] ..........225 ) Loss= 0.5051049\n",
      "Training [ 226/10000] ..........226 ) Loss= 0.60939026\n",
      "Training [ 227/10000] ..........227 ) Loss= 0.8265146\n",
      "Training [ 228/10000] ..........228 ) Loss= 0.5894725\n",
      "Training [ 229/10000] ..........229 ) Loss= 0.47689655\n",
      "Training [ 230/10000] ..........230 ) Loss= 0.45124462\n",
      "Training [ 231/10000] ..........231 ) Loss= 0.5205749\n",
      "Training [ 232/10000] ..........232 ) Loss= 0.5989959\n",
      "Training [ 233/10000] ..........233 ) Loss= 0.5065681\n",
      "Training [ 234/10000] ..........234 ) Loss= 0.4864054\n",
      "Training [ 235/10000] ..........235 ) Loss= 0.42993927\n",
      "Training [ 236/10000] ..........236 ) Loss= 0.53931105\n",
      "Training [ 237/10000] ..........237 ) Loss= 0.48133454\n",
      "Training [ 238/10000] ..........238 ) Loss= 0.6371685\n",
      "Training [ 239/10000] ..........239 ) Loss= 0.7290688\n",
      "Training [ 240/10000] ..........240 ) Loss= 0.49213132\n",
      "Training [ 241/10000] ..........241 ) Loss= 0.4619477\n",
      "Training [ 242/10000] ..........242 ) Loss= 0.4541014\n",
      "Training [ 243/10000] ..........243 ) Loss= 0.67124957\n",
      "Training [ 244/10000] ..........244 ) Loss= 0.46392116\n",
      "Training [ 245/10000] ..........245 ) Loss= 0.6964068\n",
      "Training [ 246/10000] ..........246 ) Loss= 0.5148116\n",
      "Training [ 247/10000] ..........247 ) Loss= 0.50244826\n",
      "Training [ 248/10000] ..........248 ) Loss= 0.68274504\n",
      "Training [ 249/10000] ..........249 ) Loss= 0.48900366\n",
      "Training [ 250/10000] ..........250 ) Loss= 0.5027866\n",
      "Training [ 251/10000] ..........251 ) Loss= 0.57181114\n",
      "Training [ 252/10000] ..........252 ) Loss= 0.9825267\n",
      "Training [ 253/10000] ..........253 ) Loss= 0.57921946\n",
      "Training [ 254/10000] ..........254 ) Loss= 0.4195806\n",
      "Training [ 255/10000] ..........255 ) Loss= 0.59524983\n",
      "Training [ 256/10000] ..........256 ) Loss= 0.5411131\n",
      "Training [ 257/10000] ..........257 ) Loss= 0.58002704\n",
      "Training [ 258/10000] ..........258 ) Loss= 0.5145308\n",
      "Training [ 259/10000] ..........259 ) Loss= 0.562278\n",
      "Training [ 260/10000] ..........260 ) Loss= 0.7923886\n",
      "Training [ 261/10000] ..........261 ) Loss= 0.5097207\n",
      "Training [ 262/10000] ..........262 ) Loss= 0.53117937\n",
      "Training [ 263/10000] ..........263 ) Loss= 0.6095487\n",
      "Training [ 264/10000] ..........264 ) Loss= 0.5159051\n",
      "Training [ 265/10000] ..........265 ) Loss= 0.5026654\n",
      "Training [ 266/10000] ..........266 ) Loss= 0.7138883\n",
      "Training [ 267/10000] ..........267 ) Loss= 0.5512619\n",
      "Training [ 268/10000] ..........268 ) Loss= 0.5376878\n",
      "Training [ 269/10000] ..........269 ) Loss= 0.66169596\n",
      "Training [ 270/10000] ..........270 ) Loss= 0.53586245\n",
      "Training [ 271/10000] ..........271 ) Loss= 0.66645086\n",
      "Training [ 272/10000] ..........272 ) Loss= 0.6024311\n",
      "Training [ 273/10000] ..........273 ) Loss= 0.5185819\n",
      "Training [ 274/10000] ..........274 ) Loss= 0.65145767\n",
      "Training [ 275/10000] ..........275 ) Loss= 0.75074345\n",
      "Training [ 276/10000] ..........276 ) Loss= 0.5747612\n",
      "Training [ 277/10000] ..........277 ) Loss= 0.6388829\n",
      "Training [ 278/10000] ..........278 ) Loss= 0.4944394\n",
      "Training [ 279/10000] ..........279 ) Loss= 0.45315918\n",
      "Training [ 280/10000] ..........280 ) Loss= 0.47793114\n",
      "Training [ 281/10000] ..........281 ) Loss= 0.40340856\n",
      "Training [ 282/10000] ..........282 ) Loss= 0.523384\n",
      "Training [ 283/10000] ..........283 ) Loss= 0.6536351\n",
      "Training [ 284/10000] ..........284 ) Loss= 0.5411461\n",
      "Training [ 285/10000] ..........285 ) Loss= 0.47744903\n",
      "Training [ 286/10000] ..........286 ) Loss= 0.55498725\n",
      "Training [ 287/10000] ..........287 ) Loss= 0.4865858\n",
      "Training [ 288/10000] ..........288 ) Loss= 0.50782514\n",
      "Training [ 289/10000] ..........289 ) Loss= 0.549907\n",
      "Training [ 290/10000] ..........290 ) Loss= 0.5155923\n",
      "Training [ 291/10000] ..........291 ) Loss= 0.45407072\n",
      "Training [ 292/10000] ..........292 ) Loss= 0.71385306\n",
      "Training [ 293/10000] ..........293 ) Loss= 0.46173203\n",
      "Training [ 294/10000] ..........294 ) Loss= 0.48349148\n",
      "Training [ 295/10000] ..........295 ) Loss= 0.45186546\n",
      "Training [ 296/10000] ..........296 ) Loss= 0.4285165\n",
      "Training [ 297/10000] ..........297 ) Loss= 0.40760505\n",
      "Training [ 298/10000] ..........298 ) Loss= 0.5340841\n",
      "Training [ 299/10000] ..........299 ) Loss= 0.48179662\n",
      "Training [ 300/10000] ..........300 ) Loss= 0.5359739\n",
      "Training [ 301/10000] ..........301 ) Loss= 0.5516503\n",
      "Training [ 302/10000] ..........302 ) Loss= 0.62339747\n",
      "Training [ 303/10000] ..........303 ) Loss= 0.5470684\n",
      "Training [ 304/10000] ..........304 ) Loss= 0.53857684\n",
      "Training [ 305/10000] ..........305 ) Loss= 0.57812625\n",
      "Training [ 306/10000] ..........306 ) Loss= 0.432543\n",
      "Training [ 307/10000] ..........307 ) Loss= 0.5379538\n",
      "Training [ 308/10000] ..........308 ) Loss= 0.5347386\n",
      "Training [ 309/10000] ..........309 ) Loss= 0.4957113\n",
      "Training [ 310/10000] ..........310 ) Loss= 0.6186569\n",
      "Training [ 311/10000] ..........311 ) Loss= 0.45727193\n",
      "Training [ 312/10000] ..........312 ) Loss= 0.564588\n",
      "Training [ 313/10000] ..........313 ) Loss= 0.5562531\n",
      "Training [ 314/10000] ..........314 ) Loss= 0.49364763\n",
      "Training [ 315/10000] ..........315 ) Loss= 0.54146075\n",
      "Training [ 316/10000] ..........316 ) Loss= 0.4835591\n",
      "Training [ 317/10000] ..........317 ) Loss= 0.5131382\n",
      "Training [ 318/10000] ..........318 ) Loss= 0.5363685\n",
      "Training [ 319/10000] ..........319 ) Loss= 0.5551156\n",
      "Training [ 320/10000] ..........320 ) Loss= 0.58282167\n",
      "Training [ 321/10000] ..........321 ) Loss= 0.6749496\n",
      "Training [ 322/10000] ..........322 ) Loss= 0.51208574\n",
      "Training [ 323/10000] ..........323 ) Loss= 0.4850459\n",
      "Training [ 324/10000] ..........324 ) Loss= 0.6054236\n",
      "Training [ 325/10000] ..........325 ) Loss= 0.45004505\n",
      "Training [ 326/10000] ..........326 ) Loss= 0.46708307\n",
      "Training [ 327/10000] ..........327 ) Loss= 0.62507904\n",
      "Training [ 328/10000] ..........328 ) Loss= 0.56631523\n",
      "Training [ 329/10000] ..........329 ) Loss= 0.4101362\n",
      "Training [ 330/10000] ..........330 ) Loss= 0.49422356\n",
      "Training [ 331/10000] ..........331 ) Loss= 0.37295514\n",
      "Training [ 332/10000] ..........332 ) Loss= 0.4234344\n",
      "Training [ 333/10000] ..........333 ) Loss= 0.49253136\n",
      "Training [ 334/10000] ..........334 ) Loss= 0.4851037\n",
      "Training [ 335/10000] ..........335 ) Loss= 0.3811107\n",
      "Training [ 336/10000] ..........336 ) Loss= 0.48590216\n",
      "Training [ 337/10000] ..........337 ) Loss= 0.5383987\n",
      "Training [ 338/10000] ..........338 ) Loss= 0.41130877\n",
      "Training [ 339/10000] ..........339 ) Loss= 0.6239706\n",
      "Training [ 340/10000] ..........340 ) Loss= 0.44020134\n",
      "Training [ 341/10000] ..........341 ) Loss= 0.41904995\n",
      "Training [ 342/10000] ..........342 ) Loss= 0.5095482\n",
      "Training [ 343/10000] ..........343 ) Loss= 0.43701234\n",
      "Training [ 344/10000] ..........344 ) Loss= 0.48988658\n",
      "Training [ 345/10000] ..........345 ) Loss= 0.40240815\n",
      "Training [ 346/10000] ..........346 ) Loss= 0.4934238\n",
      "Training [ 347/10000] ..........347 ) Loss= 0.44819212\n",
      "Training [ 348/10000] ..........348 ) Loss= 0.587628\n",
      "Training [ 349/10000] ..........349 ) Loss= 0.69407153\n",
      "Training [ 350/10000] ..........350 ) Loss= 0.45100325\n",
      "Training [ 351/10000] ..........351 ) Loss= 0.43760446\n",
      "Training [ 352/10000] ..........352 ) Loss= 0.4599273\n",
      "Training [ 353/10000] ..........353 ) Loss= 0.5282729\n",
      "Training [ 354/10000] ..........354 ) Loss= 0.37187093\n",
      "Training [ 355/10000] ..........355 ) Loss= 0.49018463\n",
      "Training [ 356/10000] ..........356 ) Loss= 0.41353682\n",
      "Training [ 357/10000] ..........357 ) Loss= 0.44627383\n",
      "Training [ 358/10000] ..........358 ) Loss= 0.6970785\n",
      "Training [ 359/10000] ..........359 ) Loss= 0.44799176\n",
      "Training [ 360/10000] ..........360 ) Loss= 0.47148457\n",
      "Training [ 361/10000] ..........361 ) Loss= 0.52716386\n",
      "Training [ 362/10000] ..........362 ) Loss= 0.38801908\n",
      "Training [ 363/10000] ..........363 ) Loss= 0.47831386\n",
      "Training [ 364/10000] ..........364 ) Loss= 0.38164705\n",
      "Training [ 365/10000] ..........365 ) Loss= 0.45054436\n",
      "Training [ 366/10000] ..........366 ) Loss= 0.43813863\n",
      "Training [ 367/10000] ..........367 ) Loss= 0.44635922\n",
      "Training [ 368/10000] ..........368 ) Loss= 0.51306564\n",
      "Training [ 369/10000] ..........369 ) Loss= 0.6682025\n",
      "Training [ 370/10000] ..........370 ) Loss= 0.54984695\n",
      "Training [ 371/10000] ..........371 ) Loss= 0.54151976\n",
      "Training [ 372/10000] ..........372 ) Loss= 0.38822842\n",
      "Training [ 373/10000] ..........373 ) Loss= 0.52677065\n",
      "Training [ 374/10000] ..........374 ) Loss= 0.4082793\n",
      "Training [ 375/10000] ..........375 ) Loss= 0.328761\n",
      "Training [ 376/10000] ..........376 ) Loss= 0.4651516\n",
      "Training [ 377/10000] ..........377 ) Loss= 0.48128468\n",
      "Training [ 378/10000] ..........378 ) Loss= 0.3649597\n",
      "Training [ 379/10000] ..........379 ) Loss= 0.33973706\n",
      "Training [ 380/10000] ..........380 ) Loss= 0.45215023\n",
      "Training [ 381/10000] ..........381 ) Loss= 0.4695448\n",
      "Training [ 382/10000] ..........382 ) Loss= 0.40347546\n",
      "Training [ 383/10000] ..........383 ) Loss= 0.79009336\n",
      "Training [ 384/10000] ..........384 ) Loss= 0.37928215\n",
      "Training [ 385/10000] ..........385 ) Loss= 0.39009336\n",
      "Training [ 386/10000] ..........386 ) Loss= 0.4194234\n",
      "Training [ 387/10000] ..........387 ) Loss= 0.52887064\n",
      "Training [ 388/10000] ..........388 ) Loss= 0.42987746\n",
      "Training [ 389/10000] ..........389 ) Loss= 0.57028544\n",
      "Training [ 390/10000] ..........390 ) Loss= 0.45500332\n",
      "Training [ 391/10000] ..........391 ) Loss= 0.55199957\n",
      "Training [ 392/10000] ..........392 ) Loss= 0.45679897\n",
      "Training [ 393/10000] ..........393 ) Loss= 0.37798998\n",
      "Training [ 394/10000] ..........394 ) Loss= 0.47737435\n",
      "Training [ 395/10000] ..........395 ) Loss= 0.56093\n",
      "Training [ 396/10000] ..........396 ) Loss= 0.43251348\n",
      "Training [ 397/10000] ..........397 ) Loss= 0.46077406\n",
      "Training [ 398/10000] ..........398 ) Loss= 0.38255194\n",
      "Training [ 399/10000] ..........399 ) Loss= 0.40894338\n",
      "Training [ 400/10000] ..........400 ) Loss= 0.5810507\n",
      "Training [ 401/10000] ..........401 ) Loss= 0.4458326\n",
      "Training [ 402/10000] ..........402 ) Loss= 0.59735614\n",
      "Training [ 403/10000] ..........403 ) Loss= 0.61314297\n",
      "Training [ 404/10000] ..........404 ) Loss= 0.48398042\n",
      "Training [ 405/10000] ..........405 ) Loss= 0.53558546\n",
      "Training [ 406/10000] ..........406 ) Loss= 0.3977138\n",
      "Training [ 407/10000] ..........407 ) Loss= 0.5182733\n",
      "Training [ 408/10000] ..........408 ) Loss= 0.34966856\n",
      "Training [ 409/10000] ..........409 ) Loss= 0.550404\n",
      "Training [ 410/10000] ..........410 ) Loss= 0.46435326\n",
      "Training [ 411/10000] ..........411 ) Loss= 0.35863063\n",
      "Training [ 412/10000] ..........412 ) Loss= 0.5746955\n",
      "Training [ 413/10000] ..........413 ) Loss= 0.4717301\n",
      "Training [ 414/10000] ..........414 ) Loss= 0.6077448\n",
      "Training [ 415/10000] ..........415 ) Loss= 0.9239055\n",
      "Training [ 416/10000] ..........416 ) Loss= 0.45901334\n",
      "Training [ 417/10000] ..........417 ) Loss= 0.5879984\n",
      "Training [ 418/10000] ..........418 ) Loss= 0.40368262\n",
      "Training [ 419/10000] ..........419 ) Loss= 0.47542706\n",
      "Training [ 420/10000] ..........420 ) Loss= 0.6035086\n",
      "Training [ 421/10000] ..........421 ) Loss= 0.5542458\n",
      "Training [ 422/10000] ..........422 ) Loss= 0.39071918\n",
      "Training [ 423/10000] ..........423 ) Loss= 0.4131556\n",
      "Training [ 424/10000] ..........424 ) Loss= 0.36307552\n",
      "Training [ 425/10000] ..........425 ) Loss= 0.35611257\n",
      "Training [ 426/10000] ..........426 ) Loss= 0.4675634\n",
      "Training [ 427/10000] ..........427 ) Loss= 0.4309946\n",
      "Training [ 428/10000] ..........428 ) Loss= 0.4292205\n",
      "Training [ 429/10000] ..........429 ) Loss= 0.40473118\n",
      "Training [ 430/10000] ..........430 ) Loss= 0.37592676\n",
      "Training [ 431/10000] ..........431 ) Loss= 0.41297266\n",
      "Training [ 432/10000] ..........432 ) Loss= 0.6050238\n",
      "Training [ 433/10000] ..........433 ) Loss= 0.59059864\n",
      "Training [ 434/10000] ..........434 ) Loss= 0.39421168\n",
      "Training [ 435/10000] ..........435 ) Loss= 0.39219916\n",
      "Training [ 436/10000] ..........436 ) Loss= 0.35798198\n",
      "Training [ 437/10000] ..........437 ) Loss= 0.48431876\n",
      "Training [ 438/10000] ..........438 ) Loss= 0.36507833\n",
      "Training [ 439/10000] ..........439 ) Loss= 0.5001783\n",
      "Training [ 440/10000] ..........440 ) Loss= 0.46919826\n",
      "Training [ 441/10000] ..........441 ) Loss= 0.5381874\n",
      "Training [ 442/10000] ..........442 ) Loss= 0.5025055\n",
      "Training [ 443/10000] ..........443 ) Loss= 0.35420123\n",
      "Training [ 444/10000] ..........444 ) Loss= 0.4054711\n",
      "Training [ 445/10000] ..........445 ) Loss= 0.51436716\n",
      "Training [ 446/10000] ..........446 ) Loss= 0.4054349\n",
      "Training [ 447/10000] ..........447 ) Loss= 0.3683917\n",
      "Training [ 448/10000] ..........448 ) Loss= 0.4783166\n",
      "Training [ 449/10000] ..........449 ) Loss= 0.45092008\n",
      "Training [ 450/10000] ..........450 ) Loss= 0.36614743\n",
      "Training [ 451/10000] ..........451 ) Loss= 0.6651603\n",
      "Training [ 452/10000] ..........452 ) Loss= 0.37266842\n",
      "Training [ 453/10000] ..........453 ) Loss= 0.44000813\n",
      "Training [ 454/10000] ..........454 ) Loss= 0.3718757\n",
      "Training [ 455/10000] ..........455 ) Loss= 0.58946466\n",
      "Training [ 456/10000] ..........456 ) Loss= 0.61647433\n",
      "Training [ 457/10000] ..........457 ) Loss= 0.4656653\n",
      "Training [ 458/10000] ..........458 ) Loss= 0.50151134\n",
      "Training [ 459/10000] ..........459 ) Loss= 0.37932324\n",
      "Training [ 460/10000] ..........460 ) Loss= 0.41205144\n",
      "Training [ 461/10000] ..........461 ) Loss= 0.44246668\n",
      "Training [ 462/10000] ..........462 ) Loss= 0.4371818\n",
      "Training [ 463/10000] ..........463 ) Loss= 0.5594587\n",
      "Training [ 464/10000] ..........464 ) Loss= 0.35375118\n",
      "Training [ 465/10000] ..........465 ) Loss= 0.49976566\n",
      "Training [ 466/10000] ..........466 ) Loss= 0.40276662\n",
      "Training [ 467/10000] ..........467 ) Loss= 0.46210963\n",
      "Training [ 468/10000] ..........468 ) Loss= 0.38778797\n",
      "Training [ 469/10000] ..........469 ) Loss= 0.5027358\n",
      "Training [ 470/10000] ..........470 ) Loss= 0.3841549\n",
      "Training [ 471/10000] ..........471 ) Loss= 0.39293748\n",
      "Training [ 472/10000] ..........472 ) Loss= 0.35772613\n",
      "Training [ 473/10000] ..........473 ) Loss= 0.3717415\n",
      "Training [ 474/10000] ..........474 ) Loss= 0.36014828\n",
      "Training [ 475/10000] ..........475 ) Loss= 0.3532611\n",
      "Training [ 476/10000] ..........476 ) Loss= 0.415405\n",
      "Training [ 477/10000] ..........477 ) Loss= 0.37547466\n",
      "Training [ 478/10000] ..........478 ) Loss= 0.5096044\n",
      "Training [ 479/10000] ..........479 ) Loss= 0.37760884\n",
      "Training [ 480/10000] ..........480 ) Loss= 0.36412442\n",
      "Training [ 481/10000] ..........481 ) Loss= 0.5314244\n",
      "Training [ 482/10000] ..........482 ) Loss= 0.46601516\n",
      "Training [ 483/10000] ..........483 ) Loss= 0.4204243\n",
      "Training [ 484/10000] ..........484 ) Loss= 0.5042671\n",
      "Training [ 485/10000] ..........485 ) Loss= 0.38831985\n",
      "Training [ 486/10000] ..........486 ) Loss= 0.3825265\n",
      "Training [ 487/10000] ..........487 ) Loss= 0.42964467\n",
      "Training [ 488/10000] ..........488 ) Loss= 0.3927116\n",
      "Training [ 489/10000] ..........489 ) Loss= 0.4088548\n",
      "Training [ 490/10000] ..........490 ) Loss= 0.4038189\n",
      "Training [ 491/10000] ..........491 ) Loss= 0.51796365\n",
      "Training [ 492/10000] ..........492 ) Loss= 0.32029474\n",
      "Training [ 493/10000] ..........493 ) Loss= 0.3747805\n",
      "Training [ 494/10000] ..........494 ) Loss= 0.35805994\n",
      "Training [ 495/10000] ..........495 ) Loss= 0.40815827\n",
      "Training [ 496/10000] ..........496 ) Loss= 0.40656993\n",
      "Training [ 497/10000] ..........497 ) Loss= 0.5012744\n",
      "Training [ 498/10000] ..........498 ) Loss= 0.45043424\n",
      "Training [ 499/10000] ..........499 ) Loss= 0.3539378\n",
      "Training [ 500/10000] ..........500 ) Loss= 0.5062026\n",
      "Training [ 501/10000] ..........501 ) Loss= 0.38921866\n",
      "Training [ 502/10000] ..........502 ) Loss= 0.39333302\n",
      "Training [ 503/10000] ..........503 ) Loss= 0.34067544\n",
      "Training [ 504/10000] ..........504 ) Loss= 0.5009598\n",
      "Training [ 505/10000] ..........505 ) Loss= 0.5044329\n",
      "Training [ 506/10000] ..........506 ) Loss= 0.35685205\n",
      "Training [ 507/10000] ..........507 ) Loss= 0.40270782\n",
      "Training [ 508/10000] ..........508 ) Loss= 0.44841337\n",
      "Training [ 509/10000] ..........509 ) Loss= 0.37295792\n",
      "Training [ 510/10000] ..........510 ) Loss= 0.43948996\n",
      "Training [ 511/10000] ..........511 ) Loss= 0.31260023\n",
      "Training [ 512/10000] ..........512 ) Loss= 0.34333447\n",
      "Training [ 513/10000] ..........513 ) Loss= 0.46332556\n",
      "Training [ 514/10000] ..........514 ) Loss= 0.32109094\n",
      "Training [ 515/10000] ..........515 ) Loss= 0.5119406\n",
      "Training [ 516/10000] ..........516 ) Loss= 0.6330712\n",
      "Training [ 517/10000] ..........517 ) Loss= 0.44677162\n",
      "Training [ 518/10000] ..........518 ) Loss= 0.4480433\n",
      "Training [ 519/10000] ..........519 ) Loss= 0.44816342\n",
      "Training [ 520/10000] ..........520 ) Loss= 0.31217965\n",
      "Training [ 521/10000] ..........521 ) Loss= 0.43128055\n",
      "Training [ 522/10000] ..........522 ) Loss= 0.37219307\n",
      "Training [ 523/10000] ..........523 ) Loss= 0.3072519\n",
      "Training [ 524/10000] ..........524 ) Loss= 0.5797061\n",
      "Training [ 525/10000] ..........525 ) Loss= 0.47087386\n",
      "Training [ 526/10000] ..........526 ) Loss= 0.43074265\n",
      "Training [ 527/10000] ..........527 ) Loss= 0.54000473\n",
      "Training [ 528/10000] ..........528 ) Loss= 0.5485795\n",
      "Training [ 529/10000] ..........529 ) Loss= 0.38481992\n",
      "Training [ 530/10000] ..........530 ) Loss= 0.39561614\n",
      "Training [ 531/10000] ..........531 ) Loss= 0.30669257\n",
      "Training [ 532/10000] ..........532 ) Loss= 0.4855485\n",
      "Training [ 533/10000] ..........533 ) Loss= 0.48269492\n",
      "Training [ 534/10000] ..........534 ) Loss= 0.49876058\n",
      "Training [ 535/10000] ..........535 ) Loss= 0.39242637\n",
      "Training [ 536/10000] ..........536 ) Loss= 0.3577387\n",
      "Training [ 537/10000] ..........537 ) Loss= 0.562224\n",
      "Training [ 538/10000] ..........538 ) Loss= 0.369926\n",
      "Training [ 539/10000] ..........539 ) Loss= 0.36418736\n",
      "Training [ 540/10000] ..........540 ) Loss= 0.44805047\n",
      "Training [ 541/10000] ..........541 ) Loss= 0.42815846\n",
      "Training [ 542/10000] ..........542 ) Loss= 0.44884622\n",
      "Training [ 543/10000] ..........543 ) Loss= 0.44334167\n",
      "Training [ 544/10000] ..........544 ) Loss= 0.71183217\n",
      "Training [ 545/10000] ..........545 ) Loss= 0.36601263\n",
      "Training [ 546/10000] ..........546 ) Loss= 0.38785952\n",
      "Training [ 547/10000] ..........547 ) Loss= 0.3616876\n",
      "Training [ 548/10000] ..........548 ) Loss= 0.3664498\n",
      "Training [ 549/10000] ..........549 ) Loss= 0.4886484\n",
      "Training [ 550/10000] ..........550 ) Loss= 0.32731757\n",
      "Training [ 551/10000] ..........551 ) Loss= 0.45345253\n",
      "Training [ 552/10000] ..........552 ) Loss= 0.3190868\n",
      "Training [ 553/10000] ..........553 ) Loss= 0.47644487\n",
      "Training [ 554/10000] ..........554 ) Loss= 0.3984309\n",
      "Training [ 555/10000] ..........555 ) Loss= 0.35925657\n",
      "Training [ 556/10000] ..........556 ) Loss= 0.40661934\n",
      "Training [ 557/10000] ..........557 ) Loss= 0.57319677\n",
      "Training [ 558/10000] ..........558 ) Loss= 0.556216\n",
      "Training [ 559/10000] ..........559 ) Loss= 0.29499555\n",
      "Training [ 560/10000] ..........560 ) Loss= 0.2965311\n",
      "Training [ 561/10000] ..........561 ) Loss= 0.32688558\n",
      "Training [ 562/10000] ..........562 ) Loss= 0.40276256\n",
      "Training [ 563/10000] ..........563 ) Loss= 0.48537624\n",
      "Training [ 564/10000] ..........564 ) Loss= 0.4332173\n",
      "Training [ 565/10000] ..........565 ) Loss= 0.34875977\n",
      "Training [ 566/10000] ..........566 ) Loss= 0.35758528\n",
      "Training [ 567/10000] ..........567 ) Loss= 0.42724118\n",
      "Training [ 568/10000] ..........568 ) Loss= 0.30555364\n",
      "Training [ 569/10000] ..........569 ) Loss= 0.3592374\n",
      "Training [ 570/10000] ..........570 ) Loss= 0.6360829\n",
      "Training [ 571/10000] ..........571 ) Loss= 0.58458936\n",
      "Training [ 572/10000] ..........572 ) Loss= 0.3530573\n",
      "Training [ 573/10000] ..........573 ) Loss= 0.3231133\n",
      "Training [ 574/10000] ..........574 ) Loss= 0.4821512\n",
      "Training [ 575/10000] ..........575 ) Loss= 0.5174728\n",
      "Training [ 576/10000] ..........576 ) Loss= 0.3133254\n",
      "Training [ 577/10000] ..........577 ) Loss= 0.296591\n",
      "Training [ 578/10000] ..........578 ) Loss= 0.60339683\n",
      "Training [ 579/10000] ..........579 ) Loss= 0.4174972\n",
      "Training [ 580/10000] ..........580 ) Loss= 0.40822807\n",
      "Training [ 581/10000] ..........581 ) Loss= 0.39714673\n",
      "Training [ 582/10000] ..........582 ) Loss= 0.47758788\n",
      "Training [ 583/10000] ..........583 ) Loss= 0.33158672\n",
      "Training [ 584/10000] ..........584 ) Loss= 0.4983843\n",
      "Training [ 585/10000] ..........585 ) Loss= 0.6093587\n",
      "Training [ 586/10000] ..........586 ) Loss= 0.4543536\n",
      "Training [ 587/10000] ..........587 ) Loss= 0.38942444\n",
      "Training [ 588/10000] ..........588 ) Loss= 0.3272344\n",
      "Training [ 589/10000] ..........589 ) Loss= 0.37138116\n",
      "Training [ 590/10000] ..........590 ) Loss= 0.41050467\n",
      "Training [ 591/10000] ..........591 ) Loss= 0.5983074\n",
      "Training [ 592/10000] ..........592 ) Loss= 0.52245533\n",
      "Training [ 593/10000] ..........593 ) Loss= 0.4069883\n",
      "Training [ 594/10000] ..........594 ) Loss= 0.5115042\n",
      "Training [ 595/10000] ..........595 ) Loss= 0.30572182\n",
      "Training [ 596/10000] ..........596 ) Loss= 0.49356154\n",
      "Training [ 597/10000] ..........597 ) Loss= 0.6021051\n",
      "Training [ 598/10000] ..........598 ) Loss= 0.39733648\n",
      "Training [ 599/10000] ..........599 ) Loss= 0.4377787\n",
      "Training [ 600/10000] ..........600 ) Loss= 0.34978127\n",
      "Training [ 601/10000] ..........601 ) Loss= 0.356575\n",
      "Training [ 602/10000] ..........602 ) Loss= 0.31620583\n",
      "Training [ 603/10000] ..........603 ) Loss= 0.3431455\n",
      "Training [ 604/10000] ..........604 ) Loss= 0.35992327\n",
      "Training [ 605/10000] ..........605 ) Loss= 0.4552849\n",
      "Training [ 606/10000] ..........606 ) Loss= 0.3073431\n",
      "Training [ 607/10000] ..........607 ) Loss= 0.33330524\n",
      "Training [ 608/10000] ..........608 ) Loss= 0.34521592\n",
      "Training [ 609/10000] ..........609 ) Loss= 0.33984753\n",
      "Training [ 610/10000] ..........610 ) Loss= 0.29586947\n",
      "Training [ 611/10000] ..........611 ) Loss= 0.45468372\n",
      "Training [ 612/10000] ..........612 ) Loss= 0.40027824\n",
      "Training [ 613/10000] ..........613 ) Loss= 0.46805668\n",
      "Training [ 614/10000] ..........614 ) Loss= 0.36146814\n",
      "Training [ 615/10000] ..........615 ) Loss= 0.37456998\n",
      "Training [ 616/10000] ..........616 ) Loss= 0.41942856\n",
      "Training [ 617/10000] ..........617 ) Loss= 0.46409836\n",
      "Training [ 618/10000] ..........618 ) Loss= 0.37217638\n",
      "Training [ 619/10000] ..........619 ) Loss= 0.6314282\n",
      "Training [ 620/10000] ..........620 ) Loss= 0.44384858\n",
      "Training [ 621/10000] ..........621 ) Loss= 0.36573586\n",
      "Training [ 622/10000] ..........622 ) Loss= 0.5825985\n",
      "Training [ 623/10000] ..........623 ) Loss= 0.32590201\n",
      "Training [ 624/10000] ..........624 ) Loss= 0.889389\n",
      "Training [ 625/10000] ..........625 ) Loss= 0.50337684\n",
      "Training [ 626/10000] ..........626 ) Loss= 0.39855072\n",
      "Training [ 627/10000] ..........627 ) Loss= 0.37187448\n",
      "Training [ 628/10000] ..........628 ) Loss= 0.46945414\n",
      "Training [ 629/10000] ..........629 ) Loss= 0.5492079\n",
      "Training [ 630/10000] ..........630 ) Loss= 0.30438402\n",
      "Training [ 631/10000] ..........631 ) Loss= 0.37309945\n",
      "Training [ 632/10000] ..........632 ) Loss= 0.49742392\n",
      "Training [ 633/10000] ..........633 ) Loss= 0.410694\n",
      "Training [ 634/10000] ..........634 ) Loss= 0.48960716\n",
      "Training [ 635/10000] ..........635 ) Loss= 0.3722556\n",
      "Training [ 636/10000] ..........636 ) Loss= 0.35216135\n",
      "Training [ 637/10000] ..........637 ) Loss= 0.43295732\n",
      "Training [ 638/10000] ..........638 ) Loss= 0.9504402\n",
      "Training [ 639/10000] ..........639 ) Loss= 0.54726547\n",
      "Training [ 640/10000] ..........640 ) Loss= 0.36978275\n",
      "Training [ 641/10000] ..........641 ) Loss= 0.42020303\n",
      "Training [ 642/10000] ..........642 ) Loss= 0.4436256\n",
      "Training [ 643/10000] ..........643 ) Loss= 0.44035634\n",
      "Training [ 644/10000] ..........644 ) Loss= 0.5196975\n",
      "Training [ 645/10000] ..........645 ) Loss= 0.516551\n",
      "Training [ 646/10000] ..........646 ) Loss= 0.31219232\n",
      "Training [ 647/10000] ..........647 ) Loss= 0.5030849\n",
      "Training [ 648/10000] ..........648 ) Loss= 0.31513223\n",
      "Training [ 649/10000] ..........649 ) Loss= 0.32678935\n",
      "Training [ 650/10000] ..........650 ) Loss= 0.4103832\n",
      "Training [ 651/10000] ..........651 ) Loss= 0.40691844\n",
      "Training [ 652/10000] ..........652 ) Loss= 0.47575474\n",
      "Training [ 653/10000] ..........653 ) Loss= 0.3837111\n",
      "Training [ 654/10000] ..........654 ) Loss= 0.81900436\n",
      "Training [ 655/10000] ..........655 ) Loss= 0.3795661\n",
      "Training [ 656/10000] ..........656 ) Loss= 0.38652146\n",
      "Training [ 657/10000] ..........657 ) Loss= 0.53079116\n",
      "Training [ 658/10000] ..........658 ) Loss= 0.40567467\n",
      "Training [ 659/10000] ..........659 ) Loss= 0.6756349\n",
      "Training [ 660/10000] ..........660 ) Loss= 0.34234706\n",
      "Training [ 661/10000] ..........661 ) Loss= 0.39009187\n",
      "Training [ 662/10000] ..........662 ) Loss= 0.43671352\n",
      "Training [ 663/10000] ..........663 ) Loss= 0.5313602\n",
      "Training [ 664/10000] ..........664 ) Loss= 0.4130813\n",
      "Training [ 665/10000] ..........665 ) Loss= 0.41234145\n",
      "Training [ 666/10000] ..........666 ) Loss= 0.41065302\n",
      "Training [ 667/10000] ..........667 ) Loss= 0.6918468\n",
      "Training [ 668/10000] ..........668 ) Loss= 0.26727742\n",
      "Training [ 669/10000] ..........669 ) Loss= 0.5639358\n",
      "Training [ 670/10000] ..........670 ) Loss= 0.44036055\n",
      "Training [ 671/10000] ..........671 ) Loss= 0.46395272\n",
      "Training [ 672/10000] ..........672 ) Loss= 0.5071591\n",
      "Training [ 673/10000] ..........673 ) Loss= 0.43259463\n",
      "Training [ 674/10000] ..........674 ) Loss= 0.46352458\n",
      "Training [ 675/10000] ..........675 ) Loss= 0.4406297\n",
      "Training [ 676/10000] ..........676 ) Loss= 0.30793163\n",
      "Training [ 677/10000] ..........677 ) Loss= 0.57546455\n",
      "Training [ 678/10000] ..........678 ) Loss= 0.33611134\n",
      "Training [ 679/10000] ..........679 ) Loss= 0.3538451\n",
      "Training [ 680/10000] ..........680 ) Loss= 0.54251635\n",
      "Training [ 681/10000] ..........681 ) Loss= 0.3398722\n",
      "Training [ 682/10000] ..........682 ) Loss= 0.5091158\n",
      "Training [ 683/10000] ..........683 ) Loss= 0.30311164\n",
      "Training [ 684/10000] ..........684 ) Loss= 0.26951265\n",
      "Training [ 685/10000] ..........685 ) Loss= 0.4583192\n",
      "Training [ 686/10000] ..........686 ) Loss= 0.30083382\n",
      "Training [ 687/10000] ..........687 ) Loss= 0.53787065\n",
      "Training [ 688/10000] ..........688 ) Loss= 0.3369129\n",
      "Training [ 689/10000] ..........689 ) Loss= 0.6591508\n",
      "Training [ 690/10000] ..........690 ) Loss= 0.38184592\n",
      "Training [ 691/10000] ..........691 ) Loss= 0.47253665\n",
      "Training [ 692/10000] ..........692 ) Loss= 0.5011195\n",
      "Training [ 693/10000] ..........693 ) Loss= 0.36856145\n",
      "Training [ 694/10000] ..........694 ) Loss= 0.74630797\n",
      "Training [ 695/10000] ..........695 ) Loss= 0.32366222\n",
      "Training [ 696/10000] ..........696 ) Loss= 0.28324756\n",
      "Training [ 697/10000] ..........697 ) Loss= 0.39492637\n",
      "Training [ 698/10000] ..........698 ) Loss= 0.36751923\n",
      "Training [ 699/10000] ..........699 ) Loss= 0.31721306\n",
      "Training [ 700/10000] ..........700 ) Loss= 0.44392776\n",
      "Training [ 701/10000] ..........701 ) Loss= 0.4709316\n",
      "Training [ 702/10000] ..........702 ) Loss= 0.35795993\n",
      "Training [ 703/10000] ..........703 ) Loss= 0.47673634\n",
      "Training [ 704/10000] ..........704 ) Loss= 0.63076687\n",
      "Training [ 705/10000] ..........705 ) Loss= 0.33556232\n",
      "Training [ 706/10000] ..........706 ) Loss= 0.366634\n",
      "Training [ 707/10000] ..........707 ) Loss= 0.42932227\n",
      "Training [ 708/10000] ..........708 ) Loss= 0.34119707\n",
      "Training [ 709/10000] ..........709 ) Loss= 0.4020751\n",
      "Training [ 710/10000] ..........710 ) Loss= 0.36038992\n",
      "Training [ 711/10000] ..........711 ) Loss= 0.8697486\n",
      "Training [ 712/10000] ..........712 ) Loss= 0.37150255\n",
      "Training [ 713/10000] ..........713 ) Loss= 0.33712676\n",
      "Training [ 714/10000] ..........714 ) Loss= 0.6221804\n",
      "Training [ 715/10000] ..........715 ) Loss= 0.29806882\n",
      "Training [ 716/10000] ..........716 ) Loss= 0.34048596\n",
      "Training [ 717/10000] ..........717 ) Loss= 0.39378718\n",
      "Training [ 718/10000] ..........718 ) Loss= 0.4343278\n",
      "Training [ 719/10000] ..........719 ) Loss= 0.40251586\n",
      "Training [ 720/10000] ..........720 ) Loss= 0.37442553\n",
      "Training [ 721/10000] ..........721 ) Loss= 0.3420508\n",
      "Training [ 722/10000] ..........722 ) Loss= 0.39485076\n",
      "Training [ 723/10000] ..........723 ) Loss= 0.40579316\n",
      "Training [ 724/10000] ..........724 ) Loss= 0.29847577\n",
      "Training [ 725/10000] ..........725 ) Loss= 0.37645733\n",
      "Training [ 726/10000] ..........726 ) Loss= 0.49809688\n",
      "Training [ 727/10000] ..........727 ) Loss= 0.5145392\n",
      "Training [ 728/10000] ..........728 ) Loss= 0.47064194\n",
      "Training [ 729/10000] ..........729 ) Loss= 0.44009307\n",
      "Training [ 730/10000] ..........730 ) Loss= 0.27312666\n",
      "Training [ 731/10000] ..........731 ) Loss= 0.29697567\n",
      "Training [ 732/10000] ..........732 ) Loss= 0.34394372\n",
      "Training [ 733/10000] ..........733 ) Loss= 0.3478685\n",
      "Training [ 734/10000] ..........734 ) Loss= 0.31625098\n",
      "Training [ 735/10000] ..........735 ) Loss= 0.29544178\n",
      "Training [ 736/10000] ..........736 ) Loss= 0.43450525\n",
      "Training [ 737/10000] ..........737 ) Loss= 0.3436543\n",
      "Training [ 738/10000] ..........738 ) Loss= 0.27453953\n",
      "Training [ 739/10000] ..........739 ) Loss= 0.2849308\n",
      "Training [ 740/10000] ..........740 ) Loss= 0.40919322\n",
      "Training [ 741/10000] ..........741 ) Loss= 0.39755166\n",
      "Training [ 742/10000] ..........742 ) Loss= 0.3487177\n",
      "Training [ 743/10000] ..........743 ) Loss= 0.3351518\n",
      "Training [ 744/10000] ..........744 ) Loss= 0.3946499\n",
      "Training [ 745/10000] ..........745 ) Loss= 0.40975028\n",
      "Training [ 746/10000] ..........746 ) Loss= 0.34812784\n",
      "Training [ 747/10000] ..........747 ) Loss= 0.29869837\n",
      "Training [ 748/10000] ..........748 ) Loss= 0.35274363\n",
      "Training [ 749/10000] ..........749 ) Loss= 0.5394263\n",
      "Training [ 750/10000] ..........750 ) Loss= 0.39296198\n",
      "Training [ 751/10000] ..........751 ) Loss= 0.2745311\n",
      "Training [ 752/10000] ..........752 ) Loss= 0.43388042\n",
      "Training [ 753/10000] ..........753 ) Loss= 0.3225121\n",
      "Training [ 754/10000] ..........754 ) Loss= 0.64717406\n",
      "Training [ 755/10000] ..........755 ) Loss= 0.49711084\n",
      "Training [ 756/10000] ..........756 ) Loss= 0.3101722\n",
      "Training [ 757/10000] ..........757 ) Loss= 0.2847928\n",
      "Training [ 758/10000] ..........758 ) Loss= 0.3639158\n",
      "Training [ 759/10000] ..........759 ) Loss= 0.4178671\n",
      "Training [ 760/10000] ..........760 ) Loss= 0.30752385\n",
      "Training [ 761/10000] ..........761 ) Loss= 0.34218428\n",
      "Training [ 762/10000] ..........762 ) Loss= 0.3324565\n",
      "Training [ 763/10000] ..........763 ) Loss= 0.38441643\n",
      "Training [ 764/10000] ..........764 ) Loss= 0.37624267\n",
      "Training [ 765/10000] ..........765 ) Loss= 0.22665152\n",
      "Training [ 766/10000] ..........766 ) Loss= 0.36477345\n",
      "Training [ 767/10000] ..........767 ) Loss= 0.3000568\n",
      "Training [ 768/10000] ..........768 ) Loss= 0.30476797\n",
      "Training [ 769/10000] ..........769 ) Loss= 0.27583873\n",
      "Training [ 770/10000] ..........770 ) Loss= 0.2762039\n",
      "Training [ 771/10000] ..........771 ) Loss= 0.49856463\n",
      "Training [ 772/10000] ..........772 ) Loss= 0.39507946\n",
      "Training [ 773/10000] ..........773 ) Loss= 0.27185374\n",
      "Training [ 774/10000] ..........774 ) Loss= 0.5509698\n",
      "Training [ 775/10000] ..........775 ) Loss= 0.37466428\n",
      "Training [ 776/10000] ..........776 ) Loss= 0.2997955\n",
      "Training [ 777/10000] ..........777 ) Loss= 0.39461425\n",
      "Training [ 778/10000] ..........778 ) Loss= 0.4086979\n",
      "Training [ 779/10000] ..........779 ) Loss= 0.42021763\n",
      "Training [ 780/10000] ..........780 ) Loss= 0.38924724\n",
      "Training [ 781/10000] ..........781 ) Loss= 0.34060878\n",
      "Training [ 782/10000] ..........782 ) Loss= 0.32908878\n",
      "Training [ 783/10000] ..........783 ) Loss= 0.64095104\n",
      "Training [ 784/10000] ..........784 ) Loss= 0.27870414\n",
      "Training [ 785/10000] ..........785 ) Loss= 0.4639528\n",
      "Training [ 786/10000] ..........786 ) Loss= 0.43984053\n",
      "Training [ 787/10000] ..........787 ) Loss= 0.39705735\n",
      "Training [ 788/10000] ..........788 ) Loss= 0.37906393\n",
      "Training [ 789/10000] ..........789 ) Loss= 0.37112626\n",
      "Training [ 790/10000] ..........790 ) Loss= 0.44662246\n",
      "Training [ 791/10000] ..........791 ) Loss= 0.40017927\n",
      "Training [ 792/10000] ..........792 ) Loss= 0.3777548\n",
      "Training [ 793/10000] ..........793 ) Loss= 0.33553314\n",
      "Training [ 794/10000] ..........794 ) Loss= 0.4613249\n",
      "Training [ 795/10000] ..........795 ) Loss= 0.34197003\n",
      "Training [ 796/10000] ..........796 ) Loss= 0.25871673\n",
      "Training [ 797/10000] ..........797 ) Loss= 0.6500095\n",
      "Training [ 798/10000] ..........798 ) Loss= 0.40685663\n",
      "Training [ 799/10000] ..........799 ) Loss= 0.24421328\n",
      "Training [ 800/10000] ..........800 ) Loss= 0.3378879\n",
      "Training [ 801/10000] ..........801 ) Loss= 0.29327095\n",
      "Training [ 802/10000] ..........802 ) Loss= 0.34132928\n",
      "Training [ 803/10000] ..........803 ) Loss= 0.35036987\n",
      "Training [ 804/10000] ..........804 ) Loss= 0.3599742\n",
      "Training [ 805/10000] ..........805 ) Loss= 0.26966995\n",
      "Training [ 806/10000] ..........806 ) Loss= 0.33150828\n",
      "Training [ 807/10000] ..........807 ) Loss= 0.5111372\n",
      "Training [ 808/10000] ..........808 ) Loss= 0.3514036\n",
      "Training [ 809/10000] ..........809 ) Loss= 0.37914726\n",
      "Training [ 810/10000] ..........810 ) Loss= 0.3101634\n",
      "Training [ 811/10000] ..........811 ) Loss= 0.2927672\n",
      "Training [ 812/10000] ..........812 ) Loss= 0.44692138\n",
      "Training [ 813/10000] ..........813 ) Loss= 0.40768543\n",
      "Training [ 814/10000] ..........814 ) Loss= 0.5415425\n",
      "Training [ 815/10000] ..........815 ) Loss= 0.3956392\n",
      "Training [ 816/10000] ..........816 ) Loss= 0.289081\n",
      "Training [ 817/10000] ..........817 ) Loss= 0.606511\n",
      "Training [ 818/10000] ..........818 ) Loss= 0.42115462\n",
      "Training [ 819/10000] ..........819 ) Loss= 0.4274886\n",
      "Training [ 820/10000] ..........820 ) Loss= 0.28600225\n",
      "Training [ 821/10000] ..........821 ) Loss= 0.38452467\n",
      "Training [ 822/10000] ..........822 ) Loss= 0.3285414\n",
      "Training [ 823/10000] ..........823 ) Loss= 0.27320796\n",
      "Training [ 824/10000] ..........824 ) Loss= 0.281338\n",
      "Training [ 825/10000] ..........825 ) Loss= 0.3052869\n",
      "Training [ 826/10000] ..........826 ) Loss= 0.22643971\n",
      "Training [ 827/10000] ..........827 ) Loss= 0.33942205\n",
      "Training [ 828/10000] ..........828 ) Loss= 0.72982967\n",
      "Training [ 829/10000] ..........829 ) Loss= 0.37420377\n",
      "Training [ 830/10000] ..........830 ) Loss= 0.3844815\n",
      "Training [ 831/10000] ..........831 ) Loss= 0.31261232\n",
      "Training [ 832/10000] ..........832 ) Loss= 0.4667483\n",
      "Training [ 833/10000] ..........833 ) Loss= 0.34913832\n",
      "Training [ 834/10000] ..........834 ) Loss= 0.35677847\n",
      "Training [ 835/10000] ..........835 ) Loss= 0.3051717\n",
      "Training [ 836/10000] ..........836 ) Loss= 0.32047656\n",
      "Training [ 837/10000] ..........837 ) Loss= 0.43321455\n",
      "Training [ 838/10000] ..........838 ) Loss= 0.29635775\n",
      "Training [ 839/10000] ..........839 ) Loss= 0.44406253\n",
      "Training [ 840/10000] ..........840 ) Loss= 0.2630069\n",
      "Training [ 841/10000] ..........841 ) Loss= 0.44891158\n",
      "Training [ 842/10000] ..........842 ) Loss= 0.32431734\n",
      "Training [ 843/10000] ..........843 ) Loss= 0.57285184\n",
      "Training [ 844/10000] ..........844 ) Loss= 0.3390985\n",
      "Training [ 845/10000] ..........845 ) Loss= 0.3180262\n",
      "Training [ 846/10000] ..........846 ) Loss= 0.25301954\n",
      "Training [ 847/10000] ..........847 ) Loss= 0.5039377\n",
      "Training [ 848/10000] ..........848 ) Loss= 0.27869073\n",
      "Training [ 849/10000] ..........849 ) Loss= 0.41894647\n",
      "Training [ 850/10000] ..........850 ) Loss= 0.37432328\n",
      "Training [ 851/10000] ..........851 ) Loss= 0.38804114\n",
      "Training [ 852/10000] ..........852 ) Loss= 0.4162758\n",
      "Training [ 853/10000] ..........853 ) Loss= 0.51605266\n",
      "Training [ 854/10000] ..........854 ) Loss= 0.2699419\n",
      "Training [ 855/10000] ..........855 ) Loss= 0.34563524\n",
      "Training [ 856/10000] ..........856 ) Loss= 0.2546721\n",
      "Training [ 857/10000] ..........857 ) Loss= 0.28982642\n",
      "Training [ 858/10000] ..........858 ) Loss= 0.39520198\n",
      "Training [ 859/10000] ..........859 ) Loss= 0.38226432\n",
      "Training [ 860/10000] ..........860 ) Loss= 0.30118647\n",
      "Training [ 861/10000] ..........861 ) Loss= 0.32852268\n",
      "Training [ 862/10000] ..........862 ) Loss= 0.30303574\n",
      "Training [ 863/10000] ..........863 ) Loss= 0.35100746\n",
      "Training [ 864/10000] ..........864 ) Loss= 0.24928467\n",
      "Training [ 865/10000] ..........865 ) Loss= 0.42656434\n",
      "Training [ 866/10000] ..........866 ) Loss= 0.48364082\n",
      "Training [ 867/10000] ..........867 ) Loss= 0.43764108\n",
      "Training [ 868/10000] ..........868 ) Loss= 0.29450718\n",
      "Training [ 869/10000] ..........869 ) Loss= 0.5169604\n",
      "Training [ 870/10000] ..........870 ) Loss= 0.35671967\n",
      "Training [ 871/10000] ..........871 ) Loss= 0.33126006\n",
      "Training [ 872/10000] ..........872 ) Loss= 0.31006458\n",
      "Training [ 873/10000] ..........873 ) Loss= 0.42292482\n",
      "Training [ 874/10000] ..........874 ) Loss= 0.5827091\n",
      "Training [ 875/10000] ..........875 ) Loss= 0.29607716\n",
      "Training [ 876/10000] ..........876 ) Loss= 0.42930397\n",
      "Training [ 877/10000] ..........877 ) Loss= 0.26657102\n",
      "Training [ 878/10000] ..........878 ) Loss= 0.37729725\n",
      "Training [ 879/10000] ..........879 ) Loss= 0.26687017\n",
      "Training [ 880/10000] ..........880 ) Loss= 0.45004815\n",
      "Training [ 881/10000] ..........881 ) Loss= 0.55488044\n",
      "Training [ 882/10000] ..........882 ) Loss= 0.29635662\n",
      "Training [ 883/10000] ..........883 ) Loss= 0.24264649\n",
      "Training [ 884/10000] ..........884 ) Loss= 0.32243463\n",
      "Training [ 885/10000] ..........885 ) Loss= 0.33842173\n",
      "Training [ 886/10000] ..........886 ) Loss= 0.40739825\n",
      "Training [ 887/10000] ..........887 ) Loss= 0.4346546\n",
      "Training [ 888/10000] ..........888 ) Loss= 0.6146311\n",
      "Training [ 889/10000] ..........889 ) Loss= 0.3264752\n",
      "Training [ 890/10000] ..........890 ) Loss= 0.32883698\n",
      "Training [ 891/10000] ..........891 ) Loss= 0.4951639\n",
      "Training [ 892/10000] ..........892 ) Loss= 0.2513246\n",
      "Training [ 893/10000] ..........893 ) Loss= 0.37860042\n",
      "Training [ 894/10000] ..........894 ) Loss= 0.4350871\n",
      "Training [ 895/10000] ..........895 ) Loss= 0.26617444\n",
      "Training [ 896/10000] ..........896 ) Loss= 0.30723676\n",
      "Training [ 897/10000] ..........897 ) Loss= 0.3842691\n",
      "Training [ 898/10000] ..........898 ) Loss= 0.35442019\n",
      "Training [ 899/10000] ..........899 ) Loss= 0.33979023\n",
      "Training [ 900/10000] ..........900 ) Loss= 0.37948895\n",
      "Training [ 901/10000] ..........901 ) Loss= 0.44805676\n",
      "Training [ 902/10000] ..........902 ) Loss= 0.28211188\n",
      "Training [ 903/10000] ..........903 ) Loss= 0.30986795\n",
      "Training [ 904/10000] ..........904 ) Loss= 0.3755577\n",
      "Training [ 905/10000] ..........905 ) Loss= 0.27233192\n",
      "Training [ 906/10000] ..........906 ) Loss= 0.25009796\n",
      "Training [ 907/10000] ..........907 ) Loss= 0.50986314\n",
      "Training [ 908/10000] ..........908 ) Loss= 0.29596084\n",
      "Training [ 909/10000] ..........909 ) Loss= 0.35933593\n",
      "Training [ 910/10000] ..........910 ) Loss= 0.54015553\n",
      "Training [ 911/10000] ..........911 ) Loss= 0.3291013\n",
      "Training [ 912/10000] ..........912 ) Loss= 0.26413375\n",
      "Training [ 913/10000] ..........913 ) Loss= 0.36730018\n",
      "Training [ 914/10000] ..........914 ) Loss= 0.38391092\n",
      "Training [ 915/10000] ..........915 ) Loss= 0.29364038\n",
      "Training [ 916/10000] ..........916 ) Loss= 0.25292578\n",
      "Training [ 917/10000] ..........917 ) Loss= 0.40102696\n",
      "Training [ 918/10000] ..........918 ) Loss= 0.30698383\n",
      "Training [ 919/10000] ..........919 ) Loss= 0.32822835\n",
      "Training [ 920/10000] ..........920 ) Loss= 0.24930221\n",
      "Training [ 921/10000] ..........921 ) Loss= 0.4804671\n",
      "Training [ 922/10000] ..........922 ) Loss= 0.44948152\n",
      "Training [ 923/10000] ..........923 ) Loss= 0.35162503\n",
      "Training [ 924/10000] ..........924 ) Loss= 0.6146213\n",
      "Training [ 925/10000] ..........925 ) Loss= 0.27453542\n",
      "Training [ 926/10000] ..........926 ) Loss= 0.48195082\n",
      "Training [ 927/10000] ..........927 ) Loss= 0.4023871\n",
      "Training [ 928/10000] ..........928 ) Loss= 0.35794017\n",
      "Training [ 929/10000] ..........929 ) Loss= 0.30574808\n",
      "Training [ 930/10000] ..........930 ) Loss= 0.4408942\n",
      "Training [ 931/10000] ..........931 ) Loss= 0.24670893\n",
      "Training [ 932/10000] ..........932 ) Loss= 0.27306804\n",
      "Training [ 933/10000] ..........933 ) Loss= 0.24985766\n",
      "Training [ 934/10000] ..........934 ) Loss= 0.25278774\n",
      "Training [ 935/10000] ..........935 ) Loss= 0.26985094\n",
      "Training [ 936/10000] ..........936 ) Loss= 0.6472233\n",
      "Training [ 937/10000] ..........937 ) Loss= 0.31842813\n",
      "Training [ 938/10000] ..........938 ) Loss= 0.33244935\n",
      "Training [ 939/10000] ..........939 ) Loss= 0.2976242\n",
      "Training [ 940/10000] ..........940 ) Loss= 0.55496424\n",
      "Training [ 941/10000] ..........941 ) Loss= 0.3002665\n",
      "Training [ 942/10000] ..........942 ) Loss= 0.3396189\n",
      "Training [ 943/10000] ..........943 ) Loss= 0.37237605\n",
      "Training [ 944/10000] ..........944 ) Loss= 0.27217168\n",
      "Training [ 945/10000] ..........945 ) Loss= 0.33168247\n",
      "Training [ 946/10000] ..........946 ) Loss= 0.42511693\n",
      "Training [ 947/10000] ..........947 ) Loss= 0.379472\n",
      "Training [ 948/10000] ..........948 ) Loss= 0.42688337\n",
      "Training [ 949/10000] ..........949 ) Loss= 0.50401676\n",
      "Training [ 950/10000] ..........950 ) Loss= 0.35281023\n",
      "Training [ 951/10000] ..........951 ) Loss= 0.31844258\n",
      "Training [ 952/10000] ..........952 ) Loss= 0.37485212\n",
      "Training [ 953/10000] ..........953 ) Loss= 0.31300303\n",
      "Training [ 954/10000] ..........954 ) Loss= 0.24884789\n",
      "Training [ 955/10000] ..........955 ) Loss= 0.30401185\n",
      "Training [ 956/10000] ..........956 ) Loss= 0.47064272\n",
      "Training [ 957/10000] ..........957 ) Loss= 0.2807979\n",
      "Training [ 958/10000] ..........958 ) Loss= 0.23719697\n",
      "Training [ 959/10000] ..........959 ) Loss= 0.31133467\n",
      "Training [ 960/10000] ..........960 ) Loss= 0.32971483\n",
      "Training [ 961/10000] ..........961 ) Loss= 0.56635094\n",
      "Training [ 962/10000] ..........962 ) Loss= 0.42823857\n",
      "Training [ 963/10000] ..........963 ) Loss= 0.3328811\n",
      "Training [ 964/10000] ..........964 ) Loss= 0.490335\n",
      "Training [ 965/10000] ..........965 ) Loss= 0.317648\n",
      "Training [ 966/10000] ..........966 ) Loss= 0.41703147\n",
      "Training [ 967/10000] ..........967 ) Loss= 0.4697872\n",
      "Training [ 968/10000] ..........968 ) Loss= 0.41044655\n",
      "Training [ 969/10000] ..........969 ) Loss= 0.61265725\n",
      "Training [ 970/10000] ..........970 ) Loss= 0.2641841\n",
      "Training [ 971/10000] ..........971 ) Loss= 0.2760613\n",
      "Training [ 972/10000] ..........972 ) Loss= 0.33515608\n",
      "Training [ 973/10000] ..........973 ) Loss= 0.20827179\n",
      "Training [ 974/10000] ..........974 ) Loss= 0.31587198\n",
      "Training [ 975/10000] ..........975 ) Loss= 0.3116551\n",
      "Training [ 976/10000] ..........976 ) Loss= 0.4349606\n",
      "Training [ 977/10000] ..........977 ) Loss= 0.42102557\n",
      "Training [ 978/10000] ..........978 ) Loss= 0.48081836\n",
      "Training [ 979/10000] ..........979 ) Loss= 0.8647844\n",
      "Training [ 980/10000] ..........980 ) Loss= 0.2368983\n",
      "Training [ 981/10000] ..........981 ) Loss= 0.36224005\n",
      "Training [ 982/10000] ..........982 ) Loss= 0.35255277\n",
      "Training [ 983/10000] ..........983 ) Loss= 0.29551896\n",
      "Training [ 984/10000] ..........984 ) Loss= 0.27166486\n",
      "Training [ 985/10000] ..........985 ) Loss= 0.37593246\n",
      "Training [ 986/10000] ..........986 ) Loss= 0.32051206\n",
      "Training [ 987/10000] ..........987 ) Loss= 0.4828439\n",
      "Training [ 988/10000] ..........988 ) Loss= 0.32171145\n",
      "Training [ 989/10000] ..........989 ) Loss= 0.34708804\n",
      "Training [ 990/10000] ..........990 ) Loss= 0.33987388\n",
      "Training [ 991/10000] ..........991 ) Loss= 0.27178916\n",
      "Training [ 992/10000] ..........992 ) Loss= 0.26713926\n",
      "Training [ 993/10000] ..........993 ) Loss= 0.61113113\n",
      "Training [ 994/10000] ..........994 ) Loss= 0.27963254\n",
      "Training [ 995/10000] ..........995 ) Loss= 0.2903621\n",
      "Training [ 996/10000] ..........996 ) Loss= 0.27376145\n",
      "Training [ 997/10000] ..........997 ) Loss= 0.33160365\n",
      "Training [ 998/10000] ..........998 ) Loss= 0.2872185\n",
      "Training [ 999/10000] ..........999 ) Loss= 0.43087325\n",
      "Training [1000/10000] ..........1000 ) Loss= 0.26296267 - Saving Model1000.torch\n",
      "Training [1001/10000] ..........1001 ) Loss= 0.5379751\n",
      "Training [1002/10000] ..........1002 ) Loss= 0.283492\n",
      "Training [1003/10000] ..........1003 ) Loss= 0.50100356\n",
      "Training [1004/10000] ..........1004 ) Loss= 0.5919393\n",
      "Training [1005/10000] ..........1005 ) Loss= 0.2872865\n",
      "Training [1006/10000] ..........1006 ) Loss= 0.3446077\n",
      "Training [1007/10000] ..........1007 ) Loss= 0.37338424\n",
      "Training [1008/10000] ..........1008 ) Loss= 0.21582891\n",
      "Training [1009/10000] ..........1009 ) Loss= 0.30579463\n",
      "Training [1010/10000] ..........1010 ) Loss= 0.2556077\n",
      "Training [1011/10000] ..........1011 ) Loss= 0.26792657\n",
      "Training [1012/10000] ..........1012 ) Loss= 0.30952442\n",
      "Training [1013/10000] ..........1013 ) Loss= 0.28812462\n",
      "Training [1014/10000] ..........1014 ) Loss= 0.2558337\n",
      "Training [1015/10000] ..........1015 ) Loss= 0.29926497\n",
      "Training [1016/10000] ..........1016 ) Loss= 0.3297013\n",
      "Training [1017/10000] ..........1017 ) Loss= 0.29244265\n",
      "Training [1018/10000] ..........1018 ) Loss= 0.3504033\n",
      "Training [1019/10000] ..........1019 ) Loss= 0.36004385\n",
      "Training [1020/10000] ..........1020 ) Loss= 0.47810555\n",
      "Training [1021/10000] ..........1021 ) Loss= 0.6441394\n",
      "Training [1022/10000] ..........1022 ) Loss= 0.23425351\n",
      "Training [1023/10000] ..........1023 ) Loss= 0.47802907\n",
      "Training [1024/10000] ..........1024 ) Loss= 0.3470087\n",
      "Training [1025/10000] ..........1025 ) Loss= 0.5020427\n",
      "Training [1026/10000] ..........1026 ) Loss= 0.32071388\n",
      "Training [1027/10000] ..........1027 ) Loss= 0.43177375\n",
      "Training [1028/10000] ..........1028 ) Loss= 0.4809461\n",
      "Training [1029/10000] ..........1029 ) Loss= 0.2706537\n",
      "Training [1030/10000] ..........1030 ) Loss= 0.4347709\n",
      "Training [1031/10000] ..........1031 ) Loss= 0.27487645\n",
      "Training [1032/10000] ..........1032 ) Loss= 0.46954432\n",
      "Training [1033/10000] ..........1033 ) Loss= 0.2923135\n",
      "Training [1034/10000] ..........1034 ) Loss= 0.30629563\n",
      "Training [1035/10000] ..........1035 ) Loss= 0.40688604\n",
      "Training [1036/10000] ..........1036 ) Loss= 0.2842305\n",
      "Training [1037/10000] ..........1037 ) Loss= 0.5547968\n",
      "Training [1038/10000] ..........1038 ) Loss= 0.4045174\n",
      "Training [1039/10000] ..........1039 ) Loss= 0.27413067\n",
      "Training [1040/10000] ..........1040 ) Loss= 0.5309503\n",
      "Training [1041/10000] ..........1041 ) Loss= 0.25248018\n",
      "Training [1042/10000] ..........1042 ) Loss= 0.43671653\n",
      "Training [1043/10000] ..........1043 ) Loss= 0.42337987\n",
      "Training [1044/10000] ..........1044 ) Loss= 0.24852341\n",
      "Training [1045/10000] ..........1045 ) Loss= 0.62399447\n",
      "Training [1046/10000] ..........1046 ) Loss= 0.30877566\n",
      "Training [1047/10000] ..........1047 ) Loss= 0.2337864\n",
      "Training [1048/10000] ..........1048 ) Loss= 0.3151157\n",
      "Training [1049/10000] ..........1049 ) Loss= 0.5076438\n",
      "Training [1050/10000] ..........1050 ) Loss= 0.20619486\n",
      "Training [1051/10000] ..........1051 ) Loss= 0.30748138\n",
      "Training [1052/10000] ..........1052 ) Loss= 0.7906168\n",
      "Training [1053/10000] ..........1053 ) Loss= 0.24851467\n",
      "Training [1054/10000] ..........1054 ) Loss= 0.3532747\n",
      "Training [1055/10000] ..........1055 ) Loss= 0.31940287\n",
      "Training [1056/10000] ..........1056 ) Loss= 0.240843\n",
      "Training [1057/10000] ..........1057 ) Loss= 0.22195622\n",
      "Training [1058/10000] ..........1058 ) Loss= 0.32184374\n",
      "Training [1059/10000] ..........1059 ) Loss= 0.29698122\n",
      "Training [1060/10000] ..........1060 ) Loss= 0.26801604\n",
      "Training [1061/10000] ..........1061 ) Loss= 0.2149946\n",
      "Training [1062/10000] ..........1062 ) Loss= 0.40064886\n",
      "Training [1063/10000] ..........1063 ) Loss= 0.47323558\n",
      "Training [1064/10000] ..........1064 ) Loss= 0.2617082\n",
      "Training [1065/10000] ..........1065 ) Loss= 0.28747284\n",
      "Training [1066/10000] ..........1066 ) Loss= 0.42403656\n",
      "Training [1067/10000] ..........1067 ) Loss= 0.30017388\n",
      "Training [1068/10000] ..........1068 ) Loss= 0.3502392\n",
      "Training [1069/10000] ..........1069 ) Loss= 0.29261944\n",
      "Training [1070/10000] ..........1070 ) Loss= 0.28721067\n",
      "Training [1071/10000] ..........1071 ) Loss= 0.27432618\n",
      "Training [1072/10000] ..........1072 ) Loss= 0.365512\n",
      "Training [1073/10000] ..........1073 ) Loss= 0.2898514\n",
      "Training [1074/10000] ..........1074 ) Loss= 0.4921229\n",
      "Training [1075/10000] ..........1075 ) Loss= 0.35828242\n",
      "Training [1076/10000] ..........1076 ) Loss= 0.4085464\n",
      "Training [1077/10000] ..........1077 ) Loss= 0.29225886\n",
      "Training [1078/10000] ..........1078 ) Loss= 0.2672946\n",
      "Training [1079/10000] ..........1079 ) Loss= 0.2612899\n",
      "Training [1080/10000] ..........1080 ) Loss= 0.33914176\n",
      "Training [1081/10000] ..........1081 ) Loss= 0.24131343\n",
      "Training [1082/10000] ..........1082 ) Loss= 0.38046092\n",
      "Training [1083/10000] ..........1083 ) Loss= 0.27757916\n",
      "Training [1084/10000] ..........1084 ) Loss= 0.30502746\n",
      "Training [1085/10000] ..........1085 ) Loss= 0.23937751\n",
      "Training [1086/10000] ..........1086 ) Loss= 0.3264577\n",
      "Training [1087/10000] ..........1087 ) Loss= 0.41982937\n",
      "Training [1088/10000] ..........1088 ) Loss= 0.30826038\n",
      "Training [1089/10000] ..........1089 ) Loss= 0.2611899\n",
      "Training [1090/10000] ..........1090 ) Loss= 0.26576808\n",
      "Training [1091/10000] ..........1091 ) Loss= 0.3154643\n",
      "Training [1092/10000] ..........1092 ) Loss= 0.27076456\n",
      "Training [1093/10000] ..........1093 ) Loss= 0.28944543\n",
      "Training [1094/10000] ..........1094 ) Loss= 0.34341767\n",
      "Training [1095/10000] ..........1095 ) Loss= 0.3134262\n",
      "Training [1096/10000] ..........1096 ) Loss= 0.3649658\n",
      "Training [1097/10000] ..........1097 ) Loss= 0.2539845\n",
      "Training [1098/10000] ..........1098 ) Loss= 0.3970929\n",
      "Training [1099/10000] ..........1099 ) Loss= 0.3177934\n",
      "Training [1100/10000] ..........1100 ) Loss= 0.26009378\n",
      "Training [1101/10000] ..........1101 ) Loss= 0.25206065\n",
      "Training [1102/10000] ..........1102 ) Loss= 0.23385508\n",
      "Training [1103/10000] ..........1103 ) Loss= 0.45465267\n",
      "Training [1104/10000] ..........1104 ) Loss= 0.35383347\n",
      "Training [1105/10000] ..........1105 ) Loss= 0.27399382\n",
      "Training [1106/10000] ..........1106 ) Loss= 0.20225085\n",
      "Training [1107/10000] ..........1107 ) Loss= 0.27692604\n",
      "Training [1108/10000] ..........1108 ) Loss= 0.39938766\n",
      "Training [1109/10000] ..........1109 ) Loss= 0.36255237\n",
      "Training [1110/10000] ..........1110 ) Loss= 0.51527303\n",
      "Training [1111/10000] ..........1111 ) Loss= 0.24688584\n",
      "Training [1112/10000] ..........1112 ) Loss= 0.33730787\n",
      "Training [1113/10000] ..........1113 ) Loss= 0.2702091\n",
      "Training [1114/10000] ..........1114 ) Loss= 0.38287115\n",
      "Training [1115/10000] ..........1115 ) Loss= 0.23181354\n",
      "Training [1116/10000] ..........1116 ) Loss= 0.28354195\n",
      "Training [1117/10000] ..........1117 ) Loss= 0.37856886\n",
      "Training [1118/10000] ..........1118 ) Loss= 0.47678655\n",
      "Training [1119/10000] ..........1119 ) Loss= 0.24484535\n",
      "Training [1120/10000] ..........1120 ) Loss= 0.28875992\n",
      "Training [1121/10000] ..........1121 ) Loss= 0.3922137\n",
      "Training [1122/10000] ..........1122 ) Loss= 0.2925065\n",
      "Training [1123/10000] ..........1123 ) Loss= 0.34409934\n",
      "Training [1124/10000] ..........1124 ) Loss= 0.31116596\n",
      "Training [1125/10000] ..........1125 ) Loss= 0.31133813\n",
      "Training [1126/10000] ..........1126 ) Loss= 0.35197937\n",
      "Training [1127/10000] ..........1127 ) Loss= 0.26136974\n",
      "Training [1128/10000] ..........1128 ) Loss= 0.24113226\n",
      "Training [1129/10000] ..........1129 ) Loss= 0.34267032\n",
      "Training [1130/10000] ..........1130 ) Loss= 0.3704156\n",
      "Training [1131/10000] ..........1131 ) Loss= 0.4877325\n",
      "Training [1132/10000] ..........1132 ) Loss= 0.26304746\n",
      "Training [1133/10000] ..........1133 ) Loss= 0.2725681\n",
      "Training [1134/10000] ..........1134 ) Loss= 0.27511665\n",
      "Training [1135/10000] ..........1135 ) Loss= 0.2512148\n",
      "Training [1136/10000] ..........1136 ) Loss= 0.2823379\n",
      "Training [1137/10000] ..........1137 ) Loss= 0.27138993\n",
      "Training [1138/10000] ..........1138 ) Loss= 0.32954666\n",
      "Training [1139/10000] ..........1139 ) Loss= 0.21710609\n",
      "Training [1140/10000] ..........1140 ) Loss= 0.37897307\n",
      "Training [1141/10000] ..........1141 ) Loss= 0.23241682\n",
      "Training [1142/10000] ..........1142 ) Loss= 0.23176278\n",
      "Training [1143/10000] ..........1143 ) Loss= 0.20757736\n",
      "Training [1144/10000] ..........1144 ) Loss= 0.3001614\n",
      "Training [1145/10000] ..........1145 ) Loss= 0.2760646\n",
      "Training [1146/10000] ..........1146 ) Loss= 0.40678003\n",
      "Training [1147/10000] ..........1147 ) Loss= 0.26004708\n",
      "Training [1148/10000] ..........1148 ) Loss= 0.23772387\n",
      "Training [1149/10000] ..........1149 ) Loss= 0.3109461\n",
      "Training [1150/10000] ..........1150 ) Loss= 0.3068981\n",
      "Training [1151/10000] ..........1151 ) Loss= 0.44168568\n",
      "Training [1152/10000] ..........1152 ) Loss= 0.23671359\n",
      "Training [1153/10000] ..........1153 ) Loss= 0.19641678\n",
      "Training [1154/10000] ..........1154 ) Loss= 0.30240735\n",
      "Training [1155/10000] ..........1155 ) Loss= 0.35496557\n",
      "Training [1156/10000] ..........1156 ) Loss= 0.55635315\n",
      "Training [1157/10000] ..........1157 ) Loss= 0.27349287\n",
      "Training [1158/10000] ..........1158 ) Loss= 0.33931127\n",
      "Training [1159/10000] ..........1159 ) Loss= 0.48940286\n",
      "Training [1160/10000] ..........1160 ) Loss= 0.18496206\n",
      "Training [1161/10000] ..........1161 ) Loss= 0.2854883\n",
      "Training [1162/10000] ..........1162 ) Loss= 0.32882598\n",
      "Training [1163/10000] ..........1163 ) Loss= 0.3456431\n",
      "Training [1164/10000] ..........1164 ) Loss= 0.31860912\n",
      "Training [1165/10000] ..........1165 ) Loss= 0.34506857\n",
      "Training [1166/10000] ..........1166 ) Loss= 0.31241933\n",
      "Training [1167/10000] ..........1167 ) Loss= 0.26270202\n",
      "Training [1168/10000] ..........1168 ) Loss= 0.3099358\n",
      "Training [1169/10000] ..........1169 ) Loss= 0.31342956\n",
      "Training [1170/10000] ..........1170 ) Loss= 0.35429803\n",
      "Training [1171/10000] ..........1171 ) Loss= 0.2779399\n",
      "Training [1172/10000] ..........1172 ) Loss= 0.24836402\n",
      "Training [1173/10000] ..........1173 ) Loss= 0.34144166\n",
      "Training [1174/10000] ..........1174 ) Loss= 0.23774025\n",
      "Training [1175/10000] ..........1175 ) Loss= 0.27967006\n",
      "Training [1176/10000] ..........1176 ) Loss= 0.19253744\n",
      "Training [1177/10000] ..........1177 ) Loss= 0.27066088\n",
      "Training [1178/10000] ..........1178 ) Loss= 0.22140475\n",
      "Training [1179/10000] ..........1179 ) Loss= 0.2652581\n",
      "Training [1180/10000] ..........1180 ) Loss= 0.27416533\n",
      "Training [1181/10000] ..........1181 ) Loss= 0.26455548\n",
      "Training [1182/10000] ..........1182 ) Loss= 0.30013525\n",
      "Training [1183/10000] ..........1183 ) Loss= 0.23490557\n",
      "Training [1184/10000] ..........1184 ) Loss= 0.25174534\n",
      "Training [1185/10000] ..........1185 ) Loss= 0.23396246\n",
      "Training [1186/10000] ..........1186 ) Loss= 0.27736744\n",
      "Training [1187/10000] ..........1187 ) Loss= 0.19484095\n",
      "Training [1188/10000] ..........1188 ) Loss= 0.27130237\n",
      "Training [1189/10000] ..........1189 ) Loss= 0.38649386\n",
      "Training [1190/10000] ..........1190 ) Loss= 0.34175482\n",
      "Training [1191/10000] ..........1191 ) Loss= 0.30082962\n",
      "Training [1192/10000] ..........1192 ) Loss= 0.27285808\n",
      "Training [1193/10000] ..........1193 ) Loss= 0.48483017\n",
      "Training [1194/10000] ..........1194 ) Loss= 0.26858792\n",
      "Training [1195/10000] ..........1195 ) Loss= 0.3135279\n",
      "Training [1196/10000] ..........1196 ) Loss= 0.2880331\n",
      "Training [1197/10000] ..........1197 ) Loss= 0.26346645\n",
      "Training [1198/10000] ..........1198 ) Loss= 0.31370986\n",
      "Training [1199/10000] ..........1199 ) Loss= 0.39406824\n",
      "Training [1200/10000] ..........1200 ) Loss= 0.4683368\n",
      "Training [1201/10000] ..........1201 ) Loss= 0.27748826\n",
      "Training [1202/10000] ..........1202 ) Loss= 0.23299251\n",
      "Training [1203/10000] ..........1203 ) Loss= 0.20257476\n",
      "Training [1204/10000] ..........1204 ) Loss= 0.24142376\n",
      "Training [1205/10000] ..........1205 ) Loss= 0.27045423\n",
      "Training [1206/10000] ..........1206 ) Loss= 0.2989197\n",
      "Training [1207/10000] ..........1207 ) Loss= 0.2502021\n",
      "Training [1208/10000] ..........1208 ) Loss= 0.2673187\n",
      "Training [1209/10000] ..........1209 ) Loss= 0.486554\n",
      "Training [1210/10000] ..........1210 ) Loss= 0.23723684\n",
      "Training [1211/10000] ..........1211 ) Loss= 0.22952153\n",
      "Training [1212/10000] ..........1212 ) Loss= 0.21168156\n",
      "Training [1213/10000] ..........1213 ) Loss= 0.2058039\n",
      "Training [1214/10000] ..........1214 ) Loss= 0.37469354\n",
      "Training [1215/10000] ..........1215 ) Loss= 0.2427384\n",
      "Training [1216/10000] ..........1216 ) Loss= 0.22305423\n",
      "Training [1217/10000] ..........1217 ) Loss= 0.42813006\n",
      "Training [1218/10000] ..........1218 ) Loss= 0.42997172\n",
      "Training [1219/10000] ..........1219 ) Loss= 0.1935848\n",
      "Training [1220/10000] ..........1220 ) Loss= 0.20390199\n",
      "Training [1221/10000] ..........1221 ) Loss= 0.2350122\n",
      "Training [1222/10000] ..........1222 ) Loss= 0.3423598\n",
      "Training [1223/10000] ..........1223 ) Loss= 0.3456737\n",
      "Training [1224/10000] ..........1224 ) Loss= 0.3859449\n",
      "Training [1225/10000] ..........1225 ) Loss= 0.25980398\n",
      "Training [1226/10000] ..........1226 ) Loss= 0.24536608\n",
      "Training [1227/10000] ..........1227 ) Loss= 0.25218445\n",
      "Training [1228/10000] ..........1228 ) Loss= 0.22481614\n",
      "Training [1229/10000] ..........1229 ) Loss= 0.30213758\n",
      "Training [1230/10000] ..........1230 ) Loss= 0.4133772\n",
      "Training [1231/10000] ..........1231 ) Loss= 0.22398283\n",
      "Training [1232/10000] ..........1232 ) Loss= 0.34005857\n",
      "Training [1233/10000] ..........1233 ) Loss= 0.38372803\n",
      "Training [1234/10000] ..........1234 ) Loss= 0.9373371\n",
      "Training [1235/10000] ..........1235 ) Loss= 0.19803356\n",
      "Training [1236/10000] ..........1236 ) Loss= 0.24117029\n",
      "Training [1237/10000] ..........1237 ) Loss= 0.6150137\n",
      "Training [1238/10000] ..........1238 ) Loss= 0.36815563\n",
      "Training [1239/10000] ..........1239 ) Loss= 0.30720326\n",
      "Training [1240/10000] ..........1240 ) Loss= 0.23062564\n",
      "Training [1241/10000] ..........1241 ) Loss= 0.282858\n",
      "Training [1242/10000] ..........1242 ) Loss= 0.38553792\n",
      "Training [1243/10000] ..........1243 ) Loss= 0.37950397\n",
      "Training [1244/10000] ..........1244 ) Loss= 0.5080088\n",
      "Training [1245/10000] ..........1245 ) Loss= 0.2896476\n",
      "Training [1246/10000] ..........1246 ) Loss= 0.2893999\n",
      "Training [1247/10000] ..........1247 ) Loss= 0.34705374\n",
      "Training [1248/10000] ..........1248 ) Loss= 0.6941264\n",
      "Training [1249/10000] ..........1249 ) Loss= 0.2576596\n",
      "Training [1250/10000] ..........1250 ) Loss= 0.34414697\n",
      "Training [1251/10000] ..........1251 ) Loss= 0.34177953\n",
      "Training [1252/10000] ..........1252 ) Loss= 0.25183815\n",
      "Training [1253/10000] ..........1253 ) Loss= 0.44625676\n",
      "Training [1254/10000] ..........1254 ) Loss= 0.21182759\n",
      "Training [1255/10000] ..........1255 ) Loss= 0.34602937\n",
      "Training [1256/10000] ..........1256 ) Loss= 0.53902733\n",
      "Training [1257/10000] ..........1257 ) Loss= 0.18054804\n",
      "Training [1258/10000] ..........1258 ) Loss= 0.31027335\n",
      "Training [1259/10000] ..........1259 ) Loss= 0.21616983\n",
      "Training [1260/10000] ..........1260 ) Loss= 0.4547084\n",
      "Training [1261/10000] ..........1261 ) Loss= 0.28468496\n",
      "Training [1262/10000] ..........1262 ) Loss= 0.25425994\n",
      "Training [1263/10000] ..........1263 ) Loss= 0.40414247\n",
      "Training [1264/10000] ..........1264 ) Loss= 0.18900748\n",
      "Training [1265/10000] ..........1265 ) Loss= 0.3982621\n",
      "Training [1266/10000] ..........1266 ) Loss= 0.3992263\n",
      "Training [1267/10000] ..........1267 ) Loss= 0.29123047\n",
      "Training [1268/10000] ..........1268 ) Loss= 0.2798076\n",
      "Training [1269/10000] ..........1269 ) Loss= 0.37345058\n",
      "Training [1270/10000] ..........1270 ) Loss= 0.20550235\n",
      "Training [1271/10000] ..........1271 ) Loss= 0.24027397\n",
      "Training [1272/10000] ..........1272 ) Loss= 0.39371288\n",
      "Training [1273/10000] ..........1273 ) Loss= 0.21711215\n",
      "Training [1274/10000] ..........1274 ) Loss= 0.3399402\n",
      "Training [1275/10000] ..........1275 ) Loss= 0.43151075\n",
      "Training [1276/10000] ..........1276 ) Loss= 0.2570089\n",
      "Training [1277/10000] ..........1277 ) Loss= 0.67909336\n",
      "Training [1278/10000] ..........1278 ) Loss= 0.41773707\n",
      "Training [1279/10000] ..........1279 ) Loss= 0.28373402\n",
      "Training [1280/10000] ..........1280 ) Loss= 0.21631084\n",
      "Training [1281/10000] ..........1281 ) Loss= 0.32010514\n",
      "Training [1282/10000] ..........1282 ) Loss= 0.37674886\n",
      "Training [1283/10000] ..........1283 ) Loss= 0.44898307\n",
      "Training [1284/10000] ..........1284 ) Loss= 0.31830278\n",
      "Training [1285/10000] ..........1285 ) Loss= 0.24493323\n",
      "Training [1286/10000] ..........1286 ) Loss= 0.22938383\n",
      "Training [1287/10000] ..........1287 ) Loss= 0.24005441\n",
      "Training [1288/10000] ..........1288 ) Loss= 0.27519083\n",
      "Training [1289/10000] ..........1289 ) Loss= 0.2352605\n",
      "Training [1290/10000] ..........1290 ) Loss= 0.25464743\n",
      "Training [1291/10000] ..........1291 ) Loss= 0.2494461\n",
      "Training [1292/10000] ..........1292 ) Loss= 0.24976733\n",
      "Training [1293/10000] ..........1293 ) Loss= 0.22723378\n",
      "Training [1294/10000] ..........1294 ) Loss= 0.26182276\n",
      "Training [1295/10000] ..........1295 ) Loss= 0.2357931\n",
      "Training [1296/10000] ..........1296 ) Loss= 0.3614651\n",
      "Training [1297/10000] ..........1297 ) Loss= 0.32972142\n",
      "Training [1298/10000] ..........1298 ) Loss= 0.5399509\n",
      "Training [1299/10000] ..........1299 ) Loss= 0.20533115\n",
      "Training [1300/10000] ..........1300 ) Loss= 0.25977948\n",
      "Training [1301/10000] ..........1301 ) Loss= 0.34630322\n",
      "Training [1302/10000] ..........1302 ) Loss= 0.37792093\n",
      "Training [1303/10000] ..........1303 ) Loss= 0.49100086\n",
      "Training [1304/10000] ..........1304 ) Loss= 0.2252168\n",
      "Training [1305/10000] ..........1305 ) Loss= 0.28597575\n",
      "Training [1306/10000] ..........1306 ) Loss= 0.26217186\n",
      "Training [1307/10000] ..........1307 ) Loss= 0.23424469\n",
      "Training [1308/10000] ..........1308 ) Loss= 0.2971874\n",
      "Training [1309/10000] ..........1309 ) Loss= 0.17316677\n",
      "Training [1310/10000] ..........1310 ) Loss= 0.19689538\n",
      "Training [1311/10000] ..........1311 ) Loss= 0.29601324\n",
      "Training [1312/10000] ..........1312 ) Loss= 0.27976644\n",
      "Training [1313/10000] ..........1313 ) Loss= 0.2789592\n",
      "Training [1314/10000] ..........1314 ) Loss= 0.2651268\n",
      "Training [1315/10000] ..........1315 ) Loss= 0.24453785\n",
      "Training [1316/10000] ..........1316 ) Loss= 0.46382302\n",
      "Training [1317/10000] ..........1317 ) Loss= 0.20283611\n",
      "Training [1318/10000] ..........1318 ) Loss= 0.2207671\n",
      "Training [1319/10000] ..........1319 ) Loss= 0.29318827\n",
      "Training [1320/10000] ..........1320 ) Loss= 0.25781924\n",
      "Training [1321/10000] ..........1321 ) Loss= 0.22729057\n",
      "Training [1322/10000] ..........1322 ) Loss= 0.26946843\n",
      "Training [1323/10000] ..........1323 ) Loss= 0.39759815\n",
      "Training [1324/10000] ..........1324 ) Loss= 0.28696522\n",
      "Training [1325/10000] ..........1325 ) Loss= 0.22704493\n",
      "Training [1326/10000] ..........1326 ) Loss= 0.23371334\n",
      "Training [1327/10000] ..........1327 ) Loss= 0.17648242\n",
      "Training [1328/10000] ..........1328 ) Loss= 0.2928971\n",
      "Training [1329/10000] ..........1329 ) Loss= 0.2729401\n",
      "Training [1330/10000] ..........1330 ) Loss= 0.3427759\n",
      "Training [1331/10000] ..........1331 ) Loss= 0.37654668\n",
      "Training [1332/10000] ..........1332 ) Loss= 0.26710603\n",
      "Training [1333/10000] ..........1333 ) Loss= 0.44107646\n",
      "Training [1334/10000] ..........1334 ) Loss= 0.2163275\n",
      "Training [1335/10000] ..........1335 ) Loss= 0.2505549\n",
      "Training [1336/10000] ..........1336 ) Loss= 0.3260125\n",
      "Training [1337/10000] ..........1337 ) Loss= 0.37376055\n",
      "Training [1338/10000] ..........1338 ) Loss= 0.27044657\n",
      "Training [1339/10000] ..........1339 ) Loss= 0.34357876\n",
      "Training [1340/10000] ..........1340 ) Loss= 0.32699966\n",
      "Training [1341/10000] ..........1341 ) Loss= 0.29675582\n",
      "Training [1342/10000] ..........1342 ) Loss= 0.30772117\n",
      "Training [1343/10000] ..........1343 ) Loss= 0.20185041\n",
      "Training [1344/10000] ..........1344 ) Loss= 0.2885633\n",
      "Training [1345/10000] ..........1345 ) Loss= 0.2847304\n",
      "Training [1346/10000] ..........1346 ) Loss= 0.20650135\n",
      "Training [1347/10000] ..........1347 ) Loss= 0.19515793\n",
      "Training [1348/10000] ..........1348 ) Loss= 0.28117007\n",
      "Training [1349/10000] ..........1349 ) Loss= 0.35449445\n",
      "Training [1350/10000] ..........1350 ) Loss= 0.3430349\n",
      "Training [1351/10000] ..........1351 ) Loss= 0.20286185\n",
      "Training [1352/10000] ..........1352 ) Loss= 0.26165837\n",
      "Training [1353/10000] ..........1353 ) Loss= 0.14329545\n",
      "Training [1354/10000] ..........1354 ) Loss= 0.24470985\n",
      "Training [1355/10000] ..........1355 ) Loss= 0.18746226\n",
      "Training [1356/10000] ..........1356 ) Loss= 0.26852542\n",
      "Training [1357/10000] ..........1357 ) Loss= 0.44795993\n",
      "Training [1358/10000] ..........1358 ) Loss= 0.36977884\n",
      "Training [1359/10000] ..........1359 ) Loss= 0.26421928\n",
      "Training [1360/10000] ..........1360 ) Loss= 0.45607764\n",
      "Training [1361/10000] ..........1361 ) Loss= 0.2615769\n",
      "Training [1362/10000] ..........1362 ) Loss= 0.27593783\n",
      "Training [1363/10000] ..........1363 ) Loss= 0.2030317\n",
      "Training [1364/10000] ..........1364 ) Loss= 0.17199753\n",
      "Training [1365/10000] ..........1365 ) Loss= 0.3961076\n",
      "Training [1366/10000] ..........1366 ) Loss= 0.22160485\n",
      "Training [1367/10000] ..........1367 ) Loss= 0.525805\n",
      "Training [1368/10000] ..........1368 ) Loss= 0.17546819\n",
      "Training [1369/10000] ..........1369 ) Loss= 0.19021927\n",
      "Training [1370/10000] ..........1370 ) Loss= 0.40122044\n",
      "Training [1371/10000] ..........1371 ) Loss= 0.3788431\n",
      "Training [1372/10000] ..........1372 ) Loss= 0.30797446\n",
      "Training [1373/10000] ..........1373 ) Loss= 0.24386966\n",
      "Training [1374/10000] ..........1374 ) Loss= 0.30836272\n",
      "Training [1375/10000] ..........1375 ) Loss= 0.22213058\n",
      "Training [1376/10000] ..........1376 ) Loss= 0.2083208\n",
      "Training [1377/10000] ..........1377 ) Loss= 0.36279917\n",
      "Training [1378/10000] ..........1378 ) Loss= 0.34782556\n",
      "Training [1379/10000] ..........1379 ) Loss= 0.35802963\n",
      "Training [1380/10000] ..........1380 ) Loss= 0.21150059\n",
      "Training [1381/10000] ..........1381 ) Loss= 0.32720888\n",
      "Training [1382/10000] ..........1382 ) Loss= 0.17722492\n",
      "Training [1383/10000] ..........1383 ) Loss= 0.2133426\n",
      "Training [1384/10000] ..........1384 ) Loss= 0.32770926\n",
      "Training [1385/10000] ..........1385 ) Loss= 0.2503556\n",
      "Training [1386/10000] ..........1386 ) Loss= 0.27462402\n",
      "Training [1387/10000] ..........1387 ) Loss= 0.2577369\n",
      "Training [1388/10000] ..........1388 ) Loss= 0.36840573\n",
      "Training [1389/10000] ..........1389 ) Loss= 0.3883029\n",
      "Training [1390/10000] ..........1390 ) Loss= 0.26908046\n",
      "Training [1391/10000] ..........1391 ) Loss= 0.4333627\n",
      "Training [1392/10000] ..........1392 ) Loss= 0.43245763\n",
      "Training [1393/10000] ..........1393 ) Loss= 0.3239844\n",
      "Training [1394/10000] ..........1394 ) Loss= 0.25861585\n",
      "Training [1395/10000] ..........1395 ) Loss= 0.17801751\n",
      "Training [1396/10000] ..........1396 ) Loss= 0.3873849\n",
      "Training [1397/10000] ..........1397 ) Loss= 0.22302234\n",
      "Training [1398/10000] ..........1398 ) Loss= 0.36453888\n",
      "Training [1399/10000] ..........1399 ) Loss= 0.24334261\n",
      "Training [1400/10000] ..........1400 ) Loss= 0.30755714\n",
      "Training [1401/10000] ..........1401 ) Loss= 0.26774278\n",
      "Training [1402/10000] ..........1402 ) Loss= 0.29653624\n",
      "Training [1403/10000] ..........1403 ) Loss= 0.25112382\n",
      "Training [1404/10000] ..........1404 ) Loss= 0.3872663\n",
      "Training [1405/10000] ..........1405 ) Loss= 0.28758737\n",
      "Training [1406/10000] ..........1406 ) Loss= 0.27329448\n",
      "Training [1407/10000] ..........1407 ) Loss= 0.30782795\n",
      "Training [1408/10000] ..........1408 ) Loss= 0.29480723\n",
      "Training [1409/10000] ..........1409 ) Loss= 0.48716143\n",
      "Training [1410/10000] ..........1410 ) Loss= 0.28187186\n",
      "Training [1411/10000] ..........1411 ) Loss= 0.31538376\n",
      "Training [1412/10000] ..........1412 ) Loss= 0.34893537\n",
      "Training [1413/10000] ..........1413 ) Loss= 0.29900035\n",
      "Training [1414/10000] ..........1414 ) Loss= 0.4478104\n",
      "Training [1415/10000] ..........1415 ) Loss= 0.16892844\n",
      "Training [1416/10000] ..........1416 ) Loss= 0.50045985\n",
      "Training [1417/10000] ..........1417 ) Loss= 0.38849717\n",
      "Training [1418/10000] ..........1418 ) Loss= 0.1738804\n",
      "Training [1419/10000] ..........1419 ) Loss= 0.24906312\n",
      "Training [1420/10000] ..........1420 ) Loss= 0.41634876\n",
      "Training [1421/10000] ..........1421 ) Loss= 0.33813223\n",
      "Training [1422/10000] ..........1422 ) Loss= 0.36522523\n",
      "Training [1423/10000] ..........1423 ) Loss= 0.3420511\n",
      "Training [1424/10000] ..........1424 ) Loss= 0.27929702\n",
      "Training [1425/10000] ..........1425 ) Loss= 0.36791468\n",
      "Training [1426/10000] ..........1426 ) Loss= 0.22313398\n",
      "Training [1427/10000] ..........1427 ) Loss= 0.34469548\n",
      "Training [1428/10000] ..........1428 ) Loss= 0.23583487\n",
      "Training [1429/10000] ..........1429 ) Loss= 0.21745269\n",
      "Training [1430/10000] ..........1430 ) Loss= 0.21779726\n",
      "Training [1431/10000] ..........1431 ) Loss= 0.29881778\n",
      "Training [1432/10000] ..........1432 ) Loss= 0.32054043\n",
      "Training [1433/10000] ..........1433 ) Loss= 0.21989526\n",
      "Training [1434/10000] ..........1434 ) Loss= 0.44951212\n",
      "Training [1435/10000] ..........1435 ) Loss= 0.56888145\n",
      "Training [1436/10000] ..........1436 ) Loss= 0.23752277\n",
      "Training [1437/10000] ..........1437 ) Loss= 0.20665427\n",
      "Training [1438/10000] ..........1438 ) Loss= 0.26626095\n",
      "Training [1439/10000] ..........1439 ) Loss= 0.59595966\n",
      "Training [1440/10000] ..........1440 ) Loss= 0.290321\n",
      "Training [1441/10000] ..........1441 ) Loss= 0.1888565\n",
      "Training [1442/10000] ..........1442 ) Loss= 0.23041569\n",
      "Training [1443/10000] ..........1443 ) Loss= 0.27074414\n",
      "Training [1444/10000] ..........1444 ) Loss= 0.29983938\n",
      "Training [1445/10000] ..........1445 ) Loss= 0.62270397\n",
      "Training [1446/10000] ..........1446 ) Loss= 0.16416611\n",
      "Training [1447/10000] ..........1447 ) Loss= 0.2086962\n",
      "Training [1448/10000] ..........1448 ) Loss= 0.2338002\n",
      "Training [1449/10000] ..........1449 ) Loss= 0.4013504\n",
      "Training [1450/10000] ..........1450 ) Loss= 0.31884012\n",
      "Training [1451/10000] ..........1451 ) Loss= 0.3839411\n",
      "Training [1452/10000] ..........1452 ) Loss= 0.32941973\n",
      "Training [1453/10000] ..........1453 ) Loss= 0.24060166\n",
      "Training [1454/10000] ..........1454 ) Loss= 0.24660154\n",
      "Training [1455/10000] ..........1455 ) Loss= 0.2671185\n",
      "Training [1456/10000] ..........1456 ) Loss= 0.31328568\n",
      "Training [1457/10000] ..........1457 ) Loss= 0.25980458\n",
      "Training [1458/10000] ..........1458 ) Loss= 0.3885215\n",
      "Training [1459/10000] ..........1459 ) Loss= 0.29676184\n",
      "Training [1460/10000] ..........1460 ) Loss= 0.25367704\n",
      "Training [1461/10000] ..........1461 ) Loss= 0.24284443\n",
      "Training [1462/10000] ..........1462 ) Loss= 0.24294928\n",
      "Training [1463/10000] ..........1463 ) Loss= 0.27120945\n",
      "Training [1464/10000] ..........1464 ) Loss= 0.25569892\n",
      "Training [1465/10000] ..........1465 ) Loss= 0.43054256\n",
      "Training [1466/10000] ..........1466 ) Loss= 0.19952233\n",
      "Training [1467/10000] ..........1467 ) Loss= 0.24150752\n",
      "Training [1468/10000] ..........1468 ) Loss= 0.33282724\n",
      "Training [1469/10000] ..........1469 ) Loss= 0.20285252\n",
      "Training [1470/10000] ..........1470 ) Loss= 0.3615806\n",
      "Training [1471/10000] ..........1471 ) Loss= 0.2874487\n",
      "Training [1472/10000] ..........1472 ) Loss= 0.2425442\n",
      "Training [1473/10000] ..........1473 ) Loss= 0.26494524\n",
      "Training [1474/10000] ..........1474 ) Loss= 0.21363908\n",
      "Training [1475/10000] ..........1475 ) Loss= 0.3386075\n",
      "Training [1476/10000] ..........1476 ) Loss= 0.25263798\n",
      "Training [1477/10000] ..........1477 ) Loss= 0.53603554\n",
      "Training [1478/10000] ..........1478 ) Loss= 0.31535566\n",
      "Training [1479/10000] ..........1479 ) Loss= 0.22998448\n",
      "Training [1480/10000] ..........1480 ) Loss= 0.22885478\n",
      "Training [1481/10000] ..........1481 ) Loss= 0.23102333\n",
      "Training [1482/10000] ..........1482 ) Loss= 0.29201975\n",
      "Training [1483/10000] ..........1483 ) Loss= 0.33593702\n",
      "Training [1484/10000] ..........1484 ) Loss= 0.21819228\n",
      "Training [1485/10000] ..........1485 ) Loss= 0.21867262\n",
      "Training [1486/10000] ..........1486 ) Loss= 0.38576508\n",
      "Training [1487/10000] ..........1487 ) Loss= 0.39158955\n",
      "Training [1488/10000] ..........1488 ) Loss= 0.23408863\n",
      "Training [1489/10000] ..........1489 ) Loss= 0.21508813\n",
      "Training [1490/10000] ..........1490 ) Loss= 0.30795753\n",
      "Training [1491/10000] ..........1491 ) Loss= 0.2788632\n",
      "Training [1492/10000] ..........1492 ) Loss= 0.38815302\n",
      "Training [1493/10000] ..........1493 ) Loss= 0.23165044\n",
      "Training [1494/10000] ..........1494 ) Loss= 0.27611384\n",
      "Training [1495/10000] ..........1495 ) Loss= 0.18806602\n",
      "Training [1496/10000] ..........1496 ) Loss= 0.3301257\n",
      "Training [1497/10000] ..........1497 ) Loss= 0.24547322\n",
      "Training [1498/10000] ..........1498 ) Loss= 0.35050306\n",
      "Training [1499/10000] ..........1499 ) Loss= 0.5131836\n",
      "Training [1500/10000] ..........1500 ) Loss= 0.22441395\n",
      "Training [1501/10000] ..........1501 ) Loss= 0.28433487\n",
      "Training [1502/10000] ..........1502 ) Loss= 0.226545\n",
      "Training [1503/10000] ..........1503 ) Loss= 0.2572531\n",
      "Training [1504/10000] ..........1504 ) Loss= 0.23244952\n",
      "Training [1505/10000] ..........1505 ) Loss= 0.26488656\n",
      "Training [1506/10000] ..........1506 ) Loss= 0.20578404\n",
      "Training [1507/10000] ..........1507 ) Loss= 0.22837047\n",
      "Training [1508/10000] ..........1508 ) Loss= 0.29427844\n",
      "Training [1509/10000] ..........1509 ) Loss= 0.33705425\n",
      "Training [1510/10000] ..........1510 ) Loss= 0.3442105\n",
      "Training [1511/10000] ..........1511 ) Loss= 0.3688179\n",
      "Training [1512/10000] ..........1512 ) Loss= 0.1661492\n",
      "Training [1513/10000] ..........1513 ) Loss= 0.3429831\n",
      "Training [1514/10000] ..........1514 ) Loss= 0.2494442\n",
      "Training [1515/10000] ..........1515 ) Loss= 0.21769391\n",
      "Training [1516/10000] ..........1516 ) Loss= 0.25314835\n",
      "Training [1517/10000] ..........1517 ) Loss= 0.7557718\n",
      "Training [1518/10000] ..........1518 ) Loss= 0.18083549\n",
      "Training [1519/10000] ..........1519 ) Loss= 0.24784422\n",
      "Training [1520/10000] ..........1520 ) Loss= 0.3560561\n",
      "Training [1521/10000] ..........1521 ) Loss= 0.30124307\n",
      "Training [1522/10000] ..........1522 ) Loss= 0.22369447\n",
      "Training [1523/10000] ..........1523 ) Loss= 0.41840142\n",
      "Training [1524/10000] ..........1524 ) Loss= 0.1718916\n",
      "Training [1525/10000] ..........1525 ) Loss= 0.25317663\n",
      "Training [1526/10000] ..........1526 ) Loss= 0.21028164\n",
      "Training [1527/10000] ..........1527 ) Loss= 0.2877986\n",
      "Training [1528/10000] ..........1528 ) Loss= 0.6308389\n",
      "Training [1529/10000] ..........1529 ) Loss= 0.28828678\n",
      "Training [1530/10000] ..........1530 ) Loss= 0.23807076\n",
      "Training [1531/10000] ..........1531 ) Loss= 0.23368496\n",
      "Training [1532/10000] ..........1532 ) Loss= 0.24913558\n",
      "Training [1533/10000] ..........1533 ) Loss= 0.38922936\n",
      "Training [1534/10000] ..........1534 ) Loss= 0.19491662\n",
      "Training [1535/10000] ..........1535 ) Loss= 0.5485447\n",
      "Training [1536/10000] ..........1536 ) Loss= 0.20716915\n",
      "Training [1537/10000] ..........1537 ) Loss= 0.32327753\n",
      "Training [1538/10000] ..........1538 ) Loss= 0.28361207\n",
      "Training [1539/10000] ..........1539 ) Loss= 0.5529241\n",
      "Training [1540/10000] ..........1540 ) Loss= 0.24931525\n",
      "Training [1541/10000] ..........1541 ) Loss= 0.3394034\n",
      "Training [1542/10000] ..........1542 ) Loss= 0.22349378\n",
      "Training [1543/10000] ..........1543 ) Loss= 0.3856799\n",
      "Training [1544/10000] ..........1544 ) Loss= 0.26033667\n",
      "Training [1545/10000] ..........1545 ) Loss= 0.34006917\n",
      "Training [1546/10000] ..........1546 ) Loss= 0.21454023\n",
      "Training [1547/10000] ..........1547 ) Loss= 0.2656401\n",
      "Training [1548/10000] ..........1548 ) Loss= 0.46226922\n",
      "Training [1549/10000] ..........1549 ) Loss= 0.23761779\n",
      "Training [1550/10000] ..........1550 ) Loss= 0.369606\n",
      "Training [1551/10000] ..........1551 ) Loss= 0.31448093\n",
      "Training [1552/10000] ..........1552 ) Loss= 0.22691591\n",
      "Training [1553/10000] ..........1553 ) Loss= 0.17382585\n",
      "Training [1554/10000] ..........1554 ) Loss= 0.37375566\n",
      "Training [1555/10000] ..........1555 ) Loss= 0.23023774\n",
      "Training [1556/10000] ..........1556 ) Loss= 0.32557225\n",
      "Training [1557/10000] ..........1557 ) Loss= 0.2173235\n",
      "Training [1558/10000] ..........1558 ) Loss= 0.22376038\n",
      "Training [1559/10000] ..........1559 ) Loss= 0.28732282\n",
      "Training [1560/10000] ..........1560 ) Loss= 0.27965283\n",
      "Training [1561/10000] ..........1561 ) Loss= 0.19216861\n",
      "Training [1562/10000] ..........1562 ) Loss= 0.40732455\n",
      "Training [1563/10000] ..........1563 ) Loss= 0.35990894\n",
      "Training [1564/10000] ..........1564 ) Loss= 0.24156182\n",
      "Training [1565/10000] ..........1565 ) Loss= 0.2321662\n",
      "Training [1566/10000] ..........1566 ) Loss= 0.21360062\n",
      "Training [1567/10000] ..........1567 ) Loss= 0.29333523\n",
      "Training [1568/10000] ..........1568 ) Loss= 0.22737385\n",
      "Training [1569/10000] ..........1569 ) Loss= 0.23566084\n",
      "Training [1570/10000] ..........1570 ) Loss= 0.14329848\n",
      "Training [1571/10000] ..........1571 ) Loss= 0.21808322\n",
      "Training [1572/10000] ..........1572 ) Loss= 0.2727434\n",
      "Training [1573/10000] ..........1573 ) Loss= 0.20618644\n",
      "Training [1574/10000] ..........1574 ) Loss= 0.3397757\n",
      "Training [1575/10000] ..........1575 ) Loss= 0.2173566\n",
      "Training [1576/10000] ..........1576 ) Loss= 0.4287973\n",
      "Training [1577/10000] ..........1577 ) Loss= 0.20355141\n",
      "Training [1578/10000] ..........1578 ) Loss= 0.15618838\n",
      "Training [1579/10000] ..........1579 ) Loss= 0.20087276\n",
      "Training [1580/10000] ..........1580 ) Loss= 1.0053102\n",
      "Training [1581/10000] ..........1581 ) Loss= 0.2539121\n",
      "Training [1582/10000] ..........1582 ) Loss= 0.20601922\n",
      "Training [1583/10000] ..........1583 ) Loss= 0.3219397\n",
      "Training [1584/10000] ..........1584 ) Loss= 0.3421566\n",
      "Training [1585/10000] ..........1585 ) Loss= 0.2570698\n",
      "Training [1586/10000] ..........1586 ) Loss= 0.36326683\n",
      "Training [1587/10000] ..........1587 ) Loss= 0.24883758\n",
      "Training [1588/10000] ..........1588 ) Loss= 0.26389617\n",
      "Training [1589/10000] ..........1589 ) Loss= 0.3024222\n",
      "Training [1590/10000] ..........1590 ) Loss= 0.3103147\n",
      "Training [1591/10000] ..........1591 ) Loss= 0.2705796\n",
      "Training [1592/10000] ..........1592 ) Loss= 0.2031942\n",
      "Training [1593/10000] ..........1593 ) Loss= 0.26896855\n",
      "Training [1594/10000] ..........1594 ) Loss= 0.68710625\n",
      "Training [1595/10000] ..........1595 ) Loss= 0.30964768\n",
      "Training [1596/10000] ..........1596 ) Loss= 0.20944312\n",
      "Training [1597/10000] ..........1597 ) Loss= 0.18454506\n",
      "Training [1598/10000] ..........1598 ) Loss= 0.2853232\n",
      "Training [1599/10000] ..........1599 ) Loss= 0.24443519\n",
      "Training [1600/10000] ..........1600 ) Loss= 0.16184555\n",
      "Training [1601/10000] ..........1601 ) Loss= 0.25377616\n",
      "Training [1602/10000] ..........1602 ) Loss= 0.41985455\n",
      "Training [1603/10000] ..........1603 ) Loss= 0.27057752\n",
      "Training [1604/10000] ..........1604 ) Loss= 0.18830943\n",
      "Training [1605/10000] ..........1605 ) Loss= 0.31518802\n",
      "Training [1606/10000] ..........1606 ) Loss= 0.39467975\n",
      "Training [1607/10000] ..........1607 ) Loss= 0.32653832\n",
      "Training [1608/10000] ..........1608 ) Loss= 0.1661437\n",
      "Training [1609/10000] ..........1609 ) Loss= 0.26460373\n",
      "Training [1610/10000] ..........1610 ) Loss= 0.29203424\n",
      "Training [1611/10000] ..........1611 ) Loss= 0.19044192\n",
      "Training [1612/10000] ..........1612 ) Loss= 0.38358417\n",
      "Training [1613/10000] ..........1613 ) Loss= 0.21733426\n",
      "Training [1614/10000] ..........1614 ) Loss= 0.19935858\n",
      "Training [1615/10000] ..........1615 ) Loss= 0.19906022\n",
      "Training [1616/10000] ..........1616 ) Loss= 0.21887106\n",
      "Training [1617/10000] ..........1617 ) Loss= 0.25663635\n",
      "Training [1618/10000] ..........1618 ) Loss= 0.17119087\n",
      "Training [1619/10000] ..........1619 ) Loss= 0.2146815\n",
      "Training [1620/10000] ..........1620 ) Loss= 0.51499236\n",
      "Training [1621/10000] ..........1621 ) Loss= 0.33366388\n",
      "Training [1622/10000] ..........1622 ) Loss= 0.23409651\n",
      "Training [1623/10000] ..........1623 ) Loss= 0.22457558\n",
      "Training [1624/10000] ..........1624 ) Loss= 0.31466848\n",
      "Training [1625/10000] ..........1625 ) Loss= 0.48463747\n",
      "Training [1626/10000] ..........1626 ) Loss= 0.33466715\n",
      "Training [1627/10000] ..........1627 ) Loss= 0.27705008\n",
      "Training [1628/10000] ..........1628 ) Loss= 0.23012474\n",
      "Training [1629/10000] ..........1629 ) Loss= 0.31359154\n",
      "Training [1630/10000] ..........1630 ) Loss= 0.26340297\n",
      "Training [1631/10000] ..........1631 ) Loss= 0.24057004\n",
      "Training [1632/10000] ..........1632 ) Loss= 0.19362882\n",
      "Training [1633/10000] ..........1633 ) Loss= 0.22753409\n",
      "Training [1634/10000] ..........1634 ) Loss= 0.3411068\n",
      "Training [1635/10000] ..........1635 ) Loss= 0.18522172\n",
      "Training [1636/10000] ..........1636 ) Loss= 0.2823396\n",
      "Training [1637/10000] ..........1637 ) Loss= 0.17069617\n",
      "Training [1638/10000] ..........1638 ) Loss= 0.3211704\n",
      "Training [1639/10000] ..........1639 ) Loss= 0.40367135\n",
      "Training [1640/10000] ..........1640 ) Loss= 0.45338455\n",
      "Training [1641/10000] ..........1641 ) Loss= 0.20222206\n",
      "Training [1642/10000] ..........1642 ) Loss= 0.24442415\n",
      "Training [1643/10000] ..........1643 ) Loss= 0.24486125\n",
      "Training [1644/10000] ..........1644 ) Loss= 0.24966678\n",
      "Training [1645/10000] ..........1645 ) Loss= 0.2582535\n",
      "Training [1646/10000] ..........1646 ) Loss= 0.19508597\n",
      "Training [1647/10000] ..........1647 ) Loss= 0.20195812\n",
      "Training [1648/10000] ..........1648 ) Loss= 0.30960202\n",
      "Training [1649/10000] ..........1649 ) Loss= 0.35263968\n",
      "Training [1650/10000] ..........1650 ) Loss= 0.23585653\n",
      "Training [1651/10000] ..........1651 ) Loss= 0.24078849\n",
      "Training [1652/10000] ..........1652 ) Loss= 0.30424714\n",
      "Training [1653/10000] ..........1653 ) Loss= 0.25258216\n",
      "Training [1654/10000] ..........1654 ) Loss= 0.17986472\n",
      "Training [1655/10000] ..........1655 ) Loss= 0.15383157\n",
      "Training [1656/10000] ..........1656 ) Loss= 0.6195958\n",
      "Training [1657/10000] ..........1657 ) Loss= 0.27449447\n",
      "Training [1658/10000] ..........1658 ) Loss= 0.19176434\n",
      "Training [1659/10000] ..........1659 ) Loss= 0.2674081\n",
      "Training [1660/10000] ..........1660 ) Loss= 0.35505363\n",
      "Training [1661/10000] ..........1661 ) Loss= 0.207129\n",
      "Training [1662/10000] ..........1662 ) Loss= 0.29677966\n",
      "Training [1663/10000] ..........1663 ) Loss= 0.50381047\n",
      "Training [1664/10000] ..........1664 ) Loss= 0.5090917\n",
      "Training [1665/10000] ..........1665 ) Loss= 0.2307571\n",
      "Training [1666/10000] ..........1666 ) Loss= 0.23124969\n",
      "Training [1667/10000] ..........1667 ) Loss= 0.25631624\n",
      "Training [1668/10000] ..........1668 ) Loss= 0.16457969\n",
      "Training [1669/10000] ..........1669 ) Loss= 0.17547253\n",
      "Training [1670/10000] ..........1670 ) Loss= 0.36739624\n",
      "Training [1671/10000] ..........1671 ) Loss= 0.26168007\n",
      "Training [1672/10000] ..........1672 ) Loss= 0.233139\n",
      "Training [1673/10000] ..........1673 ) Loss= 0.21272533\n",
      "Training [1674/10000] ..........1674 ) Loss= 0.2685511\n",
      "Training [1675/10000] ..........1675 ) Loss= 0.2853394\n",
      "Training [1676/10000] ..........1676 ) Loss= 0.46187988\n",
      "Training [1677/10000] ..........1677 ) Loss= 0.28820553\n",
      "Training [1678/10000] ..........1678 ) Loss= 0.22021107\n",
      "Training [1679/10000] ..........1679 ) Loss= 0.36037695\n",
      "Training [1680/10000] ..........1680 ) Loss= 0.27140585\n",
      "Training [1681/10000] ..........1681 ) Loss= 0.6968468\n",
      "Training [1682/10000] ..........1682 ) Loss= 0.42594132\n",
      "Training [1683/10000] ..........1683 ) Loss= 0.2612664\n",
      "Training [1684/10000] ..........1684 ) Loss= 0.22541384\n",
      "Training [1685/10000] ..........1685 ) Loss= 0.43444163\n",
      "Training [1686/10000] ..........1686 ) Loss= 0.27415437\n",
      "Training [1687/10000] ..........1687 ) Loss= 0.19855365\n",
      "Training [1688/10000] ..........1688 ) Loss= 0.22705376\n",
      "Training [1689/10000] ..........1689 ) Loss= 0.19604653\n",
      "Training [1690/10000] ..........1690 ) Loss= 0.28137732\n",
      "Training [1691/10000] ..........1691 ) Loss= 0.4229072\n",
      "Training [1692/10000] ..........1692 ) Loss= 0.19508663\n",
      "Training [1693/10000] ..........1693 ) Loss= 0.2676145\n",
      "Training [1694/10000] ..........1694 ) Loss= 0.21559021\n",
      "Training [1695/10000] ..........1695 ) Loss= 0.17542852\n",
      "Training [1696/10000] ..........1696 ) Loss= 0.22766097\n",
      "Training [1697/10000] ..........1697 ) Loss= 0.24091068\n",
      "Training [1698/10000] ..........1698 ) Loss= 0.65972644\n",
      "Training [1699/10000] ..........1699 ) Loss= 0.19073403\n",
      "Training [1700/10000] ..........1700 ) Loss= 0.21555513\n",
      "Training [1701/10000] ..........1701 ) Loss= 0.35861865\n",
      "Training [1702/10000] ..........1702 ) Loss= 0.29161394\n",
      "Training [1703/10000] ..........1703 ) Loss= 0.4548651\n",
      "Training [1704/10000] ..........1704 ) Loss= 0.26140082\n",
      "Training [1705/10000] ..........1705 ) Loss= 0.25782034\n",
      "Training [1706/10000] ..........1706 ) Loss= 0.15734206\n",
      "Training [1707/10000] ..........1707 ) Loss= 0.16032714\n",
      "Training [1708/10000] ..........1708 ) Loss= 0.32408392\n",
      "Training [1709/10000] ..........1709 ) Loss= 0.24795613\n",
      "Training [1710/10000] ..........1710 ) Loss= 0.20017006\n",
      "Training [1711/10000] ..........1711 ) Loss= 0.27370876\n",
      "Training [1712/10000] ..........1712 ) Loss= 0.18167393\n",
      "Training [1713/10000] ..........1713 ) Loss= 0.27070785\n",
      "Training [1714/10000] ..........1714 ) Loss= 0.2457203\n",
      "Training [1715/10000] ..........1715 ) Loss= 0.8888498\n",
      "Training [1716/10000] ..........1716 ) Loss= 0.20837347\n",
      "Training [1717/10000] ..........1717 ) Loss= 0.33396745\n",
      "Training [1718/10000] ..........1718 ) Loss= 0.14471042\n",
      "Training [1719/10000] ..........1719 ) Loss= 0.29156008\n",
      "Training [1720/10000] ..........1720 ) Loss= 0.1355946\n",
      "Training [1721/10000] ..........1721 ) Loss= 0.21772991\n",
      "Training [1722/10000] ..........1722 ) Loss= 0.19616328\n",
      "Training [1723/10000] ..........1723 ) Loss= 0.24397074\n",
      "Training [1724/10000] ..........1724 ) Loss= 0.72514296\n",
      "Training [1725/10000] ..........1725 ) Loss= 0.29558375\n",
      "Training [1726/10000] ..........1726 ) Loss= 0.26586607\n",
      "Training [1727/10000] ..........1727 ) Loss= 0.31666973\n",
      "Training [1728/10000] ..........1728 ) Loss= 0.20392221\n",
      "Training [1729/10000] ..........1729 ) Loss= 0.35467917\n",
      "Training [1730/10000] ..........1730 ) Loss= 0.20537111\n",
      "Training [1731/10000] ..........1731 ) Loss= 0.21224575\n",
      "Training [1732/10000] ..........1732 ) Loss= 0.22373827\n",
      "Training [1733/10000] ..........1733 ) Loss= 0.4191234\n",
      "Training [1734/10000] ..........1734 ) Loss= 0.25992978\n",
      "Training [1735/10000] ..........1735 ) Loss= 0.20617278\n",
      "Training [1736/10000] ..........1736 ) Loss= 0.17919263\n",
      "Training [1737/10000] ..........1737 ) Loss= 0.40940157\n",
      "Training [1738/10000] ..........1738 ) Loss= 0.22330688\n",
      "Training [1739/10000] ..........1739 ) Loss= 0.43019256\n",
      "Training [1740/10000] ..........1740 ) Loss= 0.27198422\n",
      "Training [1741/10000] ..........1741 ) Loss= 0.26505694\n",
      "Training [1742/10000] ..........1742 ) Loss= 0.24449606\n",
      "Training [1743/10000] ..........1743 ) Loss= 0.326796\n",
      "Training [1744/10000] ..........1744 ) Loss= 0.18834578\n",
      "Training [1745/10000] ..........1745 ) Loss= 0.4310743\n",
      "Training [1746/10000] ..........1746 ) Loss= 0.3235861\n",
      "Training [1747/10000] ..........1747 ) Loss= 0.27975905\n",
      "Training [1748/10000] ..........1748 ) Loss= 0.4980624\n",
      "Training [1749/10000] ..........1749 ) Loss= 0.16378555\n",
      "Training [1750/10000] ..........1750 ) Loss= 0.57349336\n",
      "Training [1751/10000] ..........1751 ) Loss= 0.20656136\n",
      "Training [1752/10000] ..........1752 ) Loss= 0.21591716\n",
      "Training [1753/10000] ..........1753 ) Loss= 0.34432533\n",
      "Training [1754/10000] ..........1754 ) Loss= 0.32286555\n",
      "Training [1755/10000] ..........1755 ) Loss= 0.18851799\n",
      "Training [1756/10000] ..........1756 ) Loss= 0.24212225\n",
      "Training [1757/10000] ..........1757 ) Loss= 0.16669244\n",
      "Training [1758/10000] ..........1758 ) Loss= 0.3076524\n",
      "Training [1759/10000] ..........1759 ) Loss= 0.27612418\n",
      "Training [1760/10000] ..........1760 ) Loss= 0.26242667\n",
      "Training [1761/10000] ..........1761 ) Loss= 0.2091069\n",
      "Training [1762/10000] ..........1762 ) Loss= 0.16387159\n",
      "Training [1763/10000] ..........1763 ) Loss= 0.2576112\n",
      "Training [1764/10000] ..........1764 ) Loss= 0.37161338\n",
      "Training [1765/10000] ..........1765 ) Loss= 0.18295059\n",
      "Training [1766/10000] ..........1766 ) Loss= 0.23620829\n",
      "Training [1767/10000] ..........1767 ) Loss= 0.19720791\n",
      "Training [1768/10000] ..........1768 ) Loss= 0.22954677\n",
      "Training [1769/10000] ..........1769 ) Loss= 0.17620893\n",
      "Training [1770/10000] ..........1770 ) Loss= 0.2816594\n",
      "Training [1771/10000] ..........1771 ) Loss= 0.20716925\n",
      "Training [1772/10000] ..........1772 ) Loss= 0.5017225\n",
      "Training [1773/10000] ..........1773 ) Loss= 0.33643308\n",
      "Training [1774/10000] ..........1774 ) Loss= 0.19666733\n",
      "Training [1775/10000] ..........1775 ) Loss= 0.18634331\n",
      "Training [1776/10000] ..........1776 ) Loss= 0.30107772\n",
      "Training [1777/10000] ..........1777 ) Loss= 0.37643823\n",
      "Training [1778/10000] ..........1778 ) Loss= 0.4261851\n",
      "Training [1779/10000] ..........1779 ) Loss= 0.1819435\n",
      "Training [1780/10000] ..........1780 ) Loss= 0.24144764\n",
      "Training [1781/10000] ..........1781 ) Loss= 0.21237493\n",
      "Training [1782/10000] ..........1782 ) Loss= 0.27365187\n",
      "Training [1783/10000] ..........1783 ) Loss= 0.14645912\n",
      "Training [1784/10000] ..........1784 ) Loss= 0.16230592\n",
      "Training [1785/10000] ..........1785 ) Loss= 0.2526648\n",
      "Training [1786/10000] ..........1786 ) Loss= 0.4012836\n",
      "Training [1787/10000] ..........1787 ) Loss= 0.36669064\n",
      "Training [1788/10000] ..........1788 ) Loss= 0.17872716\n",
      "Training [1789/10000] ..........1789 ) Loss= 0.18834983\n",
      "Training [1790/10000] ..........1790 ) Loss= 0.33406132\n",
      "Training [1791/10000] ..........1791 ) Loss= 0.22021918\n",
      "Training [1792/10000] ..........1792 ) Loss= 0.37337145\n",
      "Training [1793/10000] ..........1793 ) Loss= 0.51850784\n",
      "Training [1794/10000] ..........1794 ) Loss= 0.21292104\n",
      "Training [1795/10000] ..........1795 ) Loss= 0.2895508\n",
      "Training [1796/10000] ..........1796 ) Loss= 0.320485\n",
      "Training [1797/10000] ..........1797 ) Loss= 0.45472634\n",
      "Training [1798/10000] ..........1798 ) Loss= 0.5848257\n",
      "Training [1799/10000] ..........1799 ) Loss= 0.31704825\n",
      "Training [1800/10000] ..........1800 ) Loss= 0.21089777\n",
      "Training [1801/10000] ..........1801 ) Loss= 0.24235731\n",
      "Training [1802/10000] ..........1802 ) Loss= 0.26261613\n",
      "Training [1803/10000] ..........1803 ) Loss= 0.20041616\n",
      "Training [1804/10000] ..........1804 ) Loss= 0.26298907\n",
      "Training [1805/10000] ..........1805 ) Loss= 0.31065634\n",
      "Training [1806/10000] ..........1806 ) Loss= 0.19309704\n",
      "Training [1807/10000] ..........1807 ) Loss= 0.26752314\n",
      "Training [1808/10000] ..........1808 ) Loss= 0.39939472\n",
      "Training [1809/10000] ..........1809 ) Loss= 0.1981586\n",
      "Training [1810/10000] ..........1810 ) Loss= 0.18639547\n",
      "Training [1811/10000] ..........1811 ) Loss= 0.2303657\n",
      "Training [1812/10000] ..........1812 ) Loss= 0.22174235\n",
      "Training [1813/10000] ..........1813 ) Loss= 0.28045747\n",
      "Training [1814/10000] ..........1814 ) Loss= 0.27917123\n",
      "Training [1815/10000] ..........1815 ) Loss= 0.28693187\n",
      "Training [1816/10000] ..........1816 ) Loss= 0.16773783\n",
      "Training [1817/10000] ..........1817 ) Loss= 0.14036447\n",
      "Training [1818/10000] ..........1818 ) Loss= 0.36298278\n",
      "Training [1819/10000] ..........1819 ) Loss= 0.20218176\n",
      "Training [1820/10000] ..........1820 ) Loss= 0.3159236\n",
      "Training [1821/10000] ..........1821 ) Loss= 0.18832923\n",
      "Training [1822/10000] ..........1822 ) Loss= 0.20035146\n",
      "Training [1823/10000] ..........1823 ) Loss= 0.23186778\n",
      "Training [1824/10000] ..........1824 ) Loss= 0.18868983\n",
      "Training [1825/10000] ..........1825 ) Loss= 0.25673547\n",
      "Training [1826/10000] ..........1826 ) Loss= 0.30827382\n",
      "Training [1827/10000] ..........1827 ) Loss= 0.37387672\n",
      "Training [1828/10000] ..........1828 ) Loss= 0.25285408\n",
      "Training [1829/10000] ..........1829 ) Loss= 0.15931007\n",
      "Training [1830/10000] ..........1830 ) Loss= 0.3216232\n",
      "Training [1831/10000] ..........1831 ) Loss= 0.2418862\n",
      "Training [1832/10000] ..........1832 ) Loss= 0.2454741\n",
      "Training [1833/10000] ..........1833 ) Loss= 0.2241012\n",
      "Training [1834/10000] ..........1834 ) Loss= 0.23191597\n",
      "Training [1835/10000] ..........1835 ) Loss= 0.22078863\n",
      "Training [1836/10000] ..........1836 ) Loss= 0.1756676\n",
      "Training [1837/10000] ..........1837 ) Loss= 0.2323141\n",
      "Training [1838/10000] ..........1838 ) Loss= 0.30685785\n",
      "Training [1839/10000] ..........1839 ) Loss= 0.18425891\n",
      "Training [1840/10000] ..........1840 ) Loss= 0.20695014\n",
      "Training [1841/10000] ..........1841 ) Loss= 0.24467319\n",
      "Training [1842/10000] ..........1842 ) Loss= 0.2215714\n",
      "Training [1843/10000] ..........1843 ) Loss= 0.24222529\n",
      "Training [1844/10000] ..........1844 ) Loss= 0.15612382\n",
      "Training [1845/10000] ..........1845 ) Loss= 0.1671767\n",
      "Training [1846/10000] ..........1846 ) Loss= 0.22335629\n",
      "Training [1847/10000] ..........1847 ) Loss= 0.19088583\n",
      "Training [1848/10000] ..........1848 ) Loss= 0.17859459\n",
      "Training [1849/10000] ..........1849 ) Loss= 0.28267998\n",
      "Training [1850/10000] ..........1850 ) Loss= 0.53674674\n",
      "Training [1851/10000] ..........1851 ) Loss= 0.18718375\n",
      "Training [1852/10000] ..........1852 ) Loss= 0.17946097\n",
      "Training [1853/10000] ..........1853 ) Loss= 0.2111054\n",
      "Training [1854/10000] ..........1854 ) Loss= 0.20429623\n",
      "Training [1855/10000] ..........1855 ) Loss= 0.19792321\n",
      "Training [1856/10000] ..........1856 ) Loss= 0.21530557\n",
      "Training [1857/10000] ..........1857 ) Loss= 0.47893313\n",
      "Training [1858/10000] ..........1858 ) Loss= 0.21347347\n",
      "Training [1859/10000] ..........1859 ) Loss= 0.23680463\n",
      "Training [1860/10000] ..........1860 ) Loss= 0.21718083\n",
      "Training [1861/10000] ..........1861 ) Loss= 0.34284315\n",
      "Training [1862/10000] ..........1862 ) Loss= 0.19450866\n",
      "Training [1863/10000] ..........1863 ) Loss= 0.19311126\n",
      "Training [1864/10000] ..........1864 ) Loss= 0.20052148\n",
      "Training [1865/10000] ..........1865 ) Loss= 0.17874528\n",
      "Training [1866/10000] ..........1866 ) Loss= 0.12744695\n",
      "Training [1867/10000] ..........1867 ) Loss= 0.40939355\n",
      "Training [1868/10000] ..........1868 ) Loss= 0.2958778\n",
      "Training [1869/10000] ..........1869 ) Loss= 0.21843414\n",
      "Training [1870/10000] ..........1870 ) Loss= 0.32491067\n",
      "Training [1871/10000] ..........1871 ) Loss= 0.251296\n",
      "Training [1872/10000] ..........1872 ) Loss= 0.22847812\n",
      "Training [1873/10000] ..........1873 ) Loss= 0.38385296\n",
      "Training [1874/10000] ..........1874 ) Loss= 0.2686207\n",
      "Training [1875/10000] ..........1875 ) Loss= 0.2843811\n",
      "Training [1876/10000] ..........1876 ) Loss= 0.19453338\n",
      "Training [1877/10000] ..........1877 ) Loss= 0.18350154\n",
      "Training [1878/10000] ..........1878 ) Loss= 0.3262043\n",
      "Training [1879/10000] ..........1879 ) Loss= 0.17620844\n",
      "Training [1880/10000] ..........1880 ) Loss= 0.6040151\n",
      "Training [1881/10000] ..........1881 ) Loss= 0.26516622\n",
      "Training [1882/10000] ..........1882 ) Loss= 0.38183886\n",
      "Training [1883/10000] ..........1883 ) Loss= 0.15430677\n",
      "Training [1884/10000] ..........1884 ) Loss= 0.18440396\n",
      "Training [1885/10000] ..........1885 ) Loss= 0.19532874\n",
      "Training [1886/10000] ..........1886 ) Loss= 0.40712827\n",
      "Training [1887/10000] ..........1887 ) Loss= 0.23692895\n",
      "Training [1888/10000] ..........1888 ) Loss= 0.19399989\n",
      "Training [1889/10000] ..........1889 ) Loss= 0.28293857\n",
      "Training [1890/10000] ..........1890 ) Loss= 0.1467557\n",
      "Training [1891/10000] ..........1891 ) Loss= 0.26112223\n",
      "Training [1892/10000] ..........1892 ) Loss= 0.22364855\n",
      "Training [1893/10000] ..........1893 ) Loss= 0.23273043\n",
      "Training [1894/10000] ..........1894 ) Loss= 0.52129763\n",
      "Training [1895/10000] ..........1895 ) Loss= 0.40686348\n",
      "Training [1896/10000] ..........1896 ) Loss= 0.15883747\n",
      "Training [1897/10000] ..........1897 ) Loss= 0.33939543\n",
      "Training [1898/10000] ..........1898 ) Loss= 0.2551682\n",
      "Training [1899/10000] ..........1899 ) Loss= 0.23993558\n",
      "Training [1900/10000] ..........1900 ) Loss= 0.29541752\n",
      "Training [1901/10000] ..........1901 ) Loss= 0.21205372\n",
      "Training [1902/10000] ..........1902 ) Loss= 0.20380446\n",
      "Training [1903/10000] ..........1903 ) Loss= 0.19545399\n",
      "Training [1904/10000] ..........1904 ) Loss= 0.3647573\n",
      "Training [1905/10000] ..........1905 ) Loss= 0.3093218\n",
      "Training [1906/10000] ..........1906 ) Loss= 0.29596263\n",
      "Training [1907/10000] ..........1907 ) Loss= 0.21964015\n",
      "Training [1908/10000] ..........1908 ) Loss= 0.13196191\n",
      "Training [1909/10000] ..........1909 ) Loss= 0.17158704\n",
      "Training [1910/10000] ..........1910 ) Loss= 0.2242361\n",
      "Training [1911/10000] ..........1911 ) Loss= 0.18240581\n",
      "Training [1912/10000] ..........1912 ) Loss= 0.19505818\n",
      "Training [1913/10000] ..........1913 ) Loss= 0.11563182\n",
      "Training [1914/10000] ..........1914 ) Loss= 0.270082\n",
      "Training [1915/10000] ..........1915 ) Loss= 0.30874252\n",
      "Training [1916/10000] ..........1916 ) Loss= 0.2863721\n",
      "Training [1917/10000] ..........1917 ) Loss= 0.26967332\n",
      "Training [1918/10000] ..........1918 ) Loss= 0.4213413\n",
      "Training [1919/10000] ..........1919 ) Loss= 0.30463907\n",
      "Training [1920/10000] ..........1920 ) Loss= 0.3378422\n",
      "Training [1921/10000] ..........1921 ) Loss= 0.18339391\n",
      "Training [1922/10000] ..........1922 ) Loss= 0.2682602\n",
      "Training [1923/10000] ..........1923 ) Loss= 0.15461193\n",
      "Training [1924/10000] ..........1924 ) Loss= 0.2694122\n",
      "Training [1925/10000] ..........1925 ) Loss= 0.17389046\n",
      "Training [1926/10000] ..........1926 ) Loss= 0.15566444\n",
      "Training [1927/10000] ..........1927 ) Loss= 0.2788799\n",
      "Training [1928/10000] ..........1928 ) Loss= 0.16126092\n",
      "Training [1929/10000] ..........1929 ) Loss= 0.17374063\n",
      "Training [1930/10000] ..........1930 ) Loss= 0.17075293\n",
      "Training [1931/10000] ..........1931 ) Loss= 0.25968018\n",
      "Training [1932/10000] ..........1932 ) Loss= 0.14272249\n",
      "Training [1933/10000] ..........1933 ) Loss= 0.16431245\n",
      "Training [1934/10000] ..........1934 ) Loss= 0.15026274\n",
      "Training [1935/10000] ..........1935 ) Loss= 0.21413544\n",
      "Training [1936/10000] ..........1936 ) Loss= 0.21631707\n",
      "Training [1937/10000] ..........1937 ) Loss= 0.18265162\n",
      "Training [1938/10000] ..........1938 ) Loss= 0.23354712\n",
      "Training [1939/10000] ..........1939 ) Loss= 0.20090947\n",
      "Training [1940/10000] ..........1940 ) Loss= 0.28693804\n",
      "Training [1941/10000] ..........1941 ) Loss= 0.28614303\n",
      "Training [1942/10000] ..........1942 ) Loss= 0.2410676\n",
      "Training [1943/10000] ..........1943 ) Loss= 0.19786781\n",
      "Training [1944/10000] ..........1944 ) Loss= 0.23237585\n",
      "Training [1945/10000] ..........1945 ) Loss= 0.18659146\n",
      "Training [1946/10000] ..........1946 ) Loss= 0.13111955\n",
      "Training [1947/10000] ..........1947 ) Loss= 0.36307758\n",
      "Training [1948/10000] ..........1948 ) Loss= 0.1826279\n",
      "Training [1949/10000] ..........1949 ) Loss= 0.3578992\n",
      "Training [1950/10000] ..........1950 ) Loss= 0.2316042\n",
      "Training [1951/10000] ..........1951 ) Loss= 0.20089784\n",
      "Training [1952/10000] ..........1952 ) Loss= 0.23980443\n",
      "Training [1953/10000] ..........1953 ) Loss= 0.27479783\n",
      "Training [1954/10000] ..........1954 ) Loss= 0.3686883\n",
      "Training [1955/10000] ..........1955 ) Loss= 0.30447742\n",
      "Training [1956/10000] ..........1956 ) Loss= 0.22457269\n",
      "Training [1957/10000] ..........1957 ) Loss= 0.16660857\n",
      "Training [1958/10000] ..........1958 ) Loss= 0.22052757\n",
      "Training [1959/10000] ..........1959 ) Loss= 0.12617049\n",
      "Training [1960/10000] ..........1960 ) Loss= 0.2351212\n",
      "Training [1961/10000] ..........1961 ) Loss= 0.16542782\n",
      "Training [1962/10000] ..........1962 ) Loss= 0.16462679\n",
      "Training [1963/10000] ..........1963 ) Loss= 0.3064568\n",
      "Training [1964/10000] ..........1964 ) Loss= 0.31267625\n",
      "Training [1965/10000] ..........1965 ) Loss= 0.1649041\n",
      "Training [1966/10000] ..........1966 ) Loss= 0.18000066\n",
      "Training [1967/10000] ..........1967 ) Loss= 0.30408996\n",
      "Training [1968/10000] ..........1968 ) Loss= 0.22720233\n",
      "Training [1969/10000] ..........1969 ) Loss= 0.34201452\n",
      "Training [1970/10000] ..........1970 ) Loss= 0.13814297\n",
      "Training [1971/10000] ..........1971 ) Loss= 0.2280316\n",
      "Training [1972/10000] ..........1972 ) Loss= 0.2047615\n",
      "Training [1973/10000] ..........1973 ) Loss= 0.34772652\n",
      "Training [1974/10000] ..........1974 ) Loss= 0.20418705\n",
      "Training [1975/10000] ..........1975 ) Loss= 0.22278595\n",
      "Training [1976/10000] ..........1976 ) Loss= 0.3724478\n",
      "Training [1977/10000] ..........1977 ) Loss= 0.20810886\n",
      "Training [1978/10000] ..........1978 ) Loss= 0.2239249\n",
      "Training [1979/10000] ..........1979 ) Loss= 0.16447946\n",
      "Training [1980/10000] ..........1980 ) Loss= 0.35487473\n",
      "Training [1981/10000] ..........1981 ) Loss= 0.1740083\n",
      "Training [1982/10000] ..........1982 ) Loss= 0.16671273\n",
      "Training [1983/10000] ..........1983 ) Loss= 0.3150017\n",
      "Training [1984/10000] ..........1984 ) Loss= 0.17301151\n",
      "Training [1985/10000] ..........1985 ) Loss= 0.19134396\n",
      "Training [1986/10000] ..........1986 ) Loss= 0.34552807\n",
      "Training [1987/10000] ..........1987 ) Loss= 0.30732864\n",
      "Training [1988/10000] ..........1988 ) Loss= 0.3043302\n",
      "Training [1989/10000] ..........1989 ) Loss= 0.33060497\n",
      "Training [1990/10000] ..........1990 ) Loss= 0.26006863\n",
      "Training [1991/10000] ..........1991 ) Loss= 0.25603226\n",
      "Training [1992/10000] ..........1992 ) Loss= 0.3021477\n",
      "Training [1993/10000] ..........1993 ) Loss= 0.40204135\n",
      "Training [1994/10000] ..........1994 ) Loss= 0.51989275\n",
      "Training [1995/10000] ..........1995 ) Loss= 0.48708588\n",
      "Training [1996/10000] ..........1996 ) Loss= 0.28061295\n",
      "Training [1997/10000] ..........1997 ) Loss= 0.23814052\n",
      "Training [1998/10000] ..........1998 ) Loss= 0.41561463\n",
      "Training [1999/10000] ..........1999 ) Loss= 0.17834526\n",
      "Training [2000/10000] ..........2000 ) Loss= 0.20427221 - Saving Model2000.torch\n",
      "Training [2001/10000] ..........2001 ) Loss= 0.29287738\n",
      "Training [2002/10000] ..........2002 ) Loss= 0.3110035\n",
      "Training [2003/10000] ..........2003 ) Loss= 0.15563735\n",
      "Training [2004/10000] ..........2004 ) Loss= 0.39180663\n",
      "Training [2005/10000] ..........2005 ) Loss= 0.26665753\n",
      "Training [2006/10000] ..........2006 ) Loss= 0.23047443\n",
      "Training [2007/10000] ..........2007 ) Loss= 0.15484984\n",
      "Training [2008/10000] ..........2008 ) Loss= 0.27296448\n",
      "Training [2009/10000] ..........2009 ) Loss= 0.23675016\n",
      "Training [2010/10000] ..........2010 ) Loss= 0.13668264\n",
      "Training [2011/10000] ..........2011 ) Loss= 0.19342238\n",
      "Training [2012/10000] ..........2012 ) Loss= 0.27137974\n",
      "Training [2013/10000] ..........2013 ) Loss= 0.1966297\n",
      "Training [2014/10000] ..........2014 ) Loss= 0.35393485\n",
      "Training [2015/10000] ..........2015 ) Loss= 0.15200695\n",
      "Training [2016/10000] ..........2016 ) Loss= 0.3020844\n",
      "Training [2017/10000] ..........2017 ) Loss= 0.17866291\n",
      "Training [2018/10000] ..........2018 ) Loss= 0.15245463\n",
      "Training [2019/10000] ..........2019 ) Loss= 0.2892291\n",
      "Training [2020/10000] ..........2020 ) Loss= 0.20041123\n",
      "Training [2021/10000] ..........2021 ) Loss= 0.23632279\n",
      "Training [2022/10000] ..........2022 ) Loss= 0.19888414\n",
      "Training [2023/10000] ..........2023 ) Loss= 0.18363683\n",
      "Training [2024/10000] ..........2024 ) Loss= 0.340881\n",
      "Training [2025/10000] ..........2025 ) Loss= 0.20060167\n",
      "Training [2026/10000] ..........2026 ) Loss= 0.30479005\n",
      "Training [2027/10000] ..........2027 ) Loss= 0.2074856\n",
      "Training [2028/10000] ..........2028 ) Loss= 0.47138807\n",
      "Training [2029/10000] ..........2029 ) Loss= 0.17652191\n",
      "Training [2030/10000] ..........2030 ) Loss= 0.32399604\n",
      "Training [2031/10000] ..........2031 ) Loss= 0.16752936\n",
      "Training [2032/10000] ..........2032 ) Loss= 0.22675866\n",
      "Training [2033/10000] ..........2033 ) Loss= 0.29606482\n",
      "Training [2034/10000] ..........2034 ) Loss= 0.1373327\n",
      "Training [2035/10000] ..........2035 ) Loss= 0.2608709\n",
      "Training [2036/10000] ..........2036 ) Loss= 0.15967472\n",
      "Training [2037/10000] ..........2037 ) Loss= 0.32141745\n",
      "Training [2038/10000] ..........2038 ) Loss= 0.19971992\n",
      "Training [2039/10000] ..........2039 ) Loss= 0.18608412\n",
      "Training [2040/10000] ..........2040 ) Loss= 0.23044005\n",
      "Training [2041/10000] ..........2041 ) Loss= 0.17318062\n",
      "Training [2042/10000] ..........2042 ) Loss= 0.2729611\n",
      "Training [2043/10000] ..........2043 ) Loss= 0.16079381\n",
      "Training [2044/10000] ..........2044 ) Loss= 0.17957129\n",
      "Training [2045/10000] ..........2045 ) Loss= 0.22423922\n",
      "Training [2046/10000] ..........2046 ) Loss= 0.1884374\n",
      "Training [2047/10000] ..........2047 ) Loss= 0.26145563\n",
      "Training [2048/10000] ..........2048 ) Loss= 0.23320806\n",
      "Training [2049/10000] ..........2049 ) Loss= 0.13709436\n",
      "Training [2050/10000] ..........2050 ) Loss= 0.16038328\n",
      "Training [2051/10000] ..........2051 ) Loss= 0.30929732\n",
      "Training [2052/10000] ..........2052 ) Loss= 0.21652648\n",
      "Training [2053/10000] ..........2053 ) Loss= 0.18320554\n",
      "Training [2054/10000] ..........2054 ) Loss= 0.20072074\n",
      "Training [2055/10000] ..........2055 ) Loss= 0.15968598\n",
      "Training [2056/10000] ..........2056 ) Loss= 0.2246366\n",
      "Training [2057/10000] ..........2057 ) Loss= 0.2534198\n",
      "Training [2058/10000] ..........2058 ) Loss= 0.20421377\n",
      "Training [2059/10000] ..........2059 ) Loss= 0.18506338\n",
      "Training [2060/10000] ..........2060 ) Loss= 0.27252135\n",
      "Training [2061/10000] ..........2061 ) Loss= 0.22454284\n",
      "Training [2062/10000] ..........2062 ) Loss= 0.15676406\n",
      "Training [2063/10000] ..........2063 ) Loss= 0.21723476\n",
      "Training [2064/10000] ..........2064 ) Loss= 0.41423535\n",
      "Training [2065/10000] ..........2065 ) Loss= 0.19567063\n",
      "Training [2066/10000] ..........2066 ) Loss= 0.1424435\n",
      "Training [2067/10000] ..........2067 ) Loss= 0.12652834\n",
      "Training [2068/10000] ..........2068 ) Loss= 0.22098128\n",
      "Training [2069/10000] ..........2069 ) Loss= 0.12186547\n",
      "Training [2070/10000] ..........2070 ) Loss= 0.16395614\n",
      "Training [2071/10000] ..........2071 ) Loss= 0.13383685\n",
      "Training [2072/10000] ..........2072 ) Loss= 0.18966575\n",
      "Training [2073/10000] ..........2073 ) Loss= 0.16958854\n",
      "Training [2074/10000] ..........2074 ) Loss= 0.16942717\n",
      "Training [2075/10000] ..........2075 ) Loss= 0.24232325\n",
      "Training [2076/10000] ..........2076 ) Loss= 0.20704803\n",
      "Training [2077/10000] ..........2077 ) Loss= 0.26989555\n",
      "Training [2078/10000] ..........2078 ) Loss= 0.26821604\n",
      "Training [2079/10000] ..........2079 ) Loss= 0.11723962\n",
      "Training [2080/10000] ..........2080 ) Loss= 0.19334406\n",
      "Training [2081/10000] ..........2081 ) Loss= 0.17831735\n",
      "Training [2082/10000] ..........2082 ) Loss= 0.26370883\n",
      "Training [2083/10000] ..........2083 ) Loss= 0.20180884\n",
      "Training [2084/10000] ..........2084 ) Loss= 0.20127626\n",
      "Training [2085/10000] ..........2085 ) Loss= 0.20263806\n",
      "Training [2086/10000] ..........2086 ) Loss= 0.14807893\n",
      "Training [2087/10000] ..........2087 ) Loss= 0.1906758\n",
      "Training [2088/10000] ..........2088 ) Loss= 0.28476444\n",
      "Training [2089/10000] ..........2089 ) Loss= 0.23181221\n",
      "Training [2090/10000] ..........2090 ) Loss= 0.18250008\n",
      "Training [2091/10000] ..........2091 ) Loss= 0.14120258\n",
      "Training [2092/10000] ..........2092 ) Loss= 0.150434\n",
      "Training [2093/10000] ..........2093 ) Loss= 0.112913996\n",
      "Training [2094/10000] ..........2094 ) Loss= 0.16035208\n",
      "Training [2095/10000] ..........2095 ) Loss= 0.13855217\n",
      "Training [2096/10000] ..........2096 ) Loss= 0.31797945\n",
      "Training [2097/10000] ..........2097 ) Loss= 0.34875694\n",
      "Training [2098/10000] ..........2098 ) Loss= 0.12934352\n",
      "Training [2099/10000] ..........2099 ) Loss= 0.19109602\n",
      "Training [2100/10000] ..........2100 ) Loss= 0.26652542\n",
      "Training [2101/10000] ..........2101 ) Loss= 0.26988518\n",
      "Training [2102/10000] ..........2102 ) Loss= 0.2070546\n",
      "Training [2103/10000] ..........2103 ) Loss= 0.14812638\n",
      "Training [2104/10000] ..........2104 ) Loss= 0.2633989\n",
      "Training [2105/10000] ..........2105 ) Loss= 0.12522367\n",
      "Training [2106/10000] ..........2106 ) Loss= 0.16529222\n",
      "Training [2107/10000] ..........2107 ) Loss= 0.29131308\n",
      "Training [2108/10000] ..........2108 ) Loss= 0.37729985\n",
      "Training [2109/10000] ..........2109 ) Loss= 0.47744223\n",
      "Training [2110/10000] ..........2110 ) Loss= 0.21758154\n",
      "Training [2111/10000] ..........2111 ) Loss= 0.18856719\n",
      "Training [2112/10000] ..........2112 ) Loss= 0.17809622\n",
      "Training [2113/10000] ..........2113 ) Loss= 0.6457133\n",
      "Training [2114/10000] ..........2114 ) Loss= 0.1414389\n",
      "Training [2115/10000] ..........2115 ) Loss= 0.2209452\n",
      "Training [2116/10000] ..........2116 ) Loss= 0.65256727\n",
      "Training [2117/10000] ..........2117 ) Loss= 0.2527088\n",
      "Training [2118/10000] ..........2118 ) Loss= 0.38859665\n",
      "Training [2119/10000] ..........2119 ) Loss= 0.18667711\n",
      "Training [2120/10000] ..........2120 ) Loss= 0.34600025\n",
      "Training [2121/10000] ..........2121 ) Loss= 0.25909865\n",
      "Training [2122/10000] ..........2122 ) Loss= 0.12879181\n",
      "Training [2123/10000] ..........2123 ) Loss= 0.16096818\n",
      "Training [2124/10000] ..........2124 ) Loss= 0.39759496\n",
      "Training [2125/10000] ..........2125 ) Loss= 0.16547662\n",
      "Training [2126/10000] ..........2126 ) Loss= 0.20315865\n",
      "Training [2127/10000] ..........2127 ) Loss= 0.17625947\n",
      "Training [2128/10000] ..........2128 ) Loss= 0.16756748\n",
      "Training [2129/10000] ..........2129 ) Loss= 0.24115176\n",
      "Training [2130/10000] ..........2130 ) Loss= 0.28216508\n",
      "Training [2131/10000] ..........2131 ) Loss= 0.21421786\n",
      "Training [2132/10000] ..........2132 ) Loss= 0.41557872\n",
      "Training [2133/10000] ..........2133 ) Loss= 0.2106726\n",
      "Training [2134/10000] ..........2134 ) Loss= 0.21962966\n",
      "Training [2135/10000] ..........2135 ) Loss= 0.27373272\n",
      "Training [2136/10000] ..........2136 ) Loss= 0.27333277\n",
      "Training [2137/10000] ..........2137 ) Loss= 0.16926296\n",
      "Training [2138/10000] ..........2138 ) Loss= 0.27034524\n",
      "Training [2139/10000] ..........2139 ) Loss= 0.19507061\n",
      "Training [2140/10000] ..........2140 ) Loss= 0.17409466\n",
      "Training [2141/10000] ..........2141 ) Loss= 0.18675785\n",
      "Training [2142/10000] ..........2142 ) Loss= 0.19651845\n",
      "Training [2143/10000] ..........2143 ) Loss= 0.1439743\n",
      "Training [2144/10000] ..........2144 ) Loss= 0.29762673\n",
      "Training [2145/10000] ..........2145 ) Loss= 0.2410508\n",
      "Training [2146/10000] ..........2146 ) Loss= 0.24503444\n",
      "Training [2147/10000] ..........2147 ) Loss= 0.17637542\n",
      "Training [2148/10000] ..........2148 ) Loss= 0.17559247\n",
      "Training [2149/10000] ..........2149 ) Loss= 0.4217242\n",
      "Training [2150/10000] ..........2150 ) Loss= 0.12240559\n",
      "Training [2151/10000] ..........2151 ) Loss= 0.27928704\n",
      "Training [2152/10000] ..........2152 ) Loss= 0.19972312\n",
      "Training [2153/10000] ..........2153 ) Loss= 0.36861187\n",
      "Training [2154/10000] ..........2154 ) Loss= 0.22414427\n",
      "Training [2155/10000] ..........2155 ) Loss= 0.2522251\n",
      "Training [2156/10000] ..........2156 ) Loss= 0.13338523\n",
      "Training [2157/10000] ..........2157 ) Loss= 0.28549448\n",
      "Training [2158/10000] ..........2158 ) Loss= 0.33118528\n",
      "Training [2159/10000] ..........2159 ) Loss= 0.25857663\n",
      "Training [2160/10000] ..........2160 ) Loss= 0.26037934\n",
      "Training [2161/10000] ..........2161 ) Loss= 0.22244969\n",
      "Training [2162/10000] ..........2162 ) Loss= 0.16543883\n",
      "Training [2163/10000] ..........2163 ) Loss= 0.23151249\n",
      "Training [2164/10000] ..........2164 ) Loss= 0.20306756\n",
      "Training [2165/10000] ..........2165 ) Loss= 0.116259426\n",
      "Training [2166/10000] ..........2166 ) Loss= 0.26440558\n",
      "Training [2167/10000] ..........2167 ) Loss= 0.23430143\n",
      "Training [2168/10000] ..........2168 ) Loss= 0.15789382\n",
      "Training [2169/10000] ..........2169 ) Loss= 0.28430668\n",
      "Training [2170/10000] ..........2170 ) Loss= 0.36546016\n",
      "Training [2171/10000] ..........2171 ) Loss= 0.16350888\n",
      "Training [2172/10000] ..........2172 ) Loss= 0.16901061\n",
      "Training [2173/10000] ..........2173 ) Loss= 0.38168964\n",
      "Training [2174/10000] ..........2174 ) Loss= 0.23180011\n",
      "Training [2175/10000] ..........2175 ) Loss= 0.14968686\n",
      "Training [2176/10000] ..........2176 ) Loss= 0.27518666\n",
      "Training [2177/10000] ..........2177 ) Loss= 0.13658169\n",
      "Training [2178/10000] ..........2178 ) Loss= 0.20548883\n",
      "Training [2179/10000] ..........2179 ) Loss= 0.14135064\n",
      "Training [2180/10000] ..........2180 ) Loss= 0.18636951\n",
      "Training [2181/10000] ..........2181 ) Loss= 0.29698446\n",
      "Training [2182/10000] ..........2182 ) Loss= 0.14005075\n",
      "Training [2183/10000] ..........2183 ) Loss= 0.19710031\n",
      "Training [2184/10000] ..........2184 ) Loss= 0.17861147\n",
      "Training [2185/10000] ..........2185 ) Loss= 0.119400345\n",
      "Training [2186/10000] ..........2186 ) Loss= 0.19856817\n",
      "Training [2187/10000] ..........2187 ) Loss= 0.12411716\n",
      "Training [2188/10000] ..........2188 ) Loss= 0.26766977\n",
      "Training [2189/10000] ..........2189 ) Loss= 0.22231974\n",
      "Training [2190/10000] ..........2190 ) Loss= 0.25691667\n",
      "Training [2191/10000] ..........2191 ) Loss= 0.12955385\n",
      "Training [2192/10000] ..........2192 ) Loss= 0.22994447\n",
      "Training [2193/10000] ..........2193 ) Loss= 0.21067603\n",
      "Training [2194/10000] ..........2194 ) Loss= 0.22908409\n",
      "Training [2195/10000] ..........2195 ) Loss= 0.19610062\n",
      "Training [2196/10000] ..........2196 ) Loss= 0.2897182\n",
      "Training [2197/10000] ..........2197 ) Loss= 0.23280348\n",
      "Training [2198/10000] ..........2198 ) Loss= 0.2620543\n",
      "Training [2199/10000] ..........2199 ) Loss= 0.10945698\n",
      "Training [2200/10000] ..........2200 ) Loss= 0.15585892\n",
      "Training [2201/10000] ..........2201 ) Loss= 0.17921267\n",
      "Training [2202/10000] ..........2202 ) Loss= 0.24875638\n",
      "Training [2203/10000] ..........2203 ) Loss= 0.33710223\n",
      "Training [2204/10000] ..........2204 ) Loss= 0.18881264\n",
      "Training [2205/10000] ..........2205 ) Loss= 0.24535888\n",
      "Training [2206/10000] ..........2206 ) Loss= 0.14628842\n",
      "Training [2207/10000] ..........2207 ) Loss= 0.14975727\n",
      "Training [2208/10000] ..........2208 ) Loss= 0.30975935\n",
      "Training [2209/10000] ..........2209 ) Loss= 0.1611688\n",
      "Training [2210/10000] ..........2210 ) Loss= 0.14620546\n",
      "Training [2211/10000] ..........2211 ) Loss= 0.42670485\n",
      "Training [2212/10000] ..........2212 ) Loss= 0.30934557\n",
      "Training [2213/10000] ..........2213 ) Loss= 0.18085809\n",
      "Training [2214/10000] ..........2214 ) Loss= 0.21090128\n",
      "Training [2215/10000] ..........2215 ) Loss= 0.15214054\n",
      "Training [2216/10000] ..........2216 ) Loss= 0.17759307\n",
      "Training [2217/10000] ..........2217 ) Loss= 0.25007477\n",
      "Training [2218/10000] ..........2218 ) Loss= 0.32566026\n",
      "Training [2219/10000] ..........2219 ) Loss= 0.17157406\n",
      "Training [2220/10000] ..........2220 ) Loss= 0.15864865\n",
      "Training [2221/10000] ..........2221 ) Loss= 0.14787686\n",
      "Training [2222/10000] ..........2222 ) Loss= 0.13730085\n",
      "Training [2223/10000] ..........2223 ) Loss= 0.13177778\n",
      "Training [2224/10000] ..........2224 ) Loss= 0.38402113\n",
      "Training [2225/10000] ..........2225 ) Loss= 0.28398883\n",
      "Training [2226/10000] ..........2226 ) Loss= 0.16509487\n",
      "Training [2227/10000] ..........2227 ) Loss= 0.4202507\n",
      "Training [2228/10000] ..........2228 ) Loss= 0.17176875\n",
      "Training [2229/10000] ..........2229 ) Loss= 0.1976529\n",
      "Training [2230/10000] ..........2230 ) Loss= 0.1498523\n",
      "Training [2231/10000] ..........2231 ) Loss= 0.2186202\n",
      "Training [2232/10000] ..........2232 ) Loss= 0.29665074\n",
      "Training [2233/10000] ..........2233 ) Loss= 0.26573858\n",
      "Training [2234/10000] ..........2234 ) Loss= 0.3299441\n",
      "Training [2235/10000] ..........2235 ) Loss= 0.2098263\n",
      "Training [2236/10000] ..........2236 ) Loss= 0.10248475\n",
      "Training [2237/10000] ..........2237 ) Loss= 0.16047142\n",
      "Training [2238/10000] ..........2238 ) Loss= 0.16246302\n",
      "Training [2239/10000] ..........2239 ) Loss= 0.19964233\n",
      "Training [2240/10000] ..........2240 ) Loss= 0.12817292\n",
      "Training [2241/10000] ..........2241 ) Loss= 0.1754676\n",
      "Training [2242/10000] ..........2242 ) Loss= 0.21159084\n",
      "Training [2243/10000] ..........2243 ) Loss= 0.19665867\n",
      "Training [2244/10000] ..........2244 ) Loss= 0.20903565\n",
      "Training [2245/10000] ..........2245 ) Loss= 0.290985\n",
      "Training [2246/10000] ..........2246 ) Loss= 0.20910105\n",
      "Training [2247/10000] ..........2247 ) Loss= 0.20746295\n",
      "Training [2248/10000] ..........2248 ) Loss= 0.18361223\n",
      "Training [2249/10000] ..........2249 ) Loss= 0.1952008\n",
      "Training [2250/10000] ..........2250 ) Loss= 0.22633004\n",
      "Training [2251/10000] ..........2251 ) Loss= 0.29281387\n",
      "Training [2252/10000] ..........2252 ) Loss= 0.36584467\n",
      "Training [2253/10000] ..........2253 ) Loss= 0.19690202\n",
      "Training [2254/10000] ..........2254 ) Loss= 0.19485867\n",
      "Training [2255/10000] ..........2255 ) Loss= 0.15200782\n",
      "Training [2256/10000] ..........2256 ) Loss= 0.27419555\n",
      "Training [2257/10000] ..........2257 ) Loss= 0.13994049\n",
      "Training [2258/10000] ..........2258 ) Loss= 0.17348763\n",
      "Training [2259/10000] ..........2259 ) Loss= 0.1856128\n",
      "Training [2260/10000] ..........2260 ) Loss= 0.3287473\n",
      "Training [2261/10000] ..........2261 ) Loss= 0.2570776\n",
      "Training [2262/10000] ..........2262 ) Loss= 0.169831\n",
      "Training [2263/10000] ..........2263 ) Loss= 0.15936011\n",
      "Training [2264/10000] ..........2264 ) Loss= 0.1854662\n",
      "Training [2265/10000] ..........2265 ) Loss= 0.21724711\n",
      "Training [2266/10000] ..........2266 ) Loss= 0.1195907\n",
      "Training [2267/10000] ..........2267 ) Loss= 0.24794231\n",
      "Training [2268/10000] ..........2268 ) Loss= 0.23527326\n",
      "Training [2269/10000] ..........2269 ) Loss= 0.15228164\n",
      "Training [2270/10000] ..........2270 ) Loss= 0.2300962\n",
      "Training [2271/10000] ..........2271 ) Loss= 0.14941834\n",
      "Training [2272/10000] ..........2272 ) Loss= 0.1389203\n",
      "Training [2273/10000] ..........2273 ) Loss= 0.16152301\n",
      "Training [2274/10000] ..........2274 ) Loss= 0.20328939\n",
      "Training [2275/10000] ..........2275 ) Loss= 0.19462368\n",
      "Training [2276/10000] ..........2276 ) Loss= 0.21521966\n",
      "Training [2277/10000] ..........2277 ) Loss= 0.21577401\n",
      "Training [2278/10000] ..........2278 ) Loss= 0.24379131\n",
      "Training [2279/10000] ..........2279 ) Loss= 0.19460197\n",
      "Training [2280/10000] ..........2280 ) Loss= 0.2790648\n",
      "Training [2281/10000] ..........2281 ) Loss= 0.18632105\n",
      "Training [2282/10000] ..........2282 ) Loss= 0.26899925\n",
      "Training [2283/10000] ..........2283 ) Loss= 0.23052551\n",
      "Training [2284/10000] ..........2284 ) Loss= 0.2581503\n",
      "Training [2285/10000] ..........2285 ) Loss= 0.1936652\n",
      "Training [2286/10000] ..........2286 ) Loss= 0.30396563\n",
      "Training [2287/10000] ..........2287 ) Loss= 0.13366851\n",
      "Training [2288/10000] ..........2288 ) Loss= 0.12690668\n",
      "Training [2289/10000] ..........2289 ) Loss= 0.37154618\n",
      "Training [2290/10000] ..........2290 ) Loss= 0.24095929\n",
      "Training [2291/10000] ..........2291 ) Loss= 0.15359901\n",
      "Training [2292/10000] ..........2292 ) Loss= 0.26204887\n",
      "Training [2293/10000] ..........2293 ) Loss= 0.19547683\n",
      "Training [2294/10000] ..........2294 ) Loss= 0.16957758\n",
      "Training [2295/10000] ..........2295 ) Loss= 0.13561513\n",
      "Training [2296/10000] ..........2296 ) Loss= 0.45071054\n",
      "Training [2297/10000] ..........2297 ) Loss= 0.15851128\n",
      "Training [2298/10000] ..........2298 ) Loss= 0.37859145\n",
      "Training [2299/10000] ..........2299 ) Loss= 0.18559995\n",
      "Training [2300/10000] ..........2300 ) Loss= 0.23079899\n",
      "Training [2301/10000] ..........2301 ) Loss= 0.1487921\n",
      "Training [2302/10000] ..........2302 ) Loss= 0.21383984\n",
      "Training [2303/10000] ..........2303 ) Loss= 0.45823374\n",
      "Training [2304/10000] ..........2304 ) Loss= 0.13936791\n",
      "Training [2305/10000] ..........2305 ) Loss= 0.12159712\n",
      "Training [2306/10000] ..........2306 ) Loss= 0.19358616\n",
      "Training [2307/10000] ..........2307 ) Loss= 0.18260944\n",
      "Training [2308/10000] ..........2308 ) Loss= 0.3742623\n",
      "Training [2309/10000] ..........2309 ) Loss= 0.14153132\n",
      "Training [2310/10000] ..........2310 ) Loss= 0.10686483\n",
      "Training [2311/10000] ..........2311 ) Loss= 0.28856996\n",
      "Training [2312/10000] ..........2312 ) Loss= 0.11986683\n",
      "Training [2313/10000] ..........2313 ) Loss= 0.12682565\n",
      "Training [2314/10000] ..........2314 ) Loss= 0.19630384\n",
      "Training [2315/10000] ..........2315 ) Loss= 0.16853364\n",
      "Training [2316/10000] ..........2316 ) Loss= 0.2652201\n",
      "Training [2317/10000] ..........2317 ) Loss= 0.16474806\n",
      "Training [2318/10000] ..........2318 ) Loss= 0.1950087\n",
      "Training [2319/10000] ..........2319 ) Loss= 0.16601151\n",
      "Training [2320/10000] ..........2320 ) Loss= 0.17557369\n",
      "Training [2321/10000] ..........2321 ) Loss= 0.38795787\n",
      "Training [2322/10000] ..........2322 ) Loss= 0.18058921\n",
      "Training [2323/10000] ..........2323 ) Loss= 0.2816454\n",
      "Training [2324/10000] ..........2324 ) Loss= 0.2408392\n",
      "Training [2325/10000] ..........2325 ) Loss= 0.26426718\n",
      "Training [2326/10000] ..........2326 ) Loss= 0.22891119\n",
      "Training [2327/10000] ..........2327 ) Loss= 0.19489042\n",
      "Training [2328/10000] ..........2328 ) Loss= 0.25389734\n",
      "Training [2329/10000] ..........2329 ) Loss= 0.234015\n",
      "Training [2330/10000] ..........2330 ) Loss= 0.1499384\n",
      "Training [2331/10000] ..........2331 ) Loss= 0.14289607\n",
      "Training [2332/10000] ..........2332 ) Loss= 0.33667517\n",
      "Training [2333/10000] ..........2333 ) Loss= 0.16132805\n",
      "Training [2334/10000] ..........2334 ) Loss= 0.29258737\n",
      "Training [2335/10000] ..........2335 ) Loss= 0.17860015\n",
      "Training [2336/10000] ..........2336 ) Loss= 0.37491933\n",
      "Training [2337/10000] ..........2337 ) Loss= 0.20022672\n",
      "Training [2338/10000] ..........2338 ) Loss= 0.18232183\n",
      "Training [2339/10000] ..........2339 ) Loss= 0.18214847\n",
      "Training [2340/10000] ..........2340 ) Loss= 0.19656132\n",
      "Training [2341/10000] ..........2341 ) Loss= 0.3054219\n",
      "Training [2342/10000] ..........2342 ) Loss= 0.17841192\n",
      "Training [2343/10000] ..........2343 ) Loss= 0.20315406\n",
      "Training [2344/10000] ..........2344 ) Loss= 0.19058798\n",
      "Training [2345/10000] ..........2345 ) Loss= 0.16822052\n",
      "Training [2346/10000] ..........2346 ) Loss= 0.22665359\n",
      "Training [2347/10000] ..........2347 ) Loss= 0.18516178\n",
      "Training [2348/10000] ..........2348 ) Loss= 0.54889864\n",
      "Training [2349/10000] ..........2349 ) Loss= 0.27993894\n",
      "Training [2350/10000] ..........2350 ) Loss= 0.32861817\n",
      "Training [2351/10000] ..........2351 ) Loss= 0.18487072\n",
      "Training [2352/10000] ..........2352 ) Loss= 0.16120897\n",
      "Training [2353/10000] ..........2353 ) Loss= 0.14554527\n",
      "Training [2354/10000] ..........2354 ) Loss= 0.39550635\n",
      "Training [2355/10000] ..........2355 ) Loss= 0.21388362\n",
      "Training [2356/10000] ..........2356 ) Loss= 0.11468335\n",
      "Training [2357/10000] ..........2357 ) Loss= 0.2555694\n",
      "Training [2358/10000] ..........2358 ) Loss= 0.23722182\n",
      "Training [2359/10000] ..........2359 ) Loss= 0.21669844\n",
      "Training [2360/10000] ..........2360 ) Loss= 0.26360163\n",
      "Training [2361/10000] ..........2361 ) Loss= 0.13297838\n",
      "Training [2362/10000] ..........2362 ) Loss= 0.27935186\n",
      "Training [2363/10000] ..........2363 ) Loss= 0.14141472\n",
      "Training [2364/10000] ..........2364 ) Loss= 0.30524862\n",
      "Training [2365/10000] ..........2365 ) Loss= 0.36213166\n",
      "Training [2366/10000] ..........2366 ) Loss= 0.2057069\n",
      "Training [2367/10000] ..........2367 ) Loss= 0.20064905\n",
      "Training [2368/10000] ..........2368 ) Loss= 0.3209783\n",
      "Training [2369/10000] ..........2369 ) Loss= 0.14022323\n",
      "Training [2370/10000] ..........2370 ) Loss= 0.22418237\n",
      "Training [2371/10000] ..........2371 ) Loss= 0.4052975\n",
      "Training [2372/10000] ..........2372 ) Loss= 0.20805562\n",
      "Training [2373/10000] ..........2373 ) Loss= 0.120131806\n",
      "Training [2374/10000] ..........2374 ) Loss= 0.19655046\n",
      "Training [2375/10000] ..........2375 ) Loss= 0.17858824\n",
      "Training [2376/10000] ..........2376 ) Loss= 0.17194732\n",
      "Training [2377/10000] ..........2377 ) Loss= 0.19189575\n",
      "Training [2378/10000] ..........2378 ) Loss= 0.14777839\n",
      "Training [2379/10000] ..........2379 ) Loss= 0.13057041\n",
      "Training [2380/10000] ..........2380 ) Loss= 0.21162075\n",
      "Training [2381/10000] ..........2381 ) Loss= 0.15789866\n",
      "Training [2382/10000] ..........2382 ) Loss= 0.15457927\n",
      "Training [2383/10000] ..........2383 ) Loss= 0.37587863\n",
      "Training [2384/10000] ..........2384 ) Loss= 0.18381208\n",
      "Training [2385/10000] ..........2385 ) Loss= 0.19732767\n",
      "Training [2386/10000] ..........2386 ) Loss= 0.20261024\n",
      "Training [2387/10000] ..........2387 ) Loss= 0.20787464\n",
      "Training [2388/10000] ..........2388 ) Loss= 0.20516868\n",
      "Training [2389/10000] ..........2389 ) Loss= 0.2945196\n",
      "Training [2390/10000] ..........2390 ) Loss= 0.24945116\n",
      "Training [2391/10000] ..........2391 ) Loss= 0.11723807\n",
      "Training [2392/10000] ..........2392 ) Loss= 0.18029945\n",
      "Training [2393/10000] ..........2393 ) Loss= 0.19036986\n",
      "Training [2394/10000] ..........2394 ) Loss= 0.25541925\n",
      "Training [2395/10000] ..........2395 ) Loss= 0.14941835\n",
      "Training [2396/10000] ..........2396 ) Loss= 0.24709335\n",
      "Training [2397/10000] ..........2397 ) Loss= 0.1962286\n",
      "Training [2398/10000] ..........2398 ) Loss= 0.18023175\n",
      "Training [2399/10000] ..........2399 ) Loss= 0.21458022\n",
      "Training [2400/10000] ..........2400 ) Loss= 0.12508903\n",
      "Training [2401/10000] ..........2401 ) Loss= 0.32519558\n",
      "Training [2402/10000] ..........2402 ) Loss= 0.20764701\n",
      "Training [2403/10000] ..........2403 ) Loss= 0.17259435\n",
      "Training [2404/10000] ..........2404 ) Loss= 0.16353287\n",
      "Training [2405/10000] ..........2405 ) Loss= 0.37998837\n",
      "Training [2406/10000] ..........2406 ) Loss= 0.25920948\n",
      "Training [2407/10000] ..........2407 ) Loss= 0.13564591\n",
      "Training [2408/10000] ..........2408 ) Loss= 0.16921164\n",
      "Training [2409/10000] ..........2409 ) Loss= 0.26036367\n",
      "Training [2410/10000] ..........2410 ) Loss= 0.32500926\n",
      "Training [2411/10000] ..........2411 ) Loss= 0.34912217\n",
      "Training [2412/10000] ..........2412 ) Loss= 0.27975616\n",
      "Training [2413/10000] ..........2413 ) Loss= 0.24883553\n",
      "Training [2414/10000] ..........2414 ) Loss= 0.25392553\n",
      "Training [2415/10000] ..........2415 ) Loss= 0.20729776\n",
      "Training [2416/10000] ..........2416 ) Loss= 0.1790423\n",
      "Training [2417/10000] ..........2417 ) Loss= 0.3035013\n",
      "Training [2418/10000] ..........2418 ) Loss= 0.17729926\n",
      "Training [2419/10000] ..........2419 ) Loss= 0.31407747\n",
      "Training [2420/10000] ..........2420 ) Loss= 0.2466668\n",
      "Training [2421/10000] ..........2421 ) Loss= 0.21303552\n",
      "Training [2422/10000] ..........2422 ) Loss= 0.2330357\n",
      "Training [2423/10000] ..........2423 ) Loss= 0.3064675\n",
      "Training [2424/10000] ..........2424 ) Loss= 0.28687996\n",
      "Training [2425/10000] ..........2425 ) Loss= 0.20670311\n",
      "Training [2426/10000] ..........2426 ) Loss= 0.45387223\n",
      "Training [2427/10000] ..........2427 ) Loss= 0.18285841\n",
      "Training [2428/10000] ..........2428 ) Loss= 0.33015364\n",
      "Training [2429/10000] ..........2429 ) Loss= 0.25949895\n",
      "Training [2430/10000] ..........2430 ) Loss= 0.4851895\n",
      "Training [2431/10000] ..........2431 ) Loss= 0.11886486\n",
      "Training [2432/10000] ..........2432 ) Loss= 0.1516897\n",
      "Training [2433/10000] ..........2433 ) Loss= 0.14818607\n",
      "Training [2434/10000] ..........2434 ) Loss= 0.25120807\n",
      "Training [2435/10000] ..........2435 ) Loss= 0.202366\n",
      "Training [2436/10000] ..........2436 ) Loss= 0.25688192\n",
      "Training [2437/10000] ..........2437 ) Loss= 0.2012222\n",
      "Training [2438/10000] ..........2438 ) Loss= 0.15535001\n",
      "Training [2439/10000] ..........2439 ) Loss= 0.3025096\n",
      "Training [2440/10000] ..........2440 ) Loss= 0.14137505\n",
      "Training [2441/10000] ..........2441 ) Loss= 0.28359893\n",
      "Training [2442/10000] ..........2442 ) Loss= 0.29956153\n",
      "Training [2443/10000] ..........2443 ) Loss= 0.14288512\n",
      "Training [2444/10000] ..........2444 ) Loss= 0.23431993\n",
      "Training [2445/10000] ..........2445 ) Loss= 0.25934198\n",
      "Training [2446/10000] ..........2446 ) Loss= 0.17630197\n",
      "Training [2447/10000] ..........2447 ) Loss= 0.5994671\n",
      "Training [2448/10000] ..........2448 ) Loss= 0.10308034\n",
      "Training [2449/10000] ..........2449 ) Loss= 0.2734244\n",
      "Training [2450/10000] ..........2450 ) Loss= 0.19763269\n",
      "Training [2451/10000] ..........2451 ) Loss= 0.14743525\n",
      "Training [2452/10000] ..........2452 ) Loss= 0.13317944\n",
      "Training [2453/10000] ..........2453 ) Loss= 0.36042565\n",
      "Training [2454/10000] ..........2454 ) Loss= 0.35775048\n",
      "Training [2455/10000] ..........2455 ) Loss= 0.25005057\n",
      "Training [2456/10000] ..........2456 ) Loss= 0.16415806\n",
      "Training [2457/10000] ..........2457 ) Loss= 0.25192398\n",
      "Training [2458/10000] ..........2458 ) Loss= 0.18748547\n",
      "Training [2459/10000] ..........2459 ) Loss= 0.21632046\n",
      "Training [2460/10000] ..........2460 ) Loss= 0.16486973\n",
      "Training [2461/10000] ..........2461 ) Loss= 0.22489627\n",
      "Training [2462/10000] ..........2462 ) Loss= 0.17204155\n",
      "Training [2463/10000] ..........2463 ) Loss= 0.23585227\n",
      "Training [2464/10000] ..........2464 ) Loss= 0.1938177\n",
      "Training [2465/10000] ..........2465 ) Loss= 0.25174385\n",
      "Training [2466/10000] ..........2466 ) Loss= 0.19631575\n",
      "Training [2467/10000] ..........2467 ) Loss= 0.1794966\n",
      "Training [2468/10000] ..........2468 ) Loss= 0.4292448\n",
      "Training [2469/10000] ..........2469 ) Loss= 0.15097287\n",
      "Training [2470/10000] ..........2470 ) Loss= 0.16656809\n",
      "Training [2471/10000] ..........2471 ) Loss= 0.1492694\n",
      "Training [2472/10000] ..........2472 ) Loss= 0.25074017\n",
      "Training [2473/10000] ..........2473 ) Loss= 0.24882977\n",
      "Training [2474/10000] ..........2474 ) Loss= 0.43926394\n",
      "Training [2475/10000] ..........2475 ) Loss= 0.2263644\n",
      "Training [2476/10000] ..........2476 ) Loss= 0.29710922\n",
      "Training [2477/10000] ..........2477 ) Loss= 0.21389847\n",
      "Training [2478/10000] ..........2478 ) Loss= 0.21221691\n",
      "Training [2479/10000] ..........2479 ) Loss= 0.21799392\n",
      "Training [2480/10000] ..........2480 ) Loss= 0.28594548\n",
      "Training [2481/10000] ..........2481 ) Loss= 0.18952487\n",
      "Training [2482/10000] ..........2482 ) Loss= 0.23489037\n",
      "Training [2483/10000] ..........2483 ) Loss= 0.24666601\n",
      "Training [2484/10000] ..........2484 ) Loss= 0.40060115\n",
      "Training [2485/10000] ..........2485 ) Loss= 0.1473832\n",
      "Training [2486/10000] ..........2486 ) Loss= 0.24229862\n",
      "Training [2487/10000] ..........2487 ) Loss= 0.17695767\n",
      "Training [2488/10000] ..........2488 ) Loss= 0.13758202\n",
      "Training [2489/10000] ..........2489 ) Loss= 0.19244424\n",
      "Training [2490/10000] ..........2490 ) Loss= 0.12558909\n",
      "Training [2491/10000] ..........2491 ) Loss= 0.26191568\n",
      "Training [2492/10000] ..........2492 ) Loss= 0.244944\n",
      "Training [2493/10000] ..........2493 ) Loss= 0.13288625\n",
      "Training [2494/10000] ..........2494 ) Loss= 0.18961565\n",
      "Training [2495/10000] ..........2495 ) Loss= 0.2141672\n",
      "Training [2496/10000] ..........2496 ) Loss= 0.17152363\n",
      "Training [2497/10000] ..........2497 ) Loss= 0.32258576\n",
      "Training [2498/10000] ..........2498 ) Loss= 0.14158043\n",
      "Training [2499/10000] ..........2499 ) Loss= 0.1911112\n",
      "Training [2500/10000] ..........2500 ) Loss= 0.1644444\n",
      "Training [2501/10000] ..........2501 ) Loss= 0.12805167\n",
      "Training [2502/10000] ..........2502 ) Loss= 0.3064541\n",
      "Training [2503/10000] ..........2503 ) Loss= 0.23268028\n",
      "Training [2504/10000] ..........2504 ) Loss= 0.13388059\n",
      "Training [2505/10000] ..........2505 ) Loss= 0.17676409\n",
      "Training [2506/10000] ..........2506 ) Loss= 0.21662436\n",
      "Training [2507/10000] ..........2507 ) Loss= 0.20603362\n",
      "Training [2508/10000] ..........2508 ) Loss= 0.22214064\n",
      "Training [2509/10000] ..........2509 ) Loss= 0.238606\n",
      "Training [2510/10000] ..........2510 ) Loss= 0.3311269\n",
      "Training [2511/10000] ..........2511 ) Loss= 0.15156166\n",
      "Training [2512/10000] ..........2512 ) Loss= 0.18418327\n",
      "Training [2513/10000] ..........2513 ) Loss= 0.32097733\n",
      "Training [2514/10000] ..........2514 ) Loss= 0.15356195\n",
      "Training [2515/10000] ..........2515 ) Loss= 0.20560335\n",
      "Training [2516/10000] ..........2516 ) Loss= 0.17735542\n",
      "Training [2517/10000] ..........2517 ) Loss= 0.13876265\n",
      "Training [2518/10000] ..........2518 ) Loss= 0.14135827\n",
      "Training [2519/10000] ..........2519 ) Loss= 0.16117024\n",
      "Training [2520/10000] ..........2520 ) Loss= 0.14158694\n",
      "Training [2521/10000] ..........2521 ) Loss= 0.14848574\n",
      "Training [2522/10000] ..........2522 ) Loss= 0.13199377\n",
      "Training [2523/10000] ..........2523 ) Loss= 0.26928124\n",
      "Training [2524/10000] ..........2524 ) Loss= 0.14745417\n",
      "Training [2525/10000] ..........2525 ) Loss= 0.16513836\n",
      "Training [2526/10000] ..........2526 ) Loss= 0.28996238\n",
      "Training [2527/10000] ..........2527 ) Loss= 0.22613755\n",
      "Training [2528/10000] ..........2528 ) Loss= 0.2513826\n",
      "Training [2529/10000] ..........2529 ) Loss= 0.24512215\n",
      "Training [2530/10000] ..........2530 ) Loss= 0.19219929\n",
      "Training [2531/10000] ..........2531 ) Loss= 0.16759667\n",
      "Training [2532/10000] ..........2532 ) Loss= 0.3538136\n",
      "Training [2533/10000] ..........2533 ) Loss= 0.17479733\n",
      "Training [2534/10000] ..........2534 ) Loss= 0.21609055\n",
      "Training [2535/10000] ..........2535 ) Loss= 0.2602286\n",
      "Training [2536/10000] ..........2536 ) Loss= 0.21019092\n",
      "Training [2537/10000] ..........2537 ) Loss= 0.17760313\n",
      "Training [2538/10000] ..........2538 ) Loss= 0.1335173\n",
      "Training [2539/10000] ..........2539 ) Loss= 0.18491498\n",
      "Training [2540/10000] ..........2540 ) Loss= 0.20585927\n",
      "Training [2541/10000] ..........2541 ) Loss= 0.21103969\n",
      "Training [2542/10000] ..........2542 ) Loss= 0.24228519\n",
      "Training [2543/10000] ..........2543 ) Loss= 0.2761721\n",
      "Training [2544/10000] ..........2544 ) Loss= 0.35093126\n",
      "Training [2545/10000] ..........2545 ) Loss= 0.22964196\n",
      "Training [2546/10000] ..........2546 ) Loss= 0.33656004\n",
      "Training [2547/10000] ..........2547 ) Loss= 0.24222378\n",
      "Training [2548/10000] ..........2548 ) Loss= 0.17888828\n",
      "Training [2549/10000] ..........2549 ) Loss= 0.19453757\n",
      "Training [2550/10000] ..........2550 ) Loss= 0.2622382\n",
      "Training [2551/10000] ..........2551 ) Loss= 0.15096354\n",
      "Training [2552/10000] ..........2552 ) Loss= 0.25392893\n",
      "Training [2553/10000] ..........2553 ) Loss= 0.16093005\n",
      "Training [2554/10000] ..........2554 ) Loss= 0.2607695\n",
      "Training [2555/10000] ..........2555 ) Loss= 0.48862723\n",
      "Training [2556/10000] ..........2556 ) Loss= 0.12542734\n",
      "Training [2557/10000] ..........2557 ) Loss= 0.21607529\n",
      "Training [2558/10000] ..........2558 ) Loss= 0.26967147\n",
      "Training [2559/10000] ..........2559 ) Loss= 0.20821525\n",
      "Training [2560/10000] ..........2560 ) Loss= 0.2116333\n",
      "Training [2561/10000] ..........2561 ) Loss= 0.18701169\n",
      "Training [2562/10000] ..........2562 ) Loss= 0.4892369\n",
      "Training [2563/10000] ..........2563 ) Loss= 0.40844142\n",
      "Training [2564/10000] ..........2564 ) Loss= 0.24565431\n",
      "Training [2565/10000] ..........2565 ) Loss= 0.1396029\n",
      "Training [2566/10000] ..........2566 ) Loss= 0.18198359\n",
      "Training [2567/10000] ..........2567 ) Loss= 0.15783304\n",
      "Training [2568/10000] ..........2568 ) Loss= 0.22014356\n",
      "Training [2569/10000] ..........2569 ) Loss= 0.14988679\n",
      "Training [2570/10000] ..........2570 ) Loss= 0.37749755\n",
      "Training [2571/10000] ..........2571 ) Loss= 0.30572572\n",
      "Training [2572/10000] ..........2572 ) Loss= 0.18682273\n",
      "Training [2573/10000] ..........2573 ) Loss= 0.3695768\n",
      "Training [2574/10000] ..........2574 ) Loss= 0.17467451\n",
      "Training [2575/10000] ..........2575 ) Loss= 0.2064193\n",
      "Training [2576/10000] ..........2576 ) Loss= 0.24382173\n",
      "Training [2577/10000] ..........2577 ) Loss= 0.1349164\n",
      "Training [2578/10000] ..........2578 ) Loss= 0.1799018\n",
      "Training [2579/10000] ..........2579 ) Loss= 0.34221378\n",
      "Training [2580/10000] ..........2580 ) Loss= 0.17270367\n",
      "Training [2581/10000] ..........2581 ) Loss= 0.14824292\n",
      "Training [2582/10000] ..........2582 ) Loss= 0.22160915\n",
      "Training [2583/10000] ..........2583 ) Loss= 0.23937973\n",
      "Training [2584/10000] ..........2584 ) Loss= 0.25875908\n",
      "Training [2585/10000] ..........2585 ) Loss= 0.30872628\n",
      "Training [2586/10000] ..........2586 ) Loss= 0.26042548\n",
      "Training [2587/10000] ..........2587 ) Loss= 0.45448405\n",
      "Training [2588/10000] ..........2588 ) Loss= 0.19060445\n",
      "Training [2589/10000] ..........2589 ) Loss= 0.13535151\n",
      "Training [2590/10000] ..........2590 ) Loss= 0.1624785\n",
      "Training [2591/10000] ..........2591 ) Loss= 0.167646\n",
      "Training [2592/10000] ..........2592 ) Loss= 0.28088838\n",
      "Training [2593/10000] ..........2593 ) Loss= 0.14950065\n",
      "Training [2594/10000] ..........2594 ) Loss= 0.24047107\n",
      "Training [2595/10000] ..........2595 ) Loss= 0.22482999\n",
      "Training [2596/10000] ..........2596 ) Loss= 0.14144567\n",
      "Training [2597/10000] ..........2597 ) Loss= 0.10595192\n",
      "Training [2598/10000] ..........2598 ) Loss= 0.16324739\n",
      "Training [2599/10000] ..........2599 ) Loss= 0.20091479\n",
      "Training [2600/10000] ..........2600 ) Loss= 0.16335729\n",
      "Training [2601/10000] ..........2601 ) Loss= 0.1579126\n",
      "Training [2602/10000] ..........2602 ) Loss= 0.5637808\n",
      "Training [2603/10000] ..........2603 ) Loss= 0.3527324\n",
      "Training [2604/10000] ..........2604 ) Loss= 0.16555466\n",
      "Training [2605/10000] ..........2605 ) Loss= 0.12899378\n",
      "Training [2606/10000] ..........2606 ) Loss= 0.17378178\n",
      "Training [2607/10000] ..........2607 ) Loss= 0.26941016\n",
      "Training [2608/10000] ..........2608 ) Loss= 0.24147783\n",
      "Training [2609/10000] ..........2609 ) Loss= 0.24572483\n",
      "Training [2610/10000] ..........2610 ) Loss= 0.17865802\n",
      "Training [2611/10000] ..........2611 ) Loss= 0.34611508\n",
      "Training [2612/10000] ..........2612 ) Loss= 0.13346657\n",
      "Training [2613/10000] ..........2613 ) Loss= 0.20308837\n",
      "Training [2614/10000] ..........2614 ) Loss= 0.3577329\n",
      "Training [2615/10000] ..........2615 ) Loss= 0.14738016\n",
      "Training [2616/10000] ..........2616 ) Loss= 0.32742527\n",
      "Training [2617/10000] ..........2617 ) Loss= 0.3315773\n",
      "Training [2618/10000] ..........2618 ) Loss= 0.14139658\n",
      "Training [2619/10000] ..........2619 ) Loss= 0.57143396\n",
      "Training [2620/10000] ..........2620 ) Loss= 0.31258923\n",
      "Training [2621/10000] ..........2621 ) Loss= 0.22883336\n",
      "Training [2622/10000] ..........2622 ) Loss= 0.22097786\n",
      "Training [2623/10000] ..........2623 ) Loss= 0.32104364\n",
      "Training [2624/10000] ..........2624 ) Loss= 0.21803834\n",
      "Training [2625/10000] ..........2625 ) Loss= 0.2101296\n",
      "Training [2626/10000] ..........2626 ) Loss= 0.22069985\n",
      "Training [2627/10000] ..........2627 ) Loss= 0.4622296\n",
      "Training [2628/10000] ..........2628 ) Loss= 0.1105237\n",
      "Training [2629/10000] ..........2629 ) Loss= 0.19560589\n",
      "Training [2630/10000] ..........2630 ) Loss= 0.21903345\n",
      "Training [2631/10000] ..........2631 ) Loss= 0.33875075\n",
      "Training [2632/10000] ..........2632 ) Loss= 0.15046372\n",
      "Training [2633/10000] ..........2633 ) Loss= 0.47243375\n",
      "Training [2634/10000] ..........2634 ) Loss= 0.14820322\n",
      "Training [2635/10000] ..........2635 ) Loss= 0.2009748\n",
      "Training [2636/10000] ..........2636 ) Loss= 0.35257056\n",
      "Training [2637/10000] ..........2637 ) Loss= 0.32503328\n",
      "Training [2638/10000] ..........2638 ) Loss= 0.18890691\n",
      "Training [2639/10000] ..........2639 ) Loss= 0.13857692\n",
      "Training [2640/10000] ..........2640 ) Loss= 0.18135871\n",
      "Training [2641/10000] ..........2641 ) Loss= 0.15088148\n",
      "Training [2642/10000] ..........2642 ) Loss= 0.14385654\n",
      "Training [2643/10000] ..........2643 ) Loss= 0.16197115\n",
      "Training [2644/10000] ..........2644 ) Loss= 0.17497802\n",
      "Training [2645/10000] ..........2645 ) Loss= 0.15816483\n",
      "Training [2646/10000] ..........2646 ) Loss= 0.1897739\n",
      "Training [2647/10000] ..........2647 ) Loss= 0.2887442\n",
      "Training [2648/10000] ..........2648 ) Loss= 0.16690215\n",
      "Training [2649/10000] ..........2649 ) Loss= 0.15100595\n",
      "Training [2650/10000] ..........2650 ) Loss= 0.19991085\n",
      "Training [2651/10000] ..........2651 ) Loss= 0.16675512\n",
      "Training [2652/10000] ..........2652 ) Loss= 0.2465347\n",
      "Training [2653/10000] ..........2653 ) Loss= 0.2182068\n",
      "Training [2654/10000] ..........2654 ) Loss= 0.3917422\n",
      "Training [2655/10000] ..........2655 ) Loss= 0.2089056\n",
      "Training [2656/10000] ..........2656 ) Loss= 0.18945861\n",
      "Training [2657/10000] ..........2657 ) Loss= 0.17368902\n",
      "Training [2658/10000] ..........2658 ) Loss= 0.27073008\n",
      "Training [2659/10000] ..........2659 ) Loss= 0.37429798\n",
      "Training [2660/10000] ..........2660 ) Loss= 0.32570675\n",
      "Training [2661/10000] ..........2661 ) Loss= 0.15967034\n",
      "Training [2662/10000] ..........2662 ) Loss= 0.1784532\n",
      "Training [2663/10000] ..........2663 ) Loss= 0.12137171\n",
      "Training [2664/10000] ..........2664 ) Loss= 0.16531038\n",
      "Training [2665/10000] ..........2665 ) Loss= 0.14798196\n",
      "Training [2666/10000] ..........2666 ) Loss= 0.22105423\n",
      "Training [2667/10000] ..........2667 ) Loss= 0.1083344\n",
      "Training [2668/10000] ..........2668 ) Loss= 0.30577552\n",
      "Training [2669/10000] ..........2669 ) Loss= 0.15894891\n",
      "Training [2670/10000] ..........2670 ) Loss= 0.17330378\n",
      "Training [2671/10000] ..........2671 ) Loss= 0.15335506\n",
      "Training [2672/10000] ..........2672 ) Loss= 0.1724432\n",
      "Training [2673/10000] ..........2673 ) Loss= 0.17525133\n",
      "Training [2674/10000] ..........2674 ) Loss= 0.22921748\n",
      "Training [2675/10000] ..........2675 ) Loss= 1.2651521\n",
      "Training [2676/10000] ..........2676 ) Loss= 0.12021003\n",
      "Training [2677/10000] ..........2677 ) Loss= 0.18223879\n",
      "Training [2678/10000] ..........2678 ) Loss= 0.27698362\n",
      "Training [2679/10000] ..........2679 ) Loss= 0.14893825\n",
      "Training [2680/10000] ..........2680 ) Loss= 0.23358111\n",
      "Training [2681/10000] ..........2681 ) Loss= 0.24876449\n",
      "Training [2682/10000] ..........2682 ) Loss= 0.3924776\n",
      "Training [2683/10000] ..........2683 ) Loss= 0.16770154\n",
      "Training [2684/10000] ..........2684 ) Loss= 0.12460012\n",
      "Training [2685/10000] ..........2685 ) Loss= 0.23489648\n",
      "Training [2686/10000] ..........2686 ) Loss= 0.20827383\n",
      "Training [2687/10000] ..........2687 ) Loss= 0.14111766\n",
      "Training [2688/10000] ..........2688 ) Loss= 0.13987876\n",
      "Training [2689/10000] ..........2689 ) Loss= 0.29639694\n",
      "Training [2690/10000] ..........2690 ) Loss= 0.13815188\n",
      "Training [2691/10000] ..........2691 ) Loss= 0.16798578\n",
      "Training [2692/10000] ..........2692 ) Loss= 0.14709917\n",
      "Training [2693/10000] ..........2693 ) Loss= 0.13554424\n",
      "Training [2694/10000] ..........2694 ) Loss= 0.1223666\n",
      "Training [2695/10000] ..........2695 ) Loss= 0.15865928\n",
      "Training [2696/10000] ..........2696 ) Loss= 0.2034988\n",
      "Training [2697/10000] ..........2697 ) Loss= 0.21886504\n",
      "Training [2698/10000] ..........2698 ) Loss= 0.18213166\n",
      "Training [2699/10000] ..........2699 ) Loss= 0.2964214\n",
      "Training [2700/10000] ..........2700 ) Loss= 0.17173477\n",
      "Training [2701/10000] ..........2701 ) Loss= 0.2447616\n",
      "Training [2702/10000] ..........2702 ) Loss= 0.15920928\n",
      "Training [2703/10000] ..........2703 ) Loss= 0.17058827\n",
      "Training [2704/10000] ..........2704 ) Loss= 0.13941036\n",
      "Training [2705/10000] ..........2705 ) Loss= 0.14838389\n",
      "Training [2706/10000] ..........2706 ) Loss= 0.18996821\n",
      "Training [2707/10000] ..........2707 ) Loss= 0.19543776\n",
      "Training [2708/10000] ..........2708 ) Loss= 0.16829138\n",
      "Training [2709/10000] ..........2709 ) Loss= 0.19049259\n",
      "Training [2710/10000] ..........2710 ) Loss= 0.12244536\n",
      "Training [2711/10000] ..........2711 ) Loss= 0.12703772\n",
      "Training [2712/10000] ..........2712 ) Loss= 0.16532277\n",
      "Training [2713/10000] ..........2713 ) Loss= 0.17434062\n",
      "Training [2714/10000] ..........2714 ) Loss= 0.22744799\n",
      "Training [2715/10000] ..........2715 ) Loss= 0.1717567\n",
      "Training [2716/10000] ..........2716 ) Loss= 0.321034\n",
      "Training [2717/10000] ..........2717 ) Loss= 0.1670746\n",
      "Training [2718/10000] ..........2718 ) Loss= 0.1403788\n",
      "Training [2719/10000] ..........2719 ) Loss= 0.16667347\n",
      "Training [2720/10000] ..........2720 ) Loss= 0.12441143\n",
      "Training [2721/10000] ..........2721 ) Loss= 0.29429525\n",
      "Training [2722/10000] ..........2722 ) Loss= 0.29862562\n",
      "Training [2723/10000] ..........2723 ) Loss= 0.10428809\n",
      "Training [2724/10000] ..........2724 ) Loss= 0.2521013\n",
      "Training [2725/10000] ..........2725 ) Loss= 0.11825605\n",
      "Training [2726/10000] ..........2726 ) Loss= 0.14314823\n",
      "Training [2727/10000] ..........2727 ) Loss= 0.122455515\n",
      "Training [2728/10000] ..........2728 ) Loss= 0.21085082\n",
      "Training [2729/10000] ..........2729 ) Loss= 0.1735497\n",
      "Training [2730/10000] ..........2730 ) Loss= 0.18625596\n",
      "Training [2731/10000] ..........2731 ) Loss= 0.1631741\n",
      "Training [2732/10000] ..........2732 ) Loss= 0.25790238\n",
      "Training [2733/10000] ..........2733 ) Loss= 0.1796641\n",
      "Training [2734/10000] ..........2734 ) Loss= 0.11033818\n",
      "Training [2735/10000] ..........2735 ) Loss= 0.24027595\n",
      "Training [2736/10000] ..........2736 ) Loss= 0.363146\n",
      "Training [2737/10000] ..........2737 ) Loss= 0.21765707\n",
      "Training [2738/10000] ..........2738 ) Loss= 0.13969822\n",
      "Training [2739/10000] ..........2739 ) Loss= 0.16041331\n",
      "Training [2740/10000] ..........2740 ) Loss= 0.139648\n",
      "Training [2741/10000] ..........2741 ) Loss= 0.2856622\n",
      "Training [2742/10000] ..........2742 ) Loss= 0.20116778\n",
      "Training [2743/10000] ..........2743 ) Loss= 0.24648336\n",
      "Training [2744/10000] ..........2744 ) Loss= 0.1714162\n",
      "Training [2745/10000] ..........2745 ) Loss= 0.21383396\n",
      "Training [2746/10000] ..........2746 ) Loss= 0.17688276\n",
      "Training [2747/10000] ..........2747 ) Loss= 0.17379583\n",
      "Training [2748/10000] ..........2748 ) Loss= 0.30109045\n",
      "Training [2749/10000] ..........2749 ) Loss= 0.10706237\n",
      "Training [2750/10000] ..........2750 ) Loss= 0.16851725\n",
      "Training [2751/10000] ..........2751 ) Loss= 0.12500484\n",
      "Training [2752/10000] ..........2752 ) Loss= 0.11678054\n",
      "Training [2753/10000] ..........2753 ) Loss= 0.24970348\n",
      "Training [2754/10000] ..........2754 ) Loss= 0.1378616\n",
      "Training [2755/10000] ..........2755 ) Loss= 0.24509111\n",
      "Training [2756/10000] ..........2756 ) Loss= 0.14428543\n",
      "Training [2757/10000] ..........2757 ) Loss= 0.17949511\n",
      "Training [2758/10000] ..........2758 ) Loss= 0.36294186\n",
      "Training [2759/10000] ..........2759 ) Loss= 0.9110813\n",
      "Training [2760/10000] ..........2760 ) Loss= 0.1301296\n",
      "Training [2761/10000] ..........2761 ) Loss= 0.15965964\n",
      "Training [2762/10000] ..........2762 ) Loss= 0.21029244\n",
      "Training [2763/10000] ..........2763 ) Loss= 0.3043962\n",
      "Training [2764/10000] ..........2764 ) Loss= 0.11238459\n",
      "Training [2765/10000] ..........2765 ) Loss= 0.2886343\n",
      "Training [2766/10000] ..........2766 ) Loss= 0.23717245\n",
      "Training [2767/10000] ..........2767 ) Loss= 0.12786928\n",
      "Training [2768/10000] ..........2768 ) Loss= 0.20000918\n",
      "Training [2769/10000] ..........2769 ) Loss= 0.27323171\n",
      "Training [2770/10000] ..........2770 ) Loss= 0.34605172\n",
      "Training [2771/10000] ..........2771 ) Loss= 0.24077651\n",
      "Training [2772/10000] ..........2772 ) Loss= 0.1898706\n",
      "Training [2773/10000] ..........2773 ) Loss= 0.23564056\n",
      "Training [2774/10000] ..........2774 ) Loss= 0.16739418\n",
      "Training [2775/10000] ..........2775 ) Loss= 0.31172246\n",
      "Training [2776/10000] ..........2776 ) Loss= 0.3021178\n",
      "Training [2777/10000] ..........2777 ) Loss= 0.4333148\n",
      "Training [2778/10000] ..........2778 ) Loss= 0.1771774\n",
      "Training [2779/10000] ..........2779 ) Loss= 0.1465023\n",
      "Training [2780/10000] ..........2780 ) Loss= 0.34359446\n",
      "Training [2781/10000] ..........2781 ) Loss= 0.15488179\n",
      "Training [2782/10000] ..........2782 ) Loss= 0.55638033\n",
      "Training [2783/10000] ..........2783 ) Loss= 0.16397303\n",
      "Training [2784/10000] ..........2784 ) Loss= 0.12253582\n",
      "Training [2785/10000] ..........2785 ) Loss= 0.2033465\n",
      "Training [2786/10000] ..........2786 ) Loss= 0.14516126\n",
      "Training [2787/10000] ..........2787 ) Loss= 0.27386445\n",
      "Training [2788/10000] ..........2788 ) Loss= 0.15915045\n",
      "Training [2789/10000] ..........2789 ) Loss= 0.18324713\n",
      "Training [2790/10000] ..........2790 ) Loss= 0.17766061\n",
      "Training [2791/10000] ..........2791 ) Loss= 0.1782123\n",
      "Training [2792/10000] ..........2792 ) Loss= 0.23424253\n",
      "Training [2793/10000] ..........2793 ) Loss= 0.18375558\n",
      "Training [2794/10000] ..........2794 ) Loss= 0.15522332\n",
      "Training [2795/10000] ..........2795 ) Loss= 0.115019955\n",
      "Training [2796/10000] ..........2796 ) Loss= 0.1968394\n",
      "Training [2797/10000] ..........2797 ) Loss= 0.21233007\n",
      "Training [2798/10000] ..........2798 ) Loss= 0.13382381\n",
      "Training [2799/10000] ..........2799 ) Loss= 0.23347858\n",
      "Training [2800/10000] ..........2800 ) Loss= 0.24329746\n",
      "Training [2801/10000] ..........2801 ) Loss= 0.14500265\n",
      "Training [2802/10000] ..........2802 ) Loss= 0.11003437\n",
      "Training [2803/10000] ..........2803 ) Loss= 0.124387644\n",
      "Training [2804/10000] ..........2804 ) Loss= 0.107558824\n",
      "Training [2805/10000] ..........2805 ) Loss= 0.119885884\n",
      "Training [2806/10000] ..........2806 ) Loss= 0.19927908\n",
      "Training [2807/10000] ..........2807 ) Loss= 0.30824658\n",
      "Training [2808/10000] ..........2808 ) Loss= 0.19840352\n",
      "Training [2809/10000] ..........2809 ) Loss= 0.20184517\n",
      "Training [2810/10000] ..........2810 ) Loss= 0.13980846\n",
      "Training [2811/10000] ..........2811 ) Loss= 0.18228692\n",
      "Training [2812/10000] ..........2812 ) Loss= 0.13105093\n",
      "Training [2813/10000] ..........2813 ) Loss= 0.22200103\n",
      "Training [2814/10000] ..........2814 ) Loss= 0.12991716\n",
      "Training [2815/10000] ..........2815 ) Loss= 0.26277456\n",
      "Training [2816/10000] ..........2816 ) Loss= 0.1282604\n",
      "Training [2817/10000] ..........2817 ) Loss= 0.21765873\n",
      "Training [2818/10000] ..........2818 ) Loss= 0.1303798\n",
      "Training [2819/10000] ..........2819 ) Loss= 0.10384937\n",
      "Training [2820/10000] ..........2820 ) Loss= 0.27250287\n",
      "Training [2821/10000] ..........2821 ) Loss= 0.36698514\n",
      "Training [2822/10000] ..........2822 ) Loss= 0.18077071\n",
      "Training [2823/10000] ..........2823 ) Loss= 0.1406887\n",
      "Training [2824/10000] ..........2824 ) Loss= 0.21891804\n",
      "Training [2825/10000] ..........2825 ) Loss= 0.38421524\n",
      "Training [2826/10000] ..........2826 ) Loss= 0.23595186\n",
      "Training [2827/10000] ..........2827 ) Loss= 0.17699403\n",
      "Training [2828/10000] ..........2828 ) Loss= 0.24267577\n",
      "Training [2829/10000] ..........2829 ) Loss= 0.14180087\n",
      "Training [2830/10000] ..........2830 ) Loss= 0.100081846\n",
      "Training [2831/10000] ..........2831 ) Loss= 0.14411074\n",
      "Training [2832/10000] ..........2832 ) Loss= 0.14040588\n",
      "Training [2833/10000] ..........2833 ) Loss= 0.10502737\n",
      "Training [2834/10000] ..........2834 ) Loss= 0.13965617\n",
      "Training [2835/10000] ..........2835 ) Loss= 0.16136503\n",
      "Training [2836/10000] ..........2836 ) Loss= 0.13020772\n",
      "Training [2837/10000] ..........2837 ) Loss= 0.13177279\n",
      "Training [2838/10000] ..........2838 ) Loss= 0.22839783\n",
      "Training [2839/10000] ..........2839 ) Loss= 0.291222\n",
      "Training [2840/10000] ..........2840 ) Loss= 0.10075231\n",
      "Training [2841/10000] ..........2841 ) Loss= 0.46490327\n",
      "Training [2842/10000] ..........2842 ) Loss= 0.24108322\n",
      "Training [2843/10000] ..........2843 ) Loss= 0.14862286\n",
      "Training [2844/10000] ..........2844 ) Loss= 0.20878492\n",
      "Training [2845/10000] ..........2845 ) Loss= 0.15388227\n",
      "Training [2846/10000] ..........2846 ) Loss= 0.2522502\n",
      "Training [2847/10000] ..........2847 ) Loss= 0.1595088\n",
      "Training [2848/10000] ..........2848 ) Loss= 0.19579755\n",
      "Training [2849/10000] ..........2849 ) Loss= 0.12013631\n",
      "Training [2850/10000] ..........2850 ) Loss= 0.17391604\n",
      "Training [2851/10000] ..........2851 ) Loss= 0.22019039\n",
      "Training [2852/10000] ..........2852 ) Loss= 0.20785604\n",
      "Training [2853/10000] ..........2853 ) Loss= 0.13815507\n",
      "Training [2854/10000] ..........2854 ) Loss= 0.14565253\n",
      "Training [2855/10000] ..........2855 ) Loss= 0.19001485\n",
      "Training [2856/10000] ..........2856 ) Loss= 0.24460422\n",
      "Training [2857/10000] ..........2857 ) Loss= 0.12530784\n",
      "Training [2858/10000] ..........2858 ) Loss= 0.32335293\n",
      "Training [2859/10000] ..........2859 ) Loss= 0.19392844\n",
      "Training [2860/10000] ..........2860 ) Loss= 0.22364649\n",
      "Training [2861/10000] ..........2861 ) Loss= 0.14828184\n",
      "Training [2862/10000] ..........2862 ) Loss= 0.23058376\n",
      "Training [2863/10000] ..........2863 ) Loss= 0.16472907\n",
      "Training [2864/10000] ..........2864 ) Loss= 0.116805136\n",
      "Training [2865/10000] ..........2865 ) Loss= 0.19275333\n",
      "Training [2866/10000] ..........2866 ) Loss= 0.20972537\n",
      "Training [2867/10000] ..........2867 ) Loss= 0.31115574\n",
      "Training [2868/10000] ..........2868 ) Loss= 0.1371648\n",
      "Training [2869/10000] ..........2869 ) Loss= 0.20488714\n",
      "Training [2870/10000] ..........2870 ) Loss= 0.15215407\n",
      "Training [2871/10000] ..........2871 ) Loss= 0.12575231\n",
      "Training [2872/10000] ..........2872 ) Loss= 0.15417828\n",
      "Training [2873/10000] ..........2873 ) Loss= 0.27675644\n",
      "Training [2874/10000] ..........2874 ) Loss= 0.21998124\n",
      "Training [2875/10000] ..........2875 ) Loss= 0.27697802\n",
      "Training [2876/10000] ..........2876 ) Loss= 0.1961321\n",
      "Training [2877/10000] ..........2877 ) Loss= 0.22496626\n",
      "Training [2878/10000] ..........2878 ) Loss= 0.12853049\n",
      "Training [2879/10000] ..........2879 ) Loss= 0.2591279\n",
      "Training [2880/10000] ..........2880 ) Loss= 0.24535412\n",
      "Training [2881/10000] ..........2881 ) Loss= 0.2716173\n",
      "Training [2882/10000] ..........2882 ) Loss= 0.33337307\n",
      "Training [2883/10000] ..........2883 ) Loss= 0.12734197\n",
      "Training [2884/10000] ..........2884 ) Loss= 0.15737152\n",
      "Training [2885/10000] ..........2885 ) Loss= 0.19224215\n",
      "Training [2886/10000] ..........2886 ) Loss= 0.14380994\n",
      "Training [2887/10000] ..........2887 ) Loss= 0.27927542\n",
      "Training [2888/10000] ..........2888 ) Loss= 0.16154139\n",
      "Training [2889/10000] ..........2889 ) Loss= 0.12473119\n",
      "Training [2890/10000] ..........2890 ) Loss= 0.14782465\n",
      "Training [2891/10000] ..........2891 ) Loss= 0.16802816\n",
      "Training [2892/10000] ..........2892 ) Loss= 0.29595673\n",
      "Training [2893/10000] ..........2893 ) Loss= 0.16053092\n",
      "Training [2894/10000] ..........2894 ) Loss= 0.25268045\n",
      "Training [2895/10000] ..........2895 ) Loss= 0.13845345\n",
      "Training [2896/10000] ..........2896 ) Loss= 0.16072577\n",
      "Training [2897/10000] ..........2897 ) Loss= 0.1349329\n",
      "Training [2898/10000] ..........2898 ) Loss= 0.3137129\n",
      "Training [2899/10000] ..........2899 ) Loss= 0.20772341\n",
      "Training [2900/10000] ..........2900 ) Loss= 0.12569761\n",
      "Training [2901/10000] ..........2901 ) Loss= 0.23385005\n",
      "Training [2902/10000] ..........2902 ) Loss= 0.10764291\n",
      "Training [2903/10000] ..........2903 ) Loss= 0.20511618\n",
      "Training [2904/10000] ..........2904 ) Loss= 0.14851911\n",
      "Training [2905/10000] ..........2905 ) Loss= 0.1497831\n",
      "Training [2906/10000] ..........2906 ) Loss= 0.25930068\n",
      "Training [2907/10000] ..........2907 ) Loss= 0.24125983\n",
      "Training [2908/10000] ..........2908 ) Loss= 0.1574649\n",
      "Training [2909/10000] ..........2909 ) Loss= 0.21427622\n",
      "Training [2910/10000] ..........2910 ) Loss= 0.13865575\n",
      "Training [2911/10000] ..........2911 ) Loss= 0.1613933\n",
      "Training [2912/10000] ..........2912 ) Loss= 0.17805506\n",
      "Training [2913/10000] ..........2913 ) Loss= 0.13402283\n",
      "Training [2914/10000] ..........2914 ) Loss= 0.098995924\n",
      "Training [2915/10000] ..........2915 ) Loss= 0.19379494\n",
      "Training [2916/10000] ..........2916 ) Loss= 0.20115791\n",
      "Training [2917/10000] ..........2917 ) Loss= 0.18944913\n",
      "Training [2918/10000] ..........2918 ) Loss= 0.12982959\n",
      "Training [2919/10000] ..........2919 ) Loss= 0.21504454\n",
      "Training [2920/10000] ..........2920 ) Loss= 0.18488725\n",
      "Training [2921/10000] ..........2921 ) Loss= 0.15174732\n",
      "Training [2922/10000] ..........2922 ) Loss= 0.22666228\n",
      "Training [2923/10000] ..........2923 ) Loss= 0.13564977\n",
      "Training [2924/10000] ..........2924 ) Loss= 0.13674821\n",
      "Training [2925/10000] ..........2925 ) Loss= 0.15545304\n",
      "Training [2926/10000] ..........2926 ) Loss= 0.14396729\n",
      "Training [2927/10000] ..........2927 ) Loss= 0.19822866\n",
      "Training [2928/10000] ..........2928 ) Loss= 0.3533534\n",
      "Training [2929/10000] ..........2929 ) Loss= 0.1541787\n",
      "Training [2930/10000] ..........2930 ) Loss= 0.3412078\n",
      "Training [2931/10000] ..........2931 ) Loss= 0.17184088\n",
      "Training [2932/10000] ..........2932 ) Loss= 0.20983703\n",
      "Training [2933/10000] ..........2933 ) Loss= 0.11306093\n",
      "Training [2934/10000] ..........2934 ) Loss= 0.18083936\n",
      "Training [2935/10000] ..........2935 ) Loss= 0.14453273\n",
      "Training [2936/10000] ..........2936 ) Loss= 0.2393729\n",
      "Training [2937/10000] ..........2937 ) Loss= 0.14377601\n",
      "Training [2938/10000] ..........2938 ) Loss= 0.3601226\n",
      "Training [2939/10000] ..........2939 ) Loss= 0.1362007\n",
      "Training [2940/10000] ..........2940 ) Loss= 0.2627873\n",
      "Training [2941/10000] ..........2941 ) Loss= 0.23066528\n",
      "Training [2942/10000] ..........2942 ) Loss= 0.1380138\n",
      "Training [2943/10000] ..........2943 ) Loss= 0.15703894\n",
      "Training [2944/10000] ..........2944 ) Loss= 0.107756\n",
      "Training [2945/10000] ..........2945 ) Loss= 0.4818898\n",
      "Training [2946/10000] ..........2946 ) Loss= 0.22176018\n",
      "Training [2947/10000] ..........2947 ) Loss= 0.16924766\n",
      "Training [2948/10000] ..........2948 ) Loss= 0.32496107\n",
      "Training [2949/10000] ..........2949 ) Loss= 0.14159726\n",
      "Training [2950/10000] ..........2950 ) Loss= 0.19757973\n",
      "Training [2951/10000] ..........2951 ) Loss= 0.2129129\n",
      "Training [2952/10000] ..........2952 ) Loss= 0.1494226\n",
      "Training [2953/10000] ..........2953 ) Loss= 0.2628201\n",
      "Training [2954/10000] ..........2954 ) Loss= 0.13548364\n",
      "Training [2955/10000] ..........2955 ) Loss= 0.1331777\n",
      "Training [2956/10000] ..........2956 ) Loss= 0.13373062\n",
      "Training [2957/10000] ..........2957 ) Loss= 0.15814012\n",
      "Training [2958/10000] ..........2958 ) Loss= 0.17048994\n",
      "Training [2959/10000] ..........2959 ) Loss= 0.16058862\n",
      "Training [2960/10000] ..........2960 ) Loss= 0.10788943\n",
      "Training [2961/10000] ..........2961 ) Loss= 0.11458437\n",
      "Training [2962/10000] ..........2962 ) Loss= 0.09413264\n",
      "Training [2963/10000] ..........2963 ) Loss= 0.09090601\n",
      "Training [2964/10000] ..........2964 ) Loss= 0.20692529\n",
      "Training [2965/10000] ..........2965 ) Loss= 0.15565233\n",
      "Training [2966/10000] ..........2966 ) Loss= 0.13263686\n",
      "Training [2967/10000] ..........2967 ) Loss= 0.14198364\n",
      "Training [2968/10000] ..........2968 ) Loss= 0.34565157\n",
      "Training [2969/10000] ..........2969 ) Loss= 0.22864448\n",
      "Training [2970/10000] ..........2970 ) Loss= 0.14753808\n",
      "Training [2971/10000] ..........2971 ) Loss= 0.1890122\n",
      "Training [2972/10000] ..........2972 ) Loss= 0.26367018\n",
      "Training [2973/10000] ..........2973 ) Loss= 0.18872342\n",
      "Training [2974/10000] ..........2974 ) Loss= 0.16122264\n",
      "Training [2975/10000] ..........2975 ) Loss= 0.24381277\n",
      "Training [2976/10000] ..........2976 ) Loss= 0.103914686\n",
      "Training [2977/10000] ..........2977 ) Loss= 0.21458565\n",
      "Training [2978/10000] ..........2978 ) Loss= 0.22297798\n",
      "Training [2979/10000] ..........2979 ) Loss= 0.14786583\n",
      "Training [2980/10000] ..........2980 ) Loss= 0.14088474\n",
      "Training [2981/10000] ..........2981 ) Loss= 0.18072012\n",
      "Training [2982/10000] ..........2982 ) Loss= 0.16494294\n",
      "Training [2983/10000] ..........2983 ) Loss= 0.23357162\n",
      "Training [2984/10000] ..........2984 ) Loss= 0.22615336\n",
      "Training [2985/10000] ..........2985 ) Loss= 0.20791352\n",
      "Training [2986/10000] ..........2986 ) Loss= 0.4628986\n",
      "Training [2987/10000] ..........2987 ) Loss= 0.12318151\n",
      "Training [2988/10000] ..........2988 ) Loss= 0.15491769\n",
      "Training [2989/10000] ..........2989 ) Loss= 0.33309454\n",
      "Training [2990/10000] ..........2990 ) Loss= 0.24990469\n",
      "Training [2991/10000] ..........2991 ) Loss= 0.22098297\n",
      "Training [2992/10000] ..........2992 ) Loss= 0.13868551\n",
      "Training [2993/10000] ..........2993 ) Loss= 0.20186442\n",
      "Training [2994/10000] ..........2994 ) Loss= 0.12605084\n",
      "Training [2995/10000] ..........2995 ) Loss= 0.14987378\n",
      "Training [2996/10000] ..........2996 ) Loss= 0.23588638\n",
      "Training [2997/10000] ..........2997 ) Loss= 0.143009\n",
      "Training [2998/10000] ..........2998 ) Loss= 0.106227204\n",
      "Training [2999/10000] ..........2999 ) Loss= 0.1218685\n",
      "Training [3000/10000] ..........3000 ) Loss= 0.13035403 - Saving Model3000.torch\n",
      "Training [3001/10000] ..........3001 ) Loss= 0.10755701\n",
      "Training [3002/10000] ..........3002 ) Loss= 0.17805189\n",
      "Training [3003/10000] ..........3003 ) Loss= 0.15844697\n",
      "Training [3004/10000] ..........3004 ) Loss= 0.14793307\n",
      "Training [3005/10000] ..........3005 ) Loss= 0.20507695\n",
      "Training [3006/10000] ..........3006 ) Loss= 0.40989733\n",
      "Training [3007/10000] ..........3007 ) Loss= 0.16109906\n",
      "Training [3008/10000] ..........3008 ) Loss= 0.19123515\n",
      "Training [3009/10000] ..........3009 ) Loss= 0.14035463\n",
      "Training [3010/10000] ..........3010 ) Loss= 0.24289319\n",
      "Training [3011/10000] ..........3011 ) Loss= 0.12995994\n",
      "Training [3012/10000] ..........3012 ) Loss= 0.23004475\n",
      "Training [3013/10000] ..........3013 ) Loss= 0.1329709\n",
      "Training [3014/10000] ..........3014 ) Loss= 0.16065668\n",
      "Training [3015/10000] ..........3015 ) Loss= 0.16863199\n",
      "Training [3016/10000] ..........3016 ) Loss= 0.13276845\n",
      "Training [3017/10000] ..........3017 ) Loss= 0.16560109\n",
      "Training [3018/10000] ..........3018 ) Loss= 0.1465497\n",
      "Training [3019/10000] ..........3019 ) Loss= 0.2974392\n",
      "Training [3020/10000] ..........3020 ) Loss= 0.21505123\n",
      "Training [3021/10000] ..........3021 ) Loss= 0.13660192\n",
      "Training [3022/10000] ..........3022 ) Loss= 0.14567457\n",
      "Training [3023/10000] ..........3023 ) Loss= 0.15007578\n",
      "Training [3024/10000] ..........3024 ) Loss= 0.2726941\n",
      "Training [3025/10000] ..........3025 ) Loss= 0.41829064\n",
      "Training [3026/10000] ..........3026 ) Loss= 0.5546334\n",
      "Training [3027/10000] ..........3027 ) Loss= 0.4173795\n",
      "Training [3028/10000] ..........3028 ) Loss= 0.13959588\n",
      "Training [3029/10000] ..........3029 ) Loss= 0.105067976\n",
      "Training [3030/10000] ..........3030 ) Loss= 0.15510625\n",
      "Training [3031/10000] ..........3031 ) Loss= 0.14911927\n",
      "Training [3032/10000] ..........3032 ) Loss= 0.16696179\n",
      "Training [3033/10000] ..........3033 ) Loss= 0.13965903\n",
      "Training [3034/10000] ..........3034 ) Loss= 0.14717682\n",
      "Training [3035/10000] ..........3035 ) Loss= 0.15048864\n",
      "Training [3036/10000] ..........3036 ) Loss= 0.13899417\n",
      "Training [3037/10000] ..........3037 ) Loss= 0.14617832\n",
      "Training [3038/10000] ..........3038 ) Loss= 0.12017043\n",
      "Training [3039/10000] ..........3039 ) Loss= 0.2038267\n",
      "Training [3040/10000] ..........3040 ) Loss= 0.19774291\n",
      "Training [3041/10000] ..........3041 ) Loss= 0.23730038\n",
      "Training [3042/10000] ..........3042 ) Loss= 0.30432618\n",
      "Training [3043/10000] ..........3043 ) Loss= 0.17525232\n",
      "Training [3044/10000] ..........3044 ) Loss= 0.14842127\n",
      "Training [3045/10000] ..........3045 ) Loss= 0.280434\n",
      "Training [3046/10000] ..........3046 ) Loss= 0.143871\n",
      "Training [3047/10000] ..........3047 ) Loss= 0.12561043\n",
      "Training [3048/10000] ..........3048 ) Loss= 0.2581888\n",
      "Training [3049/10000] ..........3049 ) Loss= 0.18774347\n",
      "Training [3050/10000] ..........3050 ) Loss= 0.16820672\n",
      "Training [3051/10000] ..........3051 ) Loss= 0.15685306\n",
      "Training [3052/10000] ..........3052 ) Loss= 0.21496594\n",
      "Training [3053/10000] ..........3053 ) Loss= 0.18912789\n",
      "Training [3054/10000] ..........3054 ) Loss= 0.13764115\n",
      "Training [3055/10000] ..........3055 ) Loss= 0.22086623\n",
      "Training [3056/10000] ..........3056 ) Loss= 0.2209597\n",
      "Training [3057/10000] ..........3057 ) Loss= 0.7776113\n",
      "Training [3058/10000] ..........3058 ) Loss= 0.22860624\n",
      "Training [3059/10000] ..........3059 ) Loss= 0.33148867\n",
      "Training [3060/10000] ..........3060 ) Loss= 0.1622192\n",
      "Training [3061/10000] ..........3061 ) Loss= 0.11367994\n",
      "Training [3062/10000] ..........3062 ) Loss= 0.12908119\n",
      "Training [3063/10000] ..........3063 ) Loss= 0.112721875\n",
      "Training [3064/10000] ..........3064 ) Loss= 0.3110517\n",
      "Training [3065/10000] ..........3065 ) Loss= 0.16282399\n",
      "Training [3066/10000] ..........3066 ) Loss= 0.27994448\n",
      "Training [3067/10000] ..........3067 ) Loss= 0.13417563\n",
      "Training [3068/10000] ..........3068 ) Loss= 0.215896\n",
      "Training [3069/10000] ..........3069 ) Loss= 0.15060231\n",
      "Training [3070/10000] ..........3070 ) Loss= 0.18422943\n",
      "Training [3071/10000] ..........3071 ) Loss= 0.20496255\n",
      "Training [3072/10000] ..........3072 ) Loss= 0.17682977\n",
      "Training [3073/10000] ..........3073 ) Loss= 0.15062936\n",
      "Training [3074/10000] ..........3074 ) Loss= 0.2886463\n",
      "Training [3075/10000] ..........3075 ) Loss= 0.18083845\n",
      "Training [3076/10000] ..........3076 ) Loss= 0.14120348\n",
      "Training [3077/10000] ..........3077 ) Loss= 0.07832999\n",
      "Training [3078/10000] ..........3078 ) Loss= 0.15105793\n",
      "Training [3079/10000] ..........3079 ) Loss= 0.15881072\n",
      "Training [3080/10000] ..........3080 ) Loss= 0.11655677\n",
      "Training [3081/10000] ..........3081 ) Loss= 0.11374925\n",
      "Training [3082/10000] ..........3082 ) Loss= 0.12204961\n",
      "Training [3083/10000] ..........3083 ) Loss= 0.10395205\n",
      "Training [3084/10000] ..........3084 ) Loss= 0.13770801\n",
      "Training [3085/10000] ..........3085 ) Loss= 0.15417552\n",
      "Training [3086/10000] ..........3086 ) Loss= 0.19553047\n",
      "Training [3087/10000] ..........3087 ) Loss= 0.13133658\n",
      "Training [3088/10000] ..........3088 ) Loss= 0.10671539\n",
      "Training [3089/10000] ..........3089 ) Loss= 0.2052668\n",
      "Training [3090/10000] ..........3090 ) Loss= 0.14683278\n",
      "Training [3091/10000] ..........3091 ) Loss= 0.20911278\n",
      "Training [3092/10000] ..........3092 ) Loss= 0.14266978\n",
      "Training [3093/10000] ..........3093 ) Loss= 0.1317735\n",
      "Training [3094/10000] ..........3094 ) Loss= 0.34045938\n",
      "Training [3095/10000] ..........3095 ) Loss= 0.17062561\n",
      "Training [3096/10000] ..........3096 ) Loss= 0.20130418\n",
      "Training [3097/10000] ..........3097 ) Loss= 0.11523207\n",
      "Training [3098/10000] ..........3098 ) Loss= 0.21204112\n",
      "Training [3099/10000] ..........3099 ) Loss= 0.1494184\n",
      "Training [3100/10000] ..........3100 ) Loss= 0.13634215\n",
      "Training [3101/10000] ..........3101 ) Loss= 0.19860056\n",
      "Training [3102/10000] ..........3102 ) Loss= 0.12718073\n",
      "Training [3103/10000] ..........3103 ) Loss= 0.19612354\n",
      "Training [3104/10000] ..........3104 ) Loss= 0.13032486\n",
      "Training [3105/10000] ..........3105 ) Loss= 0.12718497\n",
      "Training [3106/10000] ..........3106 ) Loss= 0.30089778\n",
      "Training [3107/10000] ..........3107 ) Loss= 0.165665\n",
      "Training [3108/10000] ..........3108 ) Loss= 0.09339936\n",
      "Training [3109/10000] ..........3109 ) Loss= 0.21094769\n",
      "Training [3110/10000] ..........3110 ) Loss= 0.2110043\n",
      "Training [3111/10000] ..........3111 ) Loss= 0.12792657\n",
      "Training [3112/10000] ..........3112 ) Loss= 0.15650873\n",
      "Training [3113/10000] ..........3113 ) Loss= 0.29468128\n",
      "Training [3114/10000] ..........3114 ) Loss= 0.16451915\n",
      "Training [3115/10000] ..........3115 ) Loss= 0.47239524\n",
      "Training [3116/10000] ..........3116 ) Loss= 0.19870755\n",
      "Training [3117/10000] ..........3117 ) Loss= 0.1947821\n",
      "Training [3118/10000] ..........3118 ) Loss= 0.12716596\n",
      "Training [3119/10000] ..........3119 ) Loss= 0.21346772\n",
      "Training [3120/10000] ..........3120 ) Loss= 0.21836865\n",
      "Training [3121/10000] ..........3121 ) Loss= 0.13612601\n",
      "Training [3122/10000] ..........3122 ) Loss= 0.28746712\n",
      "Training [3123/10000] ..........3123 ) Loss= 0.22140348\n",
      "Training [3124/10000] ..........3124 ) Loss= 0.26719433\n",
      "Training [3125/10000] ..........3125 ) Loss= 0.16062808\n",
      "Training [3126/10000] ..........3126 ) Loss= 0.11261455\n",
      "Training [3127/10000] ..........3127 ) Loss= 0.1828042\n",
      "Training [3128/10000] ..........3128 ) Loss= 0.21402328\n",
      "Training [3129/10000] ..........3129 ) Loss= 0.2195425\n",
      "Training [3130/10000] ..........3130 ) Loss= 0.15519197\n",
      "Training [3131/10000] ..........3131 ) Loss= 0.10327704\n",
      "Training [3132/10000] ..........3132 ) Loss= 0.21644987\n",
      "Training [3133/10000] ..........3133 ) Loss= 0.15425138\n",
      "Training [3134/10000] ..........3134 ) Loss= 0.15010501\n",
      "Training [3135/10000] ..........3135 ) Loss= 0.31272218\n",
      "Training [3136/10000] ..........3136 ) Loss= 0.111099936\n",
      "Training [3137/10000] ..........3137 ) Loss= 0.38509947\n",
      "Training [3138/10000] ..........3138 ) Loss= 0.22866225\n",
      "Training [3139/10000] ..........3139 ) Loss= 0.20318627\n",
      "Training [3140/10000] ..........3140 ) Loss= 0.23441693\n",
      "Training [3141/10000] ..........3141 ) Loss= 0.15894105\n",
      "Training [3142/10000] ..........3142 ) Loss= 0.117374666\n",
      "Training [3143/10000] ..........3143 ) Loss= 0.12924734\n",
      "Training [3144/10000] ..........3144 ) Loss= 0.29068565\n",
      "Training [3145/10000] ..........3145 ) Loss= 0.1240628\n",
      "Training [3146/10000] ..........3146 ) Loss= 0.21876833\n",
      "Training [3147/10000] ..........3147 ) Loss= 0.34902313\n",
      "Training [3148/10000] ..........3148 ) Loss= 0.18964747\n",
      "Training [3149/10000] ..........3149 ) Loss= 0.12403925\n",
      "Training [3150/10000] ..........3150 ) Loss= 0.21837579\n",
      "Training [3151/10000] ..........3151 ) Loss= 0.1860412\n",
      "Training [3152/10000] ..........3152 ) Loss= 0.14149047\n",
      "Training [3153/10000] ..........3153 ) Loss= 0.16832903\n",
      "Training [3154/10000] ..........3154 ) Loss= 0.17501175\n",
      "Training [3155/10000] ..........3155 ) Loss= 0.18525138\n",
      "Training [3156/10000] ..........3156 ) Loss= 0.100968115\n",
      "Training [3157/10000] ..........3157 ) Loss= 0.17429261\n",
      "Training [3158/10000] ..........3158 ) Loss= 0.15488158\n",
      "Training [3159/10000] ..........3159 ) Loss= 0.12296234\n",
      "Training [3160/10000] ..........3160 ) Loss= 0.112192646\n",
      "Training [3161/10000] ..........3161 ) Loss= 0.09965512\n",
      "Training [3162/10000] ..........3162 ) Loss= 0.12941897\n",
      "Training [3163/10000] ..........3163 ) Loss= 0.20252635\n",
      "Training [3164/10000] ..........3164 ) Loss= 0.1595536\n",
      "Training [3165/10000] ..........3165 ) Loss= 0.20469876\n",
      "Training [3166/10000] ..........3166 ) Loss= 0.19727132\n",
      "Training [3167/10000] ..........3167 ) Loss= 0.21996641\n",
      "Training [3168/10000] ..........3168 ) Loss= 0.12395173\n",
      "Training [3169/10000] ..........3169 ) Loss= 0.15805471\n",
      "Training [3170/10000] ..........3170 ) Loss= 0.3028417\n",
      "Training [3171/10000] ..........3171 ) Loss= 0.1573841\n",
      "Training [3172/10000] ..........3172 ) Loss= 0.20589282\n",
      "Training [3173/10000] ..........3173 ) Loss= 0.13942441\n",
      "Training [3174/10000] ..........3174 ) Loss= 0.15398917\n",
      "Training [3175/10000] ..........3175 ) Loss= 0.23276702\n",
      "Training [3176/10000] ..........3176 ) Loss= 0.1866185\n",
      "Training [3177/10000] ..........3177 ) Loss= 0.18862946\n",
      "Training [3178/10000] ..........3178 ) Loss= 0.15913047\n",
      "Training [3179/10000] ..........3179 ) Loss= 0.12294126\n",
      "Training [3180/10000] ..........3180 ) Loss= 0.1708824\n",
      "Training [3181/10000] ..........3181 ) Loss= 0.71483696\n",
      "Training [3182/10000] ..........3182 ) Loss= 0.1298018\n",
      "Training [3183/10000] ..........3183 ) Loss= 0.24227706\n",
      "Training [3184/10000] ..........3184 ) Loss= 0.12666114\n",
      "Training [3185/10000] ..........3185 ) Loss= 0.1129594\n",
      "Training [3186/10000] ..........3186 ) Loss= 0.114506304\n",
      "Training [3187/10000] ..........3187 ) Loss= 0.13002576\n",
      "Training [3188/10000] ..........3188 ) Loss= 0.2769034\n",
      "Training [3189/10000] ..........3189 ) Loss= 0.18984398\n",
      "Training [3190/10000] ..........3190 ) Loss= 0.15316145\n",
      "Training [3191/10000] ..........3191 ) Loss= 0.20119794\n",
      "Training [3192/10000] ..........3192 ) Loss= 0.32610658\n",
      "Training [3193/10000] ..........3193 ) Loss= 0.27987865\n",
      "Training [3194/10000] ..........3194 ) Loss= 0.14548898\n",
      "Training [3195/10000] ..........3195 ) Loss= 0.13255343\n",
      "Training [3196/10000] ..........3196 ) Loss= 0.19874871\n",
      "Training [3197/10000] ..........3197 ) Loss= 0.15803364\n",
      "Training [3198/10000] ..........3198 ) Loss= 0.12477211\n",
      "Training [3199/10000] ..........3199 ) Loss= 0.21559997\n",
      "Training [3200/10000] ..........3200 ) Loss= 0.17109346\n",
      "Training [3201/10000] ..........3201 ) Loss= 0.17634754\n",
      "Training [3202/10000] ..........3202 ) Loss= 0.19849643\n",
      "Training [3203/10000] ..........3203 ) Loss= 0.18036027\n",
      "Training [3204/10000] ..........3204 ) Loss= 0.111872464\n",
      "Training [3205/10000] ..........3205 ) Loss= 0.1288724\n",
      "Training [3206/10000] ..........3206 ) Loss= 0.21875967\n",
      "Training [3207/10000] ..........3207 ) Loss= 0.22449139\n",
      "Training [3208/10000] ..........3208 ) Loss= 0.16782793\n",
      "Training [3209/10000] ..........3209 ) Loss= 0.13874857\n",
      "Training [3210/10000] ..........3210 ) Loss= 0.114965826\n",
      "Training [3211/10000] ..........3211 ) Loss= 0.1347496\n",
      "Training [3212/10000] ..........3212 ) Loss= 0.17794439\n",
      "Training [3213/10000] ..........3213 ) Loss= 0.16138873\n",
      "Training [3214/10000] ..........3214 ) Loss= 0.12180596\n",
      "Training [3215/10000] ..........3215 ) Loss= 0.1656731\n",
      "Training [3216/10000] ..........3216 ) Loss= 0.14939137\n",
      "Training [3217/10000] ..........3217 ) Loss= 0.15095675\n",
      "Training [3218/10000] ..........3218 ) Loss= 0.13763094\n",
      "Training [3219/10000] ..........3219 ) Loss= 0.13140926\n",
      "Training [3220/10000] ..........3220 ) Loss= 0.27170065\n",
      "Training [3221/10000] ..........3221 ) Loss= 0.55569345\n",
      "Training [3222/10000] ..........3222 ) Loss= 0.19308889\n",
      "Training [3223/10000] ..........3223 ) Loss= 0.39906856\n",
      "Training [3224/10000] ..........3224 ) Loss= 0.2199533\n",
      "Training [3225/10000] ..........3225 ) Loss= 0.11469849\n",
      "Training [3226/10000] ..........3226 ) Loss= 0.28963673\n",
      "Training [3227/10000] ..........3227 ) Loss= 0.23726462\n",
      "Training [3228/10000] ..........3228 ) Loss= 0.13774228\n",
      "Training [3229/10000] ..........3229 ) Loss= 0.19741222\n",
      "Training [3230/10000] ..........3230 ) Loss= 0.09812848\n",
      "Training [3231/10000] ..........3231 ) Loss= 0.20383778\n",
      "Training [3232/10000] ..........3232 ) Loss= 0.18034369\n",
      "Training [3233/10000] ..........3233 ) Loss= 0.09574743\n",
      "Training [3234/10000] ..........3234 ) Loss= 0.16851848\n",
      "Training [3235/10000] ..........3235 ) Loss= 0.2130539\n",
      "Training [3236/10000] ..........3236 ) Loss= 0.112738036\n",
      "Training [3237/10000] ..........3237 ) Loss= 0.20408903\n",
      "Training [3238/10000] ..........3238 ) Loss= 0.12007988\n",
      "Training [3239/10000] ..........3239 ) Loss= 0.21028541\n",
      "Training [3240/10000] ..........3240 ) Loss= 0.10922495\n",
      "Training [3241/10000] ..........3241 ) Loss= 0.13525605\n",
      "Training [3242/10000] ..........3242 ) Loss= 0.16282038\n",
      "Training [3243/10000] ..........3243 ) Loss= 0.29640058\n",
      "Training [3244/10000] ..........3244 ) Loss= 0.13352889\n",
      "Training [3245/10000] ..........3245 ) Loss= 0.18614541\n",
      "Training [3246/10000] ..........3246 ) Loss= 0.19880405\n",
      "Training [3247/10000] ..........3247 ) Loss= 0.09899625\n",
      "Training [3248/10000] ..........3248 ) Loss= 0.29497465\n",
      "Training [3249/10000] ..........3249 ) Loss= 0.20128357\n",
      "Training [3250/10000] ..........3250 ) Loss= 0.13445461\n",
      "Training [3251/10000] ..........3251 ) Loss= 0.16105701\n",
      "Training [3252/10000] ..........3252 ) Loss= 0.10399004\n",
      "Training [3253/10000] ..........3253 ) Loss= 0.18179347\n",
      "Training [3254/10000] ..........3254 ) Loss= 0.12477445\n",
      "Training [3255/10000] ..........3255 ) Loss= 0.2714923\n",
      "Training [3256/10000] ..........3256 ) Loss= 0.17472579\n",
      "Training [3257/10000] ..........3257 ) Loss= 0.16441402\n",
      "Training [3258/10000] ..........3258 ) Loss= 0.18132402\n",
      "Training [3259/10000] ..........3259 ) Loss= 0.13006435\n",
      "Training [3260/10000] ..........3260 ) Loss= 0.12253763\n",
      "Training [3261/10000] ..........3261 ) Loss= 0.14188069\n",
      "Training [3262/10000] ..........3262 ) Loss= 0.19693604\n",
      "Training [3263/10000] ..........3263 ) Loss= 0.1687977\n",
      "Training [3264/10000] ..........3264 ) Loss= 0.39491102\n",
      "Training [3265/10000] ..........3265 ) Loss= 0.16527618\n",
      "Training [3266/10000] ..........3266 ) Loss= 0.22491448\n",
      "Training [3267/10000] ..........3267 ) Loss= 0.2542368\n",
      "Training [3268/10000] ..........3268 ) Loss= 0.0767762\n",
      "Training [3269/10000] ..........3269 ) Loss= 0.14218189\n",
      "Training [3270/10000] ..........3270 ) Loss= 0.13624252\n",
      "Training [3271/10000] ..........3271 ) Loss= 0.2003755\n",
      "Training [3272/10000] ..........3272 ) Loss= 0.17648816\n",
      "Training [3273/10000] ..........3273 ) Loss= 0.15329155\n",
      "Training [3274/10000] ..........3274 ) Loss= 0.09632681\n",
      "Training [3275/10000] ..........3275 ) Loss= 0.34814194\n",
      "Training [3276/10000] ..........3276 ) Loss= 0.10893547\n",
      "Training [3277/10000] ..........3277 ) Loss= 0.10107158\n",
      "Training [3278/10000] ..........3278 ) Loss= 0.101711966\n",
      "Training [3279/10000] ..........3279 ) Loss= 0.16057983\n",
      "Training [3280/10000] ..........3280 ) Loss= 0.2153744\n",
      "Training [3281/10000] ..........3281 ) Loss= 0.2778969\n",
      "Training [3282/10000] ..........3282 ) Loss= 0.12242156\n",
      "Training [3283/10000] ..........3283 ) Loss= 0.1418542\n",
      "Training [3284/10000] ..........3284 ) Loss= 0.23936556\n",
      "Training [3285/10000] ..........3285 ) Loss= 0.21799247\n",
      "Training [3286/10000] ..........3286 ) Loss= 0.24302298\n",
      "Training [3287/10000] ..........3287 ) Loss= 0.26052886\n",
      "Training [3288/10000] ..........3288 ) Loss= 0.28327116\n",
      "Training [3289/10000] ..........3289 ) Loss= 0.2036655\n",
      "Training [3290/10000] ..........3290 ) Loss= 0.206593\n",
      "Training [3291/10000] ..........3291 ) Loss= 0.12416074\n",
      "Training [3292/10000] ..........3292 ) Loss= 0.17862962\n",
      "Training [3293/10000] ..........3293 ) Loss= 0.18367715\n",
      "Training [3294/10000] ..........3294 ) Loss= 0.3184829\n",
      "Training [3295/10000] ..........3295 ) Loss= 0.151319\n",
      "Training [3296/10000] ..........3296 ) Loss= 0.19124046\n",
      "Training [3297/10000] ..........3297 ) Loss= 0.94421136\n",
      "Training [3298/10000] ..........3298 ) Loss= 0.11532035\n",
      "Training [3299/10000] ..........3299 ) Loss= 0.42640892\n",
      "Training [3300/10000] ..........3300 ) Loss= 0.23519251\n",
      "Training [3301/10000] ..........3301 ) Loss= 0.13056402\n",
      "Training [3302/10000] ..........3302 ) Loss= 0.16948116\n",
      "Training [3303/10000] ..........3303 ) Loss= 0.36552706\n",
      "Training [3304/10000] ..........3304 ) Loss= 0.11389209\n",
      "Training [3305/10000] ..........3305 ) Loss= 0.2065363\n",
      "Training [3306/10000] ..........3306 ) Loss= 0.2519731\n",
      "Training [3307/10000] ..........3307 ) Loss= 0.16242291\n",
      "Training [3308/10000] ..........3308 ) Loss= 0.1376658\n",
      "Training [3309/10000] ..........3309 ) Loss= 0.13860977\n",
      "Training [3310/10000] ..........3310 ) Loss= 0.22028701\n",
      "Training [3311/10000] ..........3311 ) Loss= 0.17522049\n",
      "Training [3312/10000] ..........3312 ) Loss= 0.14580147\n",
      "Training [3313/10000] ..........3313 ) Loss= 0.20629318\n",
      "Training [3314/10000] ..........3314 ) Loss= 0.14646222\n",
      "Training [3315/10000] ..........3315 ) Loss= 0.21027792\n",
      "Training [3316/10000] ..........3316 ) Loss= 0.15360233\n",
      "Training [3317/10000] ..........3317 ) Loss= 0.12745453\n",
      "Training [3318/10000] ..........3318 ) Loss= 0.13818048\n",
      "Training [3319/10000] ..........3319 ) Loss= 0.17294416\n",
      "Training [3320/10000] ..........3320 ) Loss= 0.25274307\n",
      "Training [3321/10000] ..........3321 ) Loss= 0.26378122\n",
      "Training [3322/10000] ..........3322 ) Loss= 0.13152522\n",
      "Training [3323/10000] ..........3323 ) Loss= 0.13426805\n",
      "Training [3324/10000] ..........3324 ) Loss= 0.18086487\n",
      "Training [3325/10000] ..........3325 ) Loss= 0.153364\n",
      "Training [3326/10000] ..........3326 ) Loss= 0.12080755\n",
      "Training [3327/10000] ..........3327 ) Loss= 0.14334628\n",
      "Training [3328/10000] ..........3328 ) Loss= 0.1630799\n",
      "Training [3329/10000] ..........3329 ) Loss= 0.09463964\n",
      "Training [3330/10000] ..........3330 ) Loss= 0.14933403\n",
      "Training [3331/10000] ..........3331 ) Loss= 0.23518156\n",
      "Training [3332/10000] ..........3332 ) Loss= 0.15293911\n",
      "Training [3333/10000] ..........3333 ) Loss= 0.22446987\n",
      "Training [3334/10000] ..........3334 ) Loss= 0.14159493\n",
      "Training [3335/10000] ..........3335 ) Loss= 0.20548773\n",
      "Training [3336/10000] ..........3336 ) Loss= 0.1238527\n",
      "Training [3337/10000] ..........3337 ) Loss= 0.37250307\n",
      "Training [3338/10000] ..........3338 ) Loss= 0.17526846\n",
      "Training [3339/10000] ..........3339 ) Loss= 0.1992614\n",
      "Training [3340/10000] ..........3340 ) Loss= 0.17004028\n",
      "Training [3341/10000] ..........3341 ) Loss= 0.19689967\n",
      "Training [3342/10000] ..........3342 ) Loss= 0.101150826\n",
      "Training [3343/10000] ..........3343 ) Loss= 0.13533601\n",
      "Training [3344/10000] ..........3344 ) Loss= 0.13577633\n",
      "Training [3345/10000] ..........3345 ) Loss= 0.219985\n",
      "Training [3346/10000] ..........3346 ) Loss= 0.16109835\n",
      "Training [3347/10000] ..........3347 ) Loss= 0.16330256\n",
      "Training [3348/10000] ..........3348 ) Loss= 0.11938328\n",
      "Training [3349/10000] ..........3349 ) Loss= 0.1331286\n",
      "Training [3350/10000] ..........3350 ) Loss= 0.13385563\n",
      "Training [3351/10000] ..........3351 ) Loss= 0.20522419\n",
      "Training [3352/10000] ..........3352 ) Loss= 0.19920915\n",
      "Training [3353/10000] ..........3353 ) Loss= 0.14426765\n",
      "Training [3354/10000] ..........3354 ) Loss= 0.45623547\n",
      "Training [3355/10000] ..........3355 ) Loss= 0.18792276\n",
      "Training [3356/10000] ..........3356 ) Loss= 0.1264335\n",
      "Training [3357/10000] ..........3357 ) Loss= 0.08891136\n",
      "Training [3358/10000] ..........3358 ) Loss= 0.2510085\n",
      "Training [3359/10000] ..........3359 ) Loss= 0.18460047\n",
      "Training [3360/10000] ..........3360 ) Loss= 0.091923475\n",
      "Training [3361/10000] ..........3361 ) Loss= 0.100336835\n",
      "Training [3362/10000] ..........3362 ) Loss= 0.17860794\n",
      "Training [3363/10000] ..........3363 ) Loss= 0.2063031\n",
      "Training [3364/10000] ..........3364 ) Loss= 0.15590338\n",
      "Training [3365/10000] ..........3365 ) Loss= 0.16987422\n",
      "Training [3366/10000] ..........3366 ) Loss= 0.2572045\n",
      "Training [3367/10000] ..........3367 ) Loss= 0.17804924\n",
      "Training [3368/10000] ..........3368 ) Loss= 0.20442823\n",
      "Training [3369/10000] ..........3369 ) Loss= 0.0922948\n",
      "Training [3370/10000] ..........3370 ) Loss= 0.15504795\n",
      "Training [3371/10000] ..........3371 ) Loss= 0.26492\n",
      "Training [3372/10000] ..........3372 ) Loss= 0.17694955\n",
      "Training [3373/10000] ..........3373 ) Loss= 0.13975434\n",
      "Training [3374/10000] ..........3374 ) Loss= 0.11267852\n",
      "Training [3375/10000] ..........3375 ) Loss= 0.101411864\n",
      "Training [3376/10000] ..........3376 ) Loss= 0.29711732\n",
      "Training [3377/10000] ..........3377 ) Loss= 0.14858566\n",
      "Training [3378/10000] ..........3378 ) Loss= 0.108754516\n",
      "Training [3379/10000] ..........3379 ) Loss= 0.21863516\n",
      "Training [3380/10000] ..........3380 ) Loss= 0.14395753\n",
      "Training [3381/10000] ..........3381 ) Loss= 0.113224395\n",
      "Training [3382/10000] ..........3382 ) Loss= 0.1870169\n",
      "Training [3383/10000] ..........3383 ) Loss= 0.11894847\n",
      "Training [3384/10000] ..........3384 ) Loss= 0.11267716\n",
      "Training [3385/10000] ..........3385 ) Loss= 0.10214721\n",
      "Training [3386/10000] ..........3386 ) Loss= 0.23558034\n",
      "Training [3387/10000] ..........3387 ) Loss= 0.19943827\n",
      "Training [3388/10000] ..........3388 ) Loss= 0.17150453\n",
      "Training [3389/10000] ..........3389 ) Loss= 0.15552162\n",
      "Training [3390/10000] ..........3390 ) Loss= 0.16413252\n",
      "Training [3391/10000] ..........3391 ) Loss= 0.14502877\n",
      "Training [3392/10000] ..........3392 ) Loss= 0.112478055\n",
      "Training [3393/10000] ..........3393 ) Loss= 0.2527477\n",
      "Training [3394/10000] ..........3394 ) Loss= 0.07916424\n",
      "Training [3395/10000] ..........3395 ) Loss= 0.13602233\n",
      "Training [3396/10000] ..........3396 ) Loss= 0.1525585\n",
      "Training [3397/10000] ..........3397 ) Loss= 0.15975715\n",
      "Training [3398/10000] ..........3398 ) Loss= 0.12265671\n",
      "Training [3399/10000] ..........3399 ) Loss= 0.13565053\n",
      "Training [3400/10000] ..........3400 ) Loss= 0.34022376\n",
      "Training [3401/10000] ..........3401 ) Loss= 0.09527234\n",
      "Training [3402/10000] ..........3402 ) Loss= 0.15720794\n",
      "Training [3403/10000] ..........3403 ) Loss= 0.17675497\n",
      "Training [3404/10000] ..........3404 ) Loss= 0.50328326\n",
      "Training [3405/10000] ..........3405 ) Loss= 0.13053748\n",
      "Training [3406/10000] ..........3406 ) Loss= 0.16363254\n",
      "Training [3407/10000] ..........3407 ) Loss= 0.08650776\n",
      "Training [3408/10000] ..........3408 ) Loss= 0.09093856\n",
      "Training [3409/10000] ..........3409 ) Loss= 0.17020254\n",
      "Training [3410/10000] ..........3410 ) Loss= 0.0892835\n",
      "Training [3411/10000] ..........3411 ) Loss= 0.1859676\n",
      "Training [3412/10000] ..........3412 ) Loss= 0.098922215\n",
      "Training [3413/10000] ..........3413 ) Loss= 0.16764419\n",
      "Training [3414/10000] ..........3414 ) Loss= 0.1685084\n",
      "Training [3415/10000] ..........3415 ) Loss= 0.16415471\n",
      "Training [3416/10000] ..........3416 ) Loss= 0.10144263\n",
      "Training [3417/10000] ..........3417 ) Loss= 0.09959473\n",
      "Training [3418/10000] ..........3418 ) Loss= 0.16253984\n",
      "Training [3419/10000] ..........3419 ) Loss= 0.14242256\n",
      "Training [3420/10000] ..........3420 ) Loss= 0.23960707\n",
      "Training [3421/10000] ..........3421 ) Loss= 0.118058294\n",
      "Training [3422/10000] ..........3422 ) Loss= 0.10597519\n",
      "Training [3423/10000] ..........3423 ) Loss= 0.16174474\n",
      "Training [3424/10000] ..........3424 ) Loss= 0.20632347\n",
      "Training [3425/10000] ..........3425 ) Loss= 0.19130158\n",
      "Training [3426/10000] ..........3426 ) Loss= 0.11413348\n",
      "Training [3427/10000] ..........3427 ) Loss= 0.19855374\n",
      "Training [3428/10000] ..........3428 ) Loss= 0.109851286\n",
      "Training [3429/10000] ..........3429 ) Loss= 0.124270685\n",
      "Training [3430/10000] ..........3430 ) Loss= 0.08715237\n",
      "Training [3431/10000] ..........3431 ) Loss= 0.150587\n",
      "Training [3432/10000] ..........3432 ) Loss= 0.14919625\n",
      "Training [3433/10000] ..........3433 ) Loss= 0.20812178\n",
      "Training [3434/10000] ..........3434 ) Loss= 0.22016591\n",
      "Training [3435/10000] ..........3435 ) Loss= 0.21774791\n",
      "Training [3436/10000] ..........3436 ) Loss= 0.14188346\n",
      "Training [3437/10000] ..........3437 ) Loss= 0.16971211\n",
      "Training [3438/10000] ..........3438 ) Loss= 0.3015865\n",
      "Training [3439/10000] ..........3439 ) Loss= 0.19316435\n",
      "Training [3440/10000] ..........3440 ) Loss= 0.17873235\n",
      "Training [3441/10000] ..........3441 ) Loss= 0.19811931\n",
      "Training [3442/10000] ..........3442 ) Loss= 0.23291618\n",
      "Training [3443/10000] ..........3443 ) Loss= 0.6456785\n",
      "Training [3444/10000] ..........3444 ) Loss= 0.16790293\n",
      "Training [3445/10000] ..........3445 ) Loss= 0.29817626\n",
      "Training [3446/10000] ..........3446 ) Loss= 0.09151264\n",
      "Training [3447/10000] ..........3447 ) Loss= 0.24126823\n",
      "Training [3448/10000] ..........3448 ) Loss= 0.21411848\n",
      "Training [3449/10000] ..........3449 ) Loss= 0.1733512\n",
      "Training [3450/10000] ..........3450 ) Loss= 0.1403484\n",
      "Training [3451/10000] ..........3451 ) Loss= 0.16327699\n",
      "Training [3452/10000] ..........3452 ) Loss= 0.107452504\n",
      "Training [3453/10000] ..........3453 ) Loss= 0.19618173\n",
      "Training [3454/10000] ..........3454 ) Loss= 0.16801018\n",
      "Training [3455/10000] ..........3455 ) Loss= 0.14677624\n",
      "Training [3456/10000] ..........3456 ) Loss= 0.20252536\n",
      "Training [3457/10000] ..........3457 ) Loss= 0.17317963\n",
      "Training [3458/10000] ..........3458 ) Loss= 0.103476524\n",
      "Training [3459/10000] ..........3459 ) Loss= 0.16790701\n",
      "Training [3460/10000] ..........3460 ) Loss= 0.13805044\n",
      "Training [3461/10000] ..........3461 ) Loss= 0.10967557\n",
      "Training [3462/10000] ..........3462 ) Loss= 0.13460694\n",
      "Training [3463/10000] ..........3463 ) Loss= 0.18077566\n",
      "Training [3464/10000] ..........3464 ) Loss= 0.11777273\n",
      "Training [3465/10000] ..........3465 ) Loss= 0.24000569\n",
      "Training [3466/10000] ..........3466 ) Loss= 0.12394183\n",
      "Training [3467/10000] ..........3467 ) Loss= 0.12641963\n",
      "Training [3468/10000] ..........3468 ) Loss= 0.14120549\n",
      "Training [3469/10000] ..........3469 ) Loss= 0.1575997\n",
      "Training [3470/10000] ..........3470 ) Loss= 0.17540728\n",
      "Training [3471/10000] ..........3471 ) Loss= 0.26639038\n",
      "Training [3472/10000] ..........3472 ) Loss= 0.1457256\n",
      "Training [3473/10000] ..........3473 ) Loss= 0.1309936\n",
      "Training [3474/10000] ..........3474 ) Loss= 0.27300242\n",
      "Training [3475/10000] ..........3475 ) Loss= 0.11073879\n",
      "Training [3476/10000] ..........3476 ) Loss= 0.13795356\n",
      "Training [3477/10000] ..........3477 ) Loss= 0.17609157\n",
      "Training [3478/10000] ..........3478 ) Loss= 0.13290977\n",
      "Training [3479/10000] ..........3479 ) Loss= 0.12003735\n",
      "Training [3480/10000] ..........3480 ) Loss= 0.13220353\n",
      "Training [3481/10000] ..........3481 ) Loss= 0.2057515\n",
      "Training [3482/10000] ..........3482 ) Loss= 0.1618162\n",
      "Training [3483/10000] ..........3483 ) Loss= 0.16962846\n",
      "Training [3484/10000] ..........3484 ) Loss= 0.21807812\n",
      "Training [3485/10000] ..........3485 ) Loss= 0.11340596\n",
      "Training [3486/10000] ..........3486 ) Loss= 0.23499112\n",
      "Training [3487/10000] ..........3487 ) Loss= 0.14776836\n",
      "Training [3488/10000] ..........3488 ) Loss= 0.17254347\n",
      "Training [3489/10000] ..........3489 ) Loss= 0.11597929\n",
      "Training [3490/10000] ..........3490 ) Loss= 0.29144555\n",
      "Training [3491/10000] ..........3491 ) Loss= 0.16579416\n",
      "Training [3492/10000] ..........3492 ) Loss= 0.14430335\n",
      "Training [3493/10000] ..........3493 ) Loss= 0.13285775\n",
      "Training [3494/10000] ..........3494 ) Loss= 0.12011002\n",
      "Training [3495/10000] ..........3495 ) Loss= 0.095732145\n",
      "Training [3496/10000] ..........3496 ) Loss= 0.17351723\n",
      "Training [3497/10000] ..........3497 ) Loss= 0.14174613\n",
      "Training [3498/10000] ..........3498 ) Loss= 0.13849647\n",
      "Training [3499/10000] ..........3499 ) Loss= 0.1376606\n",
      "Training [3500/10000] ..........3500 ) Loss= 0.40049407\n",
      "Training [3501/10000] ..........3501 ) Loss= 0.15078959\n",
      "Training [3502/10000] ..........3502 ) Loss= 0.16538782\n",
      "Training [3503/10000] ..........3503 ) Loss= 0.2460908\n",
      "Training [3504/10000] ..........3504 ) Loss= 0.18770924\n",
      "Training [3505/10000] ..........3505 ) Loss= 0.16283372\n",
      "Training [3506/10000] ..........3506 ) Loss= 0.14695427\n",
      "Training [3507/10000] ..........3507 ) Loss= 0.10027962\n",
      "Training [3508/10000] ..........3508 ) Loss= 0.12303965\n",
      "Training [3509/10000] ..........3509 ) Loss= 0.15272732\n",
      "Training [3510/10000] ..........3510 ) Loss= 0.15339294\n",
      "Training [3511/10000] ..........3511 ) Loss= 0.595585\n",
      "Training [3512/10000] ..........3512 ) Loss= 0.12843604\n",
      "Training [3513/10000] ..........3513 ) Loss= 0.17973632\n",
      "Training [3514/10000] ..........3514 ) Loss= 0.124559976\n",
      "Training [3515/10000] ..........3515 ) Loss= 0.10403679\n",
      "Training [3516/10000] ..........3516 ) Loss= 0.16252737\n",
      "Training [3517/10000] ..........3517 ) Loss= 0.1099558\n",
      "Training [3518/10000] ..........3518 ) Loss= 0.17398724\n",
      "Training [3519/10000] ..........3519 ) Loss= 0.20962393\n",
      "Training [3520/10000] ..........3520 ) Loss= 0.08576149\n",
      "Training [3521/10000] ..........3521 ) Loss= 0.12744929\n",
      "Training [3522/10000] ..........3522 ) Loss= 1.3443375\n",
      "Training [3523/10000] ..........3523 ) Loss= 0.114815116\n",
      "Training [3524/10000] ..........3524 ) Loss= 0.113899946\n",
      "Training [3525/10000] ..........3525 ) Loss= 0.14281715\n",
      "Training [3526/10000] ..........3526 ) Loss= 0.09675833\n",
      "Training [3527/10000] ..........3527 ) Loss= 0.47196004\n",
      "Training [3528/10000] ..........3528 ) Loss= 0.15834284\n",
      "Training [3529/10000] ..........3529 ) Loss= 0.109235756\n",
      "Training [3530/10000] ..........3530 ) Loss= 0.2028284\n",
      "Training [3531/10000] ..........3531 ) Loss= 0.075982034\n",
      "Training [3532/10000] ..........3532 ) Loss= 0.1695018\n",
      "Training [3533/10000] ..........3533 ) Loss= 0.11866228\n",
      "Training [3534/10000] ..........3534 ) Loss= 0.16901582\n",
      "Training [3535/10000] ..........3535 ) Loss= 0.14494595\n",
      "Training [3536/10000] ..........3536 ) Loss= 0.12595128\n",
      "Training [3537/10000] ..........3537 ) Loss= 0.11106064\n",
      "Training [3538/10000] ..........3538 ) Loss= 0.1590359\n",
      "Training [3539/10000] ..........3539 ) Loss= 0.16502903\n",
      "Training [3540/10000] ..........3540 ) Loss= 0.21126017\n",
      "Training [3541/10000] ..........3541 ) Loss= 0.11901376\n",
      "Training [3542/10000] ..........3542 ) Loss= 0.3125202\n",
      "Training [3543/10000] ..........3543 ) Loss= 0.09494265\n",
      "Training [3544/10000] ..........3544 ) Loss= 0.61845237\n",
      "Training [3545/10000] ..........3545 ) Loss= 0.18962902\n",
      "Training [3546/10000] ..........3546 ) Loss= 0.10833415\n",
      "Training [3547/10000] ..........3547 ) Loss= 0.117453985\n",
      "Training [3548/10000] ..........3548 ) Loss= 0.24030381\n",
      "Training [3549/10000] ..........3549 ) Loss= 0.14195588\n",
      "Training [3550/10000] ..........3550 ) Loss= 0.112260975\n",
      "Training [3551/10000] ..........3551 ) Loss= 0.1784358\n",
      "Training [3552/10000] ..........3552 ) Loss= 0.32217476\n",
      "Training [3553/10000] ..........3553 ) Loss= 0.29881448\n",
      "Training [3554/10000] ..........3554 ) Loss= 0.22697148\n",
      "Training [3555/10000] ..........3555 ) Loss= 0.22846708\n",
      "Training [3556/10000] ..........3556 ) Loss= 0.124412745\n",
      "Training [3557/10000] ..........3557 ) Loss= 0.15457322\n",
      "Training [3558/10000] ..........3558 ) Loss= 0.12827806\n",
      "Training [3559/10000] ..........3559 ) Loss= 0.11140419\n",
      "Training [3560/10000] ..........3560 ) Loss= 0.15099125\n",
      "Training [3561/10000] ..........3561 ) Loss= 0.13802825\n",
      "Training [3562/10000] ..........3562 ) Loss= 0.23723444\n",
      "Training [3563/10000] ..........3563 ) Loss= 0.14553224\n",
      "Training [3564/10000] ..........3564 ) Loss= 0.30225676\n",
      "Training [3565/10000] ..........3565 ) Loss= 0.19192658\n",
      "Training [3566/10000] ..........3566 ) Loss= 0.18385349\n",
      "Training [3567/10000] ..........3567 ) Loss= 0.09136833\n",
      "Training [3568/10000] ..........3568 ) Loss= 0.10700763\n",
      "Training [3569/10000] ..........3569 ) Loss= 0.121613964\n",
      "Training [3570/10000] ..........3570 ) Loss= 0.15202288\n",
      "Training [3571/10000] ..........3571 ) Loss= 0.34356272\n",
      "Training [3572/10000] ..........3572 ) Loss= 0.20702007\n",
      "Training [3573/10000] ..........3573 ) Loss= 0.1481951\n",
      "Training [3574/10000] ..........3574 ) Loss= 0.12515233\n",
      "Training [3575/10000] ..........3575 ) Loss= 0.324309\n",
      "Training [3576/10000] ..........3576 ) Loss= 0.20740144\n",
      "Training [3577/10000] ..........3577 ) Loss= 0.18724027\n",
      "Training [3578/10000] ..........3578 ) Loss= 0.13382989\n",
      "Training [3579/10000] ..........3579 ) Loss= 0.09234031\n",
      "Training [3580/10000] ..........3580 ) Loss= 0.15463482\n",
      "Training [3581/10000] ..........3581 ) Loss= 0.08684022\n",
      "Training [3582/10000] ..........3582 ) Loss= 0.22127008\n",
      "Training [3583/10000] ..........3583 ) Loss= 0.1265266\n",
      "Training [3584/10000] ..........3584 ) Loss= 0.10829088\n",
      "Training [3585/10000] ..........3585 ) Loss= 0.069762625\n",
      "Training [3586/10000] ..........3586 ) Loss= 0.16227718\n",
      "Training [3587/10000] ..........3587 ) Loss= 0.21274026\n",
      "Training [3588/10000] ..........3588 ) Loss= 0.095400214\n",
      "Training [3589/10000] ..........3589 ) Loss= 0.17970605\n",
      "Training [3590/10000] ..........3590 ) Loss= 0.2959622\n",
      "Training [3591/10000] ..........3591 ) Loss= 0.17539196\n",
      "Training [3592/10000] ..........3592 ) Loss= 0.17911386\n",
      "Training [3593/10000] ..........3593 ) Loss= 0.24447823\n",
      "Training [3594/10000] ..........3594 ) Loss= 0.14834665\n",
      "Training [3595/10000] ..........3595 ) Loss= 0.20362692\n",
      "Training [3596/10000] ..........3596 ) Loss= 0.24554592\n",
      "Training [3597/10000] ..........3597 ) Loss= 0.18427224\n",
      "Training [3598/10000] ..........3598 ) Loss= 0.16017713\n",
      "Training [3599/10000] ..........3599 ) Loss= 0.22113962\n",
      "Training [3600/10000] ..........3600 ) Loss= 0.15867305\n",
      "Training [3601/10000] ..........3601 ) Loss= 0.13874654\n",
      "Training [3602/10000] ..........3602 ) Loss= 0.18091771\n",
      "Training [3603/10000] ..........3603 ) Loss= 0.17220537\n",
      "Training [3604/10000] ..........3604 ) Loss= 0.28782848\n",
      "Training [3605/10000] ..........3605 ) Loss= 0.1409436\n",
      "Training [3606/10000] ..........3606 ) Loss= 0.17383604\n",
      "Training [3607/10000] ..........3607 ) Loss= 0.1859271\n",
      "Training [3608/10000] ..........3608 ) Loss= 0.12038003\n",
      "Training [3609/10000] ..........3609 ) Loss= 0.22379857\n",
      "Training [3610/10000] ..........3610 ) Loss= 0.13105348\n",
      "Training [3611/10000] ..........3611 ) Loss= 0.18134533\n",
      "Training [3612/10000] ..........3612 ) Loss= 0.08552302\n",
      "Training [3613/10000] ..........3613 ) Loss= 0.13000312\n",
      "Training [3614/10000] ..........3614 ) Loss= 0.105301544\n",
      "Training [3615/10000] ..........3615 ) Loss= 0.17221768\n",
      "Training [3616/10000] ..........3616 ) Loss= 0.33231908\n",
      "Training [3617/10000] ..........3617 ) Loss= 0.09439923\n",
      "Training [3618/10000] ..........3618 ) Loss= 0.14802456\n",
      "Training [3619/10000] ..........3619 ) Loss= 0.100236185\n",
      "Training [3620/10000] ..........3620 ) Loss= 0.21897084\n",
      "Training [3621/10000] ..........3621 ) Loss= 0.2121487\n",
      "Training [3622/10000] ..........3622 ) Loss= 0.15150468\n",
      "Training [3623/10000] ..........3623 ) Loss= 0.087056234\n",
      "Training [3624/10000] ..........3624 ) Loss= 0.12092479\n",
      "Training [3625/10000] ..........3625 ) Loss= 0.19771269\n",
      "Training [3626/10000] ..........3626 ) Loss= 0.14816253\n",
      "Training [3627/10000] ..........3627 ) Loss= 0.16272525\n",
      "Training [3628/10000] ..........3628 ) Loss= 0.2073853\n",
      "Training [3629/10000] ..........3629 ) Loss= 0.105548054\n",
      "Training [3630/10000] ..........3630 ) Loss= 0.1732284\n",
      "Training [3631/10000] ..........3631 ) Loss= 0.12918524\n",
      "Training [3632/10000] ..........3632 ) Loss= 0.08807561\n",
      "Training [3633/10000] ..........3633 ) Loss= 0.19776082\n",
      "Training [3634/10000] ..........3634 ) Loss= 0.13824338\n",
      "Training [3635/10000] ..........3635 ) Loss= 0.14725608\n",
      "Training [3636/10000] ..........3636 ) Loss= 0.08023179\n",
      "Training [3637/10000] ..........3637 ) Loss= 0.1574239\n",
      "Training [3638/10000] ..........3638 ) Loss= 0.1360213\n",
      "Training [3639/10000] ..........3639 ) Loss= 0.17459497\n",
      "Training [3640/10000] ..........3640 ) Loss= 0.17931694\n",
      "Training [3641/10000] ..........3641 ) Loss= 0.12573439\n",
      "Training [3642/10000] ..........3642 ) Loss= 0.26667\n",
      "Training [3643/10000] ..........3643 ) Loss= 0.19344209\n",
      "Training [3644/10000] ..........3644 ) Loss= 0.092379004\n",
      "Training [3645/10000] ..........3645 ) Loss= 0.1857415\n",
      "Training [3646/10000] ..........3646 ) Loss= 0.11464013\n",
      "Training [3647/10000] ..........3647 ) Loss= 0.16607107\n",
      "Training [3648/10000] ..........3648 ) Loss= 0.14148057\n",
      "Training [3649/10000] ..........3649 ) Loss= 0.088253215\n",
      "Training [3650/10000] ..........3650 ) Loss= 0.15939136\n",
      "Training [3651/10000] ..........3651 ) Loss= 0.28556487\n",
      "Training [3652/10000] ..........3652 ) Loss= 0.26209044\n",
      "Training [3653/10000] ..........3653 ) Loss= 0.12524939\n",
      "Training [3654/10000] ..........3654 ) Loss= 0.12706678\n",
      "Training [3655/10000] ..........3655 ) Loss= 0.08905179\n",
      "Training [3656/10000] ..........3656 ) Loss= 0.2603969\n",
      "Training [3657/10000] ..........3657 ) Loss= 0.10329087\n",
      "Training [3658/10000] ..........3658 ) Loss= 0.26148182\n",
      "Training [3659/10000] ..........3659 ) Loss= 0.111498274\n",
      "Training [3660/10000] ..........3660 ) Loss= 0.12381474\n",
      "Training [3661/10000] ..........3661 ) Loss= 0.13667369\n",
      "Training [3662/10000] ..........3662 ) Loss= 0.09681967\n",
      "Training [3663/10000] ..........3663 ) Loss= 0.15172675\n",
      "Training [3664/10000] ..........3664 ) Loss= 0.26594558\n",
      "Training [3665/10000] ..........3665 ) Loss= 0.20210803\n",
      "Training [3666/10000] ..........3666 ) Loss= 0.21810867\n",
      "Training [3667/10000] ..........3667 ) Loss= 0.3589186\n",
      "Training [3668/10000] ..........3668 ) Loss= 0.12811396\n",
      "Training [3669/10000] ..........3669 ) Loss= 0.077030435\n",
      "Training [3670/10000] ..........3670 ) Loss= 0.15198228\n",
      "Training [3671/10000] ..........3671 ) Loss= 0.118682034\n",
      "Training [3672/10000] ..........3672 ) Loss= 0.13176508\n",
      "Training [3673/10000] ..........3673 ) Loss= 0.11770677\n",
      "Training [3674/10000] ..........3674 ) Loss= 0.22151402\n",
      "Training [3675/10000] ..........3675 ) Loss= 0.11922272\n",
      "Training [3676/10000] ..........3676 ) Loss= 0.14758682\n",
      "Training [3677/10000] ..........3677 ) Loss= 0.1956122\n",
      "Training [3678/10000] ..........3678 ) Loss= 0.17229356\n",
      "Training [3679/10000] ..........3679 ) Loss= 0.1068122\n",
      "Training [3680/10000] ..........3680 ) Loss= 0.22731288\n",
      "Training [3681/10000] ..........3681 ) Loss= 0.254118\n",
      "Training [3682/10000] ..........3682 ) Loss= 0.090150945\n",
      "Training [3683/10000] ..........3683 ) Loss= 0.23152328\n",
      "Training [3684/10000] ..........3684 ) Loss= 0.10736634\n",
      "Training [3685/10000] ..........3685 ) Loss= 0.29534253\n",
      "Training [3686/10000] ..........3686 ) Loss= 0.16306953\n",
      "Training [3687/10000] ..........3687 ) Loss= 0.111240104\n",
      "Training [3688/10000] ..........3688 ) Loss= 0.39754832\n",
      "Training [3689/10000] ..........3689 ) Loss= 0.14655067\n",
      "Training [3690/10000] ..........3690 ) Loss= 0.13799772\n",
      "Training [3691/10000] ..........3691 ) Loss= 0.155144\n",
      "Training [3692/10000] ..........3692 ) Loss= 0.28332922\n",
      "Training [3693/10000] ..........3693 ) Loss= 0.13761489\n",
      "Training [3694/10000] ..........3694 ) Loss= 0.0860463\n",
      "Training [3695/10000] ..........3695 ) Loss= 0.1927399\n",
      "Training [3696/10000] ..........3696 ) Loss= 0.13050437\n",
      "Training [3697/10000] ..........3697 ) Loss= 0.23802632\n",
      "Training [3698/10000] ..........3698 ) Loss= 0.11081554\n",
      "Training [3699/10000] ..........3699 ) Loss= 0.20275141\n",
      "Training [3700/10000] ..........3700 ) Loss= 0.13294515\n",
      "Training [3701/10000] ..........3701 ) Loss= 0.09957287\n",
      "Training [3702/10000] ..........3702 ) Loss= 0.0968623\n",
      "Training [3703/10000] ..........3703 ) Loss= 0.113304876\n",
      "Training [3704/10000] ..........3704 ) Loss= 0.29051712\n",
      "Training [3705/10000] ..........3705 ) Loss= 0.13382821\n",
      "Training [3706/10000] ..........3706 ) Loss= 0.16486585\n",
      "Training [3707/10000] ..........3707 ) Loss= 0.23699224\n",
      "Training [3708/10000] ..........3708 ) Loss= 0.1515863\n",
      "Training [3709/10000] ..........3709 ) Loss= 0.10534035\n",
      "Training [3710/10000] ..........3710 ) Loss= 0.121830925\n",
      "Training [3711/10000] ..........3711 ) Loss= 0.11995584\n",
      "Training [3712/10000] ..........3712 ) Loss= 0.19897729\n",
      "Training [3713/10000] ..........3713 ) Loss= 0.1675157\n",
      "Training [3714/10000] ..........3714 ) Loss= 0.15863697\n",
      "Training [3715/10000] ..........3715 ) Loss= 0.08783361\n",
      "Training [3716/10000] ..........3716 ) Loss= 0.1560485\n",
      "Training [3717/10000] ..........3717 ) Loss= 0.1980822\n",
      "Training [3718/10000] ..........3718 ) Loss= 0.08468331\n",
      "Training [3719/10000] ..........3719 ) Loss= 0.12260549\n",
      "Training [3720/10000] ..........3720 ) Loss= 0.19701947\n",
      "Training [3721/10000] ..........3721 ) Loss= 0.111017294\n",
      "Training [3722/10000] ..........3722 ) Loss= 0.16801584\n",
      "Training [3723/10000] ..........3723 ) Loss= 0.15926741\n",
      "Training [3724/10000] ..........3724 ) Loss= 0.26539677\n",
      "Training [3725/10000] ..........3725 ) Loss= 0.11317709\n",
      "Training [3726/10000] ..........3726 ) Loss= 0.31688267\n",
      "Training [3727/10000] ..........3727 ) Loss= 0.11858177\n",
      "Training [3728/10000] ..........3728 ) Loss= 0.12960236\n",
      "Training [3729/10000] ..........3729 ) Loss= 0.13380755\n",
      "Training [3730/10000] ..........3730 ) Loss= 0.104283154\n",
      "Training [3731/10000] ..........3731 ) Loss= 0.1301187\n",
      "Training [3732/10000] ..........3732 ) Loss= 0.17715578\n",
      "Training [3733/10000] ..........3733 ) Loss= 0.38628894\n",
      "Training [3734/10000] ..........3734 ) Loss= 0.15350714\n",
      "Training [3735/10000] ..........3735 ) Loss= 0.09827117\n",
      "Training [3736/10000] ..........3736 ) Loss= 0.16887634\n",
      "Training [3737/10000] ..........3737 ) Loss= 0.50497323\n",
      "Training [3738/10000] ..........3738 ) Loss= 0.12593003\n",
      "Training [3739/10000] ..........3739 ) Loss= 0.11328916\n",
      "Training [3740/10000] ..........3740 ) Loss= 0.15886673\n",
      "Training [3741/10000] ..........3741 ) Loss= 0.17803048\n",
      "Training [3742/10000] ..........3742 ) Loss= 0.17466204\n",
      "Training [3743/10000] ..........3743 ) Loss= 0.16689655\n",
      "Training [3744/10000] ..........3744 ) Loss= 0.1444404\n",
      "Training [3745/10000] ..........3745 ) Loss= 0.09860161\n",
      "Training [3746/10000] ..........3746 ) Loss= 0.13412924\n",
      "Training [3747/10000] ..........3747 ) Loss= 0.11953617\n",
      "Training [3748/10000] ..........3748 ) Loss= 0.15631874\n",
      "Training [3749/10000] ..........3749 ) Loss= 0.12125664\n",
      "Training [3750/10000] ..........3750 ) Loss= 0.09536004\n",
      "Training [3751/10000] ..........3751 ) Loss= 0.14964037\n",
      "Training [3752/10000] ..........3752 ) Loss= 0.09674071\n",
      "Training [3753/10000] ..........3753 ) Loss= 0.23029618\n",
      "Training [3754/10000] ..........3754 ) Loss= 0.12851095\n",
      "Training [3755/10000] ..........3755 ) Loss= 0.17786998\n",
      "Training [3756/10000] ..........3756 ) Loss= 0.11391239\n",
      "Training [3757/10000] ..........3757 ) Loss= 0.14549957\n",
      "Training [3758/10000] ..........3758 ) Loss= 0.26890886\n",
      "Training [3759/10000] ..........3759 ) Loss= 0.19757171\n",
      "Training [3760/10000] ..........3760 ) Loss= 0.09197796\n",
      "Training [3761/10000] ..........3761 ) Loss= 0.1747449\n",
      "Training [3762/10000] ..........3762 ) Loss= 0.111096606\n",
      "Training [3763/10000] ..........3763 ) Loss= 0.11443839\n",
      "Training [3764/10000] ..........3764 ) Loss= 0.12770905\n",
      "Training [3765/10000] ..........3765 ) Loss= 0.16715208\n",
      "Training [3766/10000] ..........3766 ) Loss= 0.2065328\n",
      "Training [3767/10000] ..........3767 ) Loss= 0.22581452\n",
      "Training [3768/10000] ..........3768 ) Loss= 0.14548679\n",
      "Training [3769/10000] ..........3769 ) Loss= 0.15922344\n",
      "Training [3770/10000] ..........3770 ) Loss= 0.122507334\n",
      "Training [3771/10000] ..........3771 ) Loss= 0.22270647\n",
      "Training [3772/10000] ..........3772 ) Loss= 0.1633185\n",
      "Training [3773/10000] ..........3773 ) Loss= 0.19135241\n",
      "Training [3774/10000] ..........3774 ) Loss= 0.14617562\n",
      "Training [3775/10000] ..........3775 ) Loss= 0.17347778\n",
      "Training [3776/10000] ..........3776 ) Loss= 0.30939388\n",
      "Training [3777/10000] ..........3777 ) Loss= 0.13247626\n",
      "Training [3778/10000] ..........3778 ) Loss= 0.35578734\n",
      "Training [3779/10000] ..........3779 ) Loss= 0.11287854\n",
      "Training [3780/10000] ..........3780 ) Loss= 0.17593345\n",
      "Training [3781/10000] ..........3781 ) Loss= 0.14370026\n",
      "Training [3782/10000] ..........3782 ) Loss= 0.19021194\n",
      "Training [3783/10000] ..........3783 ) Loss= 0.107989915\n",
      "Training [3784/10000] ..........3784 ) Loss= 0.1611973\n",
      "Training [3785/10000] ..........3785 ) Loss= 0.12815888\n",
      "Training [3786/10000] ..........3786 ) Loss= 0.13596381\n",
      "Training [3787/10000] ..........3787 ) Loss= 0.15804261\n",
      "Training [3788/10000] ..........3788 ) Loss= 0.06878569\n",
      "Training [3789/10000] ..........3789 ) Loss= 0.12893231\n",
      "Training [3790/10000] ..........3790 ) Loss= 0.16913489\n",
      "Training [3791/10000] ..........3791 ) Loss= 0.23800145\n",
      "Training [3792/10000] ..........3792 ) Loss= 0.11109755\n",
      "Training [3793/10000] ..........3793 ) Loss= 0.15963642\n",
      "Training [3794/10000] ..........3794 ) Loss= 0.16838731\n",
      "Training [3795/10000] ..........3795 ) Loss= 0.12645198\n",
      "Training [3796/10000] ..........3796 ) Loss= 0.115594044\n",
      "Training [3797/10000] ..........3797 ) Loss= 0.17616504\n",
      "Training [3798/10000] ..........3798 ) Loss= 0.11584595\n",
      "Training [3799/10000] ..........3799 ) Loss= 0.11491623\n",
      "Training [3800/10000] ..........3800 ) Loss= 0.09715165\n",
      "Training [3801/10000] ..........3801 ) Loss= 0.10724487\n",
      "Training [3802/10000] ..........3802 ) Loss= 0.12967366\n",
      "Training [3803/10000] ..........3803 ) Loss= 0.16082801\n",
      "Training [3804/10000] ..........3804 ) Loss= 0.17790931\n",
      "Training [3805/10000] ..........3805 ) Loss= 0.13678113\n",
      "Training [3806/10000] ..........3806 ) Loss= 0.12562776\n",
      "Training [3807/10000] ..........3807 ) Loss= 0.14308219\n",
      "Training [3808/10000] ..........3808 ) Loss= 0.111827426\n",
      "Training [3809/10000] ..........3809 ) Loss= 0.14114219\n",
      "Training [3810/10000] ..........3810 ) Loss= 0.10012575\n",
      "Training [3811/10000] ..........3811 ) Loss= 0.09548377\n",
      "Training [3812/10000] ..........3812 ) Loss= 0.12271971\n",
      "Training [3813/10000] ..........3813 ) Loss= 0.10729928\n",
      "Training [3814/10000] ..........3814 ) Loss= 0.123138666\n",
      "Training [3815/10000] ..........3815 ) Loss= 0.13797727\n",
      "Training [3816/10000] ..........3816 ) Loss= 0.120061904\n",
      "Training [3817/10000] ..........3817 ) Loss= 0.15769273\n",
      "Training [3818/10000] ..........3818 ) Loss= 0.12717587\n",
      "Training [3819/10000] ..........3819 ) Loss= 0.14845088\n",
      "Training [3820/10000] ..........3820 ) Loss= 0.0707089\n",
      "Training [3821/10000] ..........3821 ) Loss= 0.30576095\n",
      "Training [3822/10000] ..........3822 ) Loss= 0.18281487\n",
      "Training [3823/10000] ..........3823 ) Loss= 0.19671433\n",
      "Training [3824/10000] ..........3824 ) Loss= 0.121355705\n",
      "Training [3825/10000] ..........3825 ) Loss= 0.08348426\n",
      "Training [3826/10000] ..........3826 ) Loss= 0.08680061\n",
      "Training [3827/10000] ..........3827 ) Loss= 0.16660333\n",
      "Training [3828/10000] ..........3828 ) Loss= 0.13875706\n",
      "Training [3829/10000] ..........3829 ) Loss= 0.12409509\n",
      "Training [3830/10000] ..........3830 ) Loss= 0.12596099\n",
      "Training [3831/10000] ..........3831 ) Loss= 0.1627456\n",
      "Training [3832/10000] ..........3832 ) Loss= 0.15331393\n",
      "Training [3833/10000] ..........3833 ) Loss= 0.17643483\n",
      "Training [3834/10000] ..........3834 ) Loss= 0.098293245\n",
      "Training [3835/10000] ..........3835 ) Loss= 0.20601253\n",
      "Training [3836/10000] ..........3836 ) Loss= 0.15187907\n",
      "Training [3837/10000] ..........3837 ) Loss= 0.2475332\n",
      "Training [3838/10000] ..........3838 ) Loss= 0.18732241\n",
      "Training [3839/10000] ..........3839 ) Loss= 0.18112789\n",
      "Training [3840/10000] ..........3840 ) Loss= 0.18012452\n",
      "Training [3841/10000] ..........3841 ) Loss= 0.4356627\n",
      "Training [3842/10000] ..........3842 ) Loss= 0.1711713\n",
      "Training [3843/10000] ..........3843 ) Loss= 0.12256326\n",
      "Training [3844/10000] ..........3844 ) Loss= 0.0876076\n",
      "Training [3845/10000] ..........3845 ) Loss= 0.13715842\n",
      "Training [3846/10000] ..........3846 ) Loss= 0.08466571\n",
      "Training [3847/10000] ..........3847 ) Loss= 0.28102103\n",
      "Training [3848/10000] ..........3848 ) Loss= 0.22939801\n",
      "Training [3849/10000] ..........3849 ) Loss= 0.20273939\n",
      "Training [3850/10000] ..........3850 ) Loss= 0.120386705\n",
      "Training [3851/10000] ..........3851 ) Loss= 0.13649985\n",
      "Training [3852/10000] ..........3852 ) Loss= 0.24556099\n",
      "Training [3853/10000] ..........3853 ) Loss= 0.15205845\n",
      "Training [3854/10000] ..........3854 ) Loss= 0.20931692\n",
      "Training [3855/10000] ..........3855 ) Loss= 0.077994816\n",
      "Training [3856/10000] ..........3856 ) Loss= 0.08875284\n",
      "Training [3857/10000] ..........3857 ) Loss= 0.25761315\n",
      "Training [3858/10000] ..........3858 ) Loss= 0.099033\n",
      "Training [3859/10000] ..........3859 ) Loss= 0.12328508\n",
      "Training [3860/10000] ..........3860 ) Loss= 0.16449286\n",
      "Training [3861/10000] ..........3861 ) Loss= 0.09542646\n",
      "Training [3862/10000] ..........3862 ) Loss= 0.09969671\n",
      "Training [3863/10000] ..........3863 ) Loss= 0.12911862\n",
      "Training [3864/10000] ..........3864 ) Loss= 0.098791294\n",
      "Training [3865/10000] ..........3865 ) Loss= 0.17678128\n",
      "Training [3866/10000] ..........3866 ) Loss= 0.13493824\n",
      "Training [3867/10000] ..........3867 ) Loss= 0.120193884\n",
      "Training [3868/10000] ..........3868 ) Loss= 0.13444795\n",
      "Training [3869/10000] ..........3869 ) Loss= 0.1628728\n",
      "Training [3870/10000] ..........3870 ) Loss= 0.11384425\n",
      "Training [3871/10000] ..........3871 ) Loss= 0.11138227\n",
      "Training [3872/10000] ..........3872 ) Loss= 0.11443733\n",
      "Training [3873/10000] ..........3873 ) Loss= 0.22093676\n",
      "Training [3874/10000] ..........3874 ) Loss= 0.10881992\n",
      "Training [3875/10000] ..........3875 ) Loss= 0.11311955\n",
      "Training [3876/10000] ..........3876 ) Loss= 0.14747895\n",
      "Training [3877/10000] ..........3877 ) Loss= 0.11066623\n",
      "Training [3878/10000] ..........3878 ) Loss= 0.1133739\n",
      "Training [3879/10000] ..........3879 ) Loss= 0.19433641\n",
      "Training [3880/10000] ..........3880 ) Loss= 0.13768578\n",
      "Training [3881/10000] ..........3881 ) Loss= 0.15644331\n",
      "Training [3882/10000] ..........3882 ) Loss= 0.18594728\n",
      "Training [3883/10000] ..........3883 ) Loss= 0.12572786\n",
      "Training [3884/10000] ..........3884 ) Loss= 0.10087584\n",
      "Training [3885/10000] ..........3885 ) Loss= 0.1656258\n",
      "Training [3886/10000] ..........3886 ) Loss= 0.12758192\n",
      "Training [3887/10000] ..........3887 ) Loss= 0.074885964\n",
      "Training [3888/10000] ..........3888 ) Loss= 0.08149889\n",
      "Training [3889/10000] ..........3889 ) Loss= 0.10297392\n",
      "Training [3890/10000] ..........3890 ) Loss= 0.12963769\n",
      "Training [3891/10000] ..........3891 ) Loss= 0.12258609\n",
      "Training [3892/10000] ..........3892 ) Loss= 0.1595426\n",
      "Training [3893/10000] ..........3893 ) Loss= 0.1617806\n",
      "Training [3894/10000] ..........3894 ) Loss= 0.08829409\n",
      "Training [3895/10000] ..........3895 ) Loss= 0.12973222\n",
      "Training [3896/10000] ..........3896 ) Loss= 0.17589815\n",
      "Training [3897/10000] ..........3897 ) Loss= 0.11934134\n",
      "Training [3898/10000] ..........3898 ) Loss= 0.11766681\n",
      "Training [3899/10000] ..........3899 ) Loss= 0.1516273\n",
      "Training [3900/10000] ..........3900 ) Loss= 0.1360352\n",
      "Training [3901/10000] ..........3901 ) Loss= 0.146911\n",
      "Training [3902/10000] ..........3902 ) Loss= 0.11049322\n",
      "Training [3903/10000] ..........3903 ) Loss= 0.13295677\n",
      "Training [3904/10000] ..........3904 ) Loss= 0.09433944\n",
      "Training [3905/10000] ..........3905 ) Loss= 0.14401737\n",
      "Training [3906/10000] ..........3906 ) Loss= 0.09786998\n",
      "Training [3907/10000] ..........3907 ) Loss= 0.07730156\n",
      "Training [3908/10000] ..........3908 ) Loss= 0.32859114\n",
      "Training [3909/10000] ..........3909 ) Loss= 0.113899395\n",
      "Training [3910/10000] ..........3910 ) Loss= 0.09151622\n",
      "Training [3911/10000] ..........3911 ) Loss= 0.14871715\n",
      "Training [3912/10000] ..........3912 ) Loss= 0.15050985\n",
      "Training [3913/10000] ..........3913 ) Loss= 0.11189399\n",
      "Training [3914/10000] ..........3914 ) Loss= 0.08142816\n",
      "Training [3915/10000] ..........3915 ) Loss= 0.14423542\n",
      "Training [3916/10000] ..........3916 ) Loss= 0.14705376\n",
      "Training [3917/10000] ..........3917 ) Loss= 0.10334423\n",
      "Training [3918/10000] ..........3918 ) Loss= 0.0884583\n",
      "Training [3919/10000] ..........3919 ) Loss= 0.11825091\n",
      "Training [3920/10000] ..........3920 ) Loss= 0.2882987\n",
      "Training [3921/10000] ..........3921 ) Loss= 0.09239524\n",
      "Training [3922/10000] ..........3922 ) Loss= 0.0833164\n",
      "Training [3923/10000] ..........3923 ) Loss= 0.14904663\n",
      "Training [3924/10000] ..........3924 ) Loss= 0.084136024\n",
      "Training [3925/10000] ..........3925 ) Loss= 0.14469019\n",
      "Training [3926/10000] ..........3926 ) Loss= 0.09408451\n",
      "Training [3927/10000] ..........3927 ) Loss= 0.103043556\n",
      "Training [3928/10000] ..........3928 ) Loss= 0.26042983\n",
      "Training [3929/10000] ..........3929 ) Loss= 0.17289004\n",
      "Training [3930/10000] ..........3930 ) Loss= 0.2211063\n",
      "Training [3931/10000] ..........3931 ) Loss= 0.09195812\n",
      "Training [3932/10000] ..........3932 ) Loss= 0.13147806\n",
      "Training [3933/10000] ..........3933 ) Loss= 0.17200965\n",
      "Training [3934/10000] ..........3934 ) Loss= 0.09668118\n",
      "Training [3935/10000] ..........3935 ) Loss= 0.0715076\n",
      "Training [3936/10000] ..........3936 ) Loss= 0.111902416\n",
      "Training [3937/10000] ..........3937 ) Loss= 0.21600702\n",
      "Training [3938/10000] ..........3938 ) Loss= 0.11566843\n",
      "Training [3939/10000] ..........3939 ) Loss= 0.20920299\n",
      "Training [3940/10000] ..........3940 ) Loss= 0.18627095\n",
      "Training [3941/10000] ..........3941 ) Loss= 0.3278702\n",
      "Training [3942/10000] ..........3942 ) Loss= 0.32613757\n",
      "Training [3943/10000] ..........3943 ) Loss= 0.1964664\n",
      "Training [3944/10000] ..........3944 ) Loss= 0.12736852\n",
      "Training [3945/10000] ..........3945 ) Loss= 0.20786123\n",
      "Training [3946/10000] ..........3946 ) Loss= 0.17308086\n",
      "Training [3947/10000] ..........3947 ) Loss= 0.12703936\n",
      "Training [3948/10000] ..........3948 ) Loss= 0.17997254\n",
      "Training [3949/10000] ..........3949 ) Loss= 0.08932357\n",
      "Training [3950/10000] ..........3950 ) Loss= 0.104342505\n",
      "Training [3951/10000] ..........3951 ) Loss= 0.23428172\n",
      "Training [3952/10000] ..........3952 ) Loss= 0.11444878\n",
      "Training [3953/10000] ..........3953 ) Loss= 0.20923373\n",
      "Training [3954/10000] ..........3954 ) Loss= 0.20355858\n",
      "Training [3955/10000] ..........3955 ) Loss= 0.104913935\n",
      "Training [3956/10000] ..........3956 ) Loss= 0.10375967\n",
      "Training [3957/10000] ..........3957 ) Loss= 0.13117555\n",
      "Training [3958/10000] ..........3958 ) Loss= 0.097444646\n",
      "Training [3959/10000] ..........3959 ) Loss= 0.09742475\n",
      "Training [3960/10000] ..........3960 ) Loss= 0.23378623\n",
      "Training [3961/10000] ..........3961 ) Loss= 0.10820799\n",
      "Training [3962/10000] ..........3962 ) Loss= 0.12964146\n",
      "Training [3963/10000] ..........3963 ) Loss= 0.16724396\n",
      "Training [3964/10000] ..........3964 ) Loss= 0.12870072\n",
      "Training [3965/10000] ..........3965 ) Loss= 0.12514737\n",
      "Training [3966/10000] ..........3966 ) Loss= 0.13564469\n",
      "Training [3967/10000] ..........3967 ) Loss= 0.11813269\n",
      "Training [3968/10000] ..........3968 ) Loss= 0.15159558\n",
      "Training [3969/10000] ..........3969 ) Loss= 0.13446952\n",
      "Training [3970/10000] ..........3970 ) Loss= 0.17387159\n",
      "Training [3971/10000] ..........3971 ) Loss= 0.2007048\n",
      "Training [3972/10000] ..........3972 ) Loss= 0.16862662\n",
      "Training [3973/10000] ..........3973 ) Loss= 0.08495616\n",
      "Training [3974/10000] ..........3974 ) Loss= 0.12970512\n",
      "Training [3975/10000] ..........3975 ) Loss= 0.0802567\n",
      "Training [3976/10000] ..........3976 ) Loss= 0.20187025\n",
      "Training [3977/10000] ..........3977 ) Loss= 0.13529055\n",
      "Training [3978/10000] ..........3978 ) Loss= 0.16417263\n",
      "Training [3979/10000] ..........3979 ) Loss= 0.16394362\n",
      "Training [3980/10000] ..........3980 ) Loss= 0.15486522\n",
      "Training [3981/10000] ..........3981 ) Loss= 0.13078862\n",
      "Training [3982/10000] ..........3982 ) Loss= 0.11109112\n",
      "Training [3983/10000] ..........3983 ) Loss= 0.19282341\n",
      "Training [3984/10000] ..........3984 ) Loss= 0.06828898\n",
      "Training [3985/10000] ..........3985 ) Loss= 0.14399609\n",
      "Training [3986/10000] ..........3986 ) Loss= 0.22831509\n",
      "Training [3987/10000] ..........3987 ) Loss= 0.089535296\n",
      "Training [3988/10000] ..........3988 ) Loss= 0.15427479\n",
      "Training [3989/10000] ..........3989 ) Loss= 0.16467977\n",
      "Training [3990/10000] ..........3990 ) Loss= 0.14796312\n",
      "Training [3991/10000] ..........3991 ) Loss= 0.57065517\n",
      "Training [3992/10000] ..........3992 ) Loss= 0.078135744\n",
      "Training [3993/10000] ..........3993 ) Loss= 0.27391952\n",
      "Training [3994/10000] ..........3994 ) Loss= 0.27349868\n",
      "Training [3995/10000] ..........3995 ) Loss= 0.08647536\n",
      "Training [3996/10000] ..........3996 ) Loss= 0.12884645\n",
      "Training [3997/10000] ..........3997 ) Loss= 0.10280971\n",
      "Training [3998/10000] ..........3998 ) Loss= 0.082243614\n",
      "Training [3999/10000] ..........3999 ) Loss= 0.15570052\n",
      "Training [4000/10000] ..........4000 ) Loss= 0.1218175 - Saving Model4000.torch\n",
      "Training [4001/10000] ..........4001 ) Loss= 0.17066889\n",
      "Training [4002/10000] ..........4002 ) Loss= 0.09039234\n",
      "Training [4003/10000] ..........4003 ) Loss= 0.11813934\n",
      "Training [4004/10000] ..........4004 ) Loss= 0.106492646\n",
      "Training [4005/10000] ..........4005 ) Loss= 0.13388407\n",
      "Training [4006/10000] ..........4006 ) Loss= 0.079876505\n",
      "Training [4007/10000] ..........4007 ) Loss= 0.15068497\n",
      "Training [4008/10000] ..........4008 ) Loss= 0.3456297\n",
      "Training [4009/10000] ..........4009 ) Loss= 0.08975841\n",
      "Training [4010/10000] ..........4010 ) Loss= 0.21168277\n",
      "Training [4011/10000] ..........4011 ) Loss= 0.08817153\n",
      "Training [4012/10000] ..........4012 ) Loss= 0.093664356\n",
      "Training [4013/10000] ..........4013 ) Loss= 0.07112049\n",
      "Training [4014/10000] ..........4014 ) Loss= 0.12859607\n",
      "Training [4015/10000] ..........4015 ) Loss= 0.07556689\n",
      "Training [4016/10000] ..........4016 ) Loss= 0.10617515\n",
      "Training [4017/10000] ..........4017 ) Loss= 0.09915253\n",
      "Training [4018/10000] ..........4018 ) Loss= 0.26013333\n",
      "Training [4019/10000] ..........4019 ) Loss= 0.10779219\n",
      "Training [4020/10000] ..........4020 ) Loss= 0.11817093\n",
      "Training [4021/10000] ..........4021 ) Loss= 0.14428838\n",
      "Training [4022/10000] ..........4022 ) Loss= 0.19523488\n",
      "Training [4023/10000] ..........4023 ) Loss= 0.13755062\n",
      "Training [4024/10000] ..........4024 ) Loss= 0.08886208\n",
      "Training [4025/10000] ..........4025 ) Loss= 0.16108292\n",
      "Training [4026/10000] ..........4026 ) Loss= 0.12550251\n",
      "Training [4027/10000] ..........4027 ) Loss= 0.08246756\n",
      "Training [4028/10000] ..........4028 ) Loss= 0.100642614\n",
      "Training [4029/10000] ..........4029 ) Loss= 0.21697187\n",
      "Training [4030/10000] ..........4030 ) Loss= 0.19095875\n",
      "Training [4031/10000] ..........4031 ) Loss= 0.109869845\n",
      "Training [4032/10000] ..........4032 ) Loss= 0.11019257\n",
      "Training [4033/10000] ..........4033 ) Loss= 0.12027523\n",
      "Training [4034/10000] ..........4034 ) Loss= 0.078191355\n",
      "Training [4035/10000] ..........4035 ) Loss= 0.095353685\n",
      "Training [4036/10000] ..........4036 ) Loss= 0.13950802\n",
      "Training [4037/10000] ..........4037 ) Loss= 0.1022335\n",
      "Training [4038/10000] ..........4038 ) Loss= 0.21737854\n",
      "Training [4039/10000] ..........4039 ) Loss= 0.12579578\n",
      "Training [4040/10000] ..........4040 ) Loss= 0.17691483\n",
      "Training [4041/10000] ..........4041 ) Loss= 0.17209342\n",
      "Training [4042/10000] ..........4042 ) Loss= 0.13754411\n",
      "Training [4043/10000] ..........4043 ) Loss= 0.20566171\n",
      "Training [4044/10000] ..........4044 ) Loss= 0.123297915\n",
      "Training [4045/10000] ..........4045 ) Loss= 0.12829813\n",
      "Training [4046/10000] ..........4046 ) Loss= 0.23365913\n",
      "Training [4047/10000] ..........4047 ) Loss= 0.14630051\n",
      "Training [4048/10000] ..........4048 ) Loss= 0.11963426\n",
      "Training [4049/10000] ..........4049 ) Loss= 0.14748147\n",
      "Training [4050/10000] ..........4050 ) Loss= 0.15546642\n",
      "Training [4051/10000] ..........4051 ) Loss= 0.10277216\n",
      "Training [4052/10000] ..........4052 ) Loss= 0.090016544\n",
      "Training [4053/10000] ..........4053 ) Loss= 0.11323625\n",
      "Training [4054/10000] ..........4054 ) Loss= 0.09140327\n",
      "Training [4055/10000] ..........4055 ) Loss= 0.13715686\n",
      "Training [4056/10000] ..........4056 ) Loss= 0.14585625\n",
      "Training [4057/10000] ..........4057 ) Loss= 0.19660059\n",
      "Training [4058/10000] ..........4058 ) Loss= 0.122074686\n",
      "Training [4059/10000] ..........4059 ) Loss= 0.092633024\n",
      "Training [4060/10000] ..........4060 ) Loss= 0.24663311\n",
      "Training [4061/10000] ..........4061 ) Loss= 0.13009928\n",
      "Training [4062/10000] ..........4062 ) Loss= 0.16136931\n",
      "Training [4063/10000] ..........4063 ) Loss= 0.08198386\n",
      "Training [4064/10000] ..........4064 ) Loss= 0.123191826\n",
      "Training [4065/10000] ..........4065 ) Loss= 0.18465985\n",
      "Training [4066/10000] ..........4066 ) Loss= 0.11617892\n",
      "Training [4067/10000] ..........4067 ) Loss= 0.15412734\n",
      "Training [4068/10000] ..........4068 ) Loss= 0.10092026\n",
      "Training [4069/10000] ..........4069 ) Loss= 0.0690689\n",
      "Training [4070/10000] ..........4070 ) Loss= 0.15578389\n",
      "Training [4071/10000] ..........4071 ) Loss= 0.09112592\n",
      "Training [4072/10000] ..........4072 ) Loss= 0.13835172\n",
      "Training [4073/10000] ..........4073 ) Loss= 0.14119203\n",
      "Training [4074/10000] ..........4074 ) Loss= 0.15965886\n",
      "Training [4075/10000] ..........4075 ) Loss= 0.120383896\n",
      "Training [4076/10000] ..........4076 ) Loss= 0.0647555\n",
      "Training [4077/10000] ..........4077 ) Loss= 0.07971354\n",
      "Training [4078/10000] ..........4078 ) Loss= 0.10879247\n",
      "Training [4079/10000] ..........4079 ) Loss= 0.091495864\n",
      "Training [4080/10000] ..........4080 ) Loss= 0.13039377\n",
      "Training [4081/10000] ..........4081 ) Loss= 0.626316\n",
      "Training [4082/10000] ..........4082 ) Loss= 0.19475842\n",
      "Training [4083/10000] ..........4083 ) Loss= 0.15309498\n",
      "Training [4084/10000] ..........4084 ) Loss= 0.13338034\n",
      "Training [4085/10000] ..........4085 ) Loss= 0.07410274\n",
      "Training [4086/10000] ..........4086 ) Loss= 0.13991368\n",
      "Training [4087/10000] ..........4087 ) Loss= 0.16074063\n",
      "Training [4088/10000] ..........4088 ) Loss= 0.2049223\n",
      "Training [4089/10000] ..........4089 ) Loss= 0.11114975\n",
      "Training [4090/10000] ..........4090 ) Loss= 0.16998132\n",
      "Training [4091/10000] ..........4091 ) Loss= 0.11645381\n",
      "Training [4092/10000] ..........4092 ) Loss= 0.19196276\n",
      "Training [4093/10000] ..........4093 ) Loss= 0.14615919\n",
      "Training [4094/10000] ..........4094 ) Loss= 0.10356228\n",
      "Training [4095/10000] ..........4095 ) Loss= 0.08959471\n",
      "Training [4096/10000] ..........4096 ) Loss= 0.098960936\n",
      "Training [4097/10000] ..........4097 ) Loss= 0.10418224\n",
      "Training [4098/10000] ..........4098 ) Loss= 0.20598662\n",
      "Training [4099/10000] ..........4099 ) Loss= 0.1089168\n",
      "Training [4100/10000] ..........4100 ) Loss= 0.092431895\n",
      "Training [4101/10000] ..........4101 ) Loss= 0.10776348\n",
      "Training [4102/10000] ..........4102 ) Loss= 0.12425965\n",
      "Training [4103/10000] ..........4103 ) Loss= 0.09102826\n",
      "Training [4104/10000] ..........4104 ) Loss= 0.09554797\n",
      "Training [4105/10000] ..........4105 ) Loss= 0.10654135\n",
      "Training [4106/10000] ..........4106 ) Loss= 0.25068012\n",
      "Training [4107/10000] ..........4107 ) Loss= 0.1282744\n",
      "Training [4108/10000] ..........4108 ) Loss= 0.1313686\n",
      "Training [4109/10000] ..........4109 ) Loss= 0.14171617\n",
      "Training [4110/10000] ..........4110 ) Loss= 0.12503898\n",
      "Training [4111/10000] ..........4111 ) Loss= 0.08651095\n",
      "Training [4112/10000] ..........4112 ) Loss= 0.13189796\n",
      "Training [4113/10000] ..........4113 ) Loss= 0.09100559\n",
      "Training [4114/10000] ..........4114 ) Loss= 0.21622144\n",
      "Training [4115/10000] ..........4115 ) Loss= 0.1577574\n",
      "Training [4116/10000] ..........4116 ) Loss= 0.115655206\n",
      "Training [4117/10000] ..........4117 ) Loss= 0.19084501\n",
      "Training [4118/10000] ..........4118 ) Loss= 0.11027051\n",
      "Training [4119/10000] ..........4119 ) Loss= 0.07587151\n",
      "Training [4120/10000] ..........4120 ) Loss= 0.32431218\n",
      "Training [4121/10000] ..........4121 ) Loss= 0.14741133\n",
      "Training [4122/10000] ..........4122 ) Loss= 0.1395058\n",
      "Training [4123/10000] ..........4123 ) Loss= 0.18635371\n",
      "Training [4124/10000] ..........4124 ) Loss= 0.1945383\n",
      "Training [4125/10000] ..........4125 ) Loss= 0.08475043\n",
      "Training [4126/10000] ..........4126 ) Loss= 0.11986332\n",
      "Training [4127/10000] ..........4127 ) Loss= 0.12426633\n",
      "Training [4128/10000] ..........4128 ) Loss= 0.09070052\n",
      "Training [4129/10000] ..........4129 ) Loss= 0.09881369\n",
      "Training [4130/10000] ..........4130 ) Loss= 0.17860043\n",
      "Training [4131/10000] ..........4131 ) Loss= 0.09642058\n",
      "Training [4132/10000] ..........4132 ) Loss= 0.16727722\n",
      "Training [4133/10000] ..........4133 ) Loss= 0.20294078\n",
      "Training [4134/10000] ..........4134 ) Loss= 0.108487956\n",
      "Training [4135/10000] ..........4135 ) Loss= 0.11474371\n",
      "Training [4136/10000] ..........4136 ) Loss= 0.18710904\n",
      "Training [4137/10000] ..........4137 ) Loss= 0.16805476\n",
      "Training [4138/10000] ..........4138 ) Loss= 0.12129957\n",
      "Training [4139/10000] ..........4139 ) Loss= 0.141842\n",
      "Training [4140/10000] ..........4140 ) Loss= 0.22205046\n",
      "Training [4141/10000] ..........4141 ) Loss= 0.11084094\n",
      "Training [4142/10000] ..........4142 ) Loss= 0.24028307\n",
      "Training [4143/10000] ..........4143 ) Loss= 0.13473065\n",
      "Training [4144/10000] ..........4144 ) Loss= 0.14594091\n",
      "Training [4145/10000] ..........4145 ) Loss= 0.091074646\n",
      "Training [4146/10000] ..........4146 ) Loss= 0.21763079\n",
      "Training [4147/10000] ..........4147 ) Loss= 0.1499935\n",
      "Training [4148/10000] ..........4148 ) Loss= 0.09527336\n",
      "Training [4149/10000] ..........4149 ) Loss= 0.21811403\n",
      "Training [4150/10000] ..........4150 ) Loss= 0.12983938\n",
      "Training [4151/10000] ..........4151 ) Loss= 0.08467079\n",
      "Training [4152/10000] ..........4152 ) Loss= 0.0933083\n",
      "Training [4153/10000] ..........4153 ) Loss= 0.16983683\n",
      "Training [4154/10000] ..........4154 ) Loss= 0.06731124\n",
      "Training [4155/10000] ..........4155 ) Loss= 0.11980485\n",
      "Training [4156/10000] ..........4156 ) Loss= 0.12630531\n",
      "Training [4157/10000] ..........4157 ) Loss= 0.111444324\n",
      "Training [4158/10000] ..........4158 ) Loss= 0.0974416\n",
      "Training [4159/10000] ..........4159 ) Loss= 0.26581943\n",
      "Training [4160/10000] ..........4160 ) Loss= 0.10893627\n",
      "Training [4161/10000] ..........4161 ) Loss= 0.086424984\n",
      "Training [4162/10000] ..........4162 ) Loss= 0.10453653\n",
      "Training [4163/10000] ..........4163 ) Loss= 0.20447375\n",
      "Training [4164/10000] ..........4164 ) Loss= 0.08007128\n",
      "Training [4165/10000] ..........4165 ) Loss= 0.09468081\n",
      "Training [4166/10000] ..........4166 ) Loss= 0.10283691\n",
      "Training [4167/10000] ..........4167 ) Loss= 0.29387248\n",
      "Training [4168/10000] ..........4168 ) Loss= 0.11298095\n",
      "Training [4169/10000] ..........4169 ) Loss= 0.1512404\n",
      "Training [4170/10000] ..........4170 ) Loss= 0.12657475\n",
      "Training [4171/10000] ..........4171 ) Loss= 0.20398283\n",
      "Training [4172/10000] ..........4172 ) Loss= 0.15352383\n",
      "Training [4173/10000] ..........4173 ) Loss= 0.124350585\n",
      "Training [4174/10000] ..........4174 ) Loss= 0.1255119\n",
      "Training [4175/10000] ..........4175 ) Loss= 0.19028232\n",
      "Training [4176/10000] ..........4176 ) Loss= 0.11682271\n",
      "Training [4177/10000] ..........4177 ) Loss= 0.072946675\n",
      "Training [4178/10000] ..........4178 ) Loss= 0.10499838\n",
      "Training [4179/10000] ..........4179 ) Loss= 0.102153376\n",
      "Training [4180/10000] ..........4180 ) Loss= 0.12946263\n",
      "Training [4181/10000] ..........4181 ) Loss= 0.21666911\n",
      "Training [4182/10000] ..........4182 ) Loss= 0.20301238\n",
      "Training [4183/10000] ..........4183 ) Loss= 0.17635573\n",
      "Training [4184/10000] ..........4184 ) Loss= 0.11279025\n",
      "Training [4185/10000] ..........4185 ) Loss= 0.12540579\n",
      "Training [4186/10000] ..........4186 ) Loss= 0.09534211\n",
      "Training [4187/10000] ..........4187 ) Loss= 0.16844746\n",
      "Training [4188/10000] ..........4188 ) Loss= 0.18781137\n",
      "Training [4189/10000] ..........4189 ) Loss= 0.21600407\n",
      "Training [4190/10000] ..........4190 ) Loss= 0.06943998\n",
      "Training [4191/10000] ..........4191 ) Loss= 0.09736413\n",
      "Training [4192/10000] ..........4192 ) Loss= 0.13502584\n",
      "Training [4193/10000] ..........4193 ) Loss= 0.08277292\n",
      "Training [4194/10000] ..........4194 ) Loss= 0.27652362\n",
      "Training [4195/10000] ..........4195 ) Loss= 0.20650609\n",
      "Training [4196/10000] ..........4196 ) Loss= 0.08810448\n",
      "Training [4197/10000] ..........4197 ) Loss= 0.08483484\n",
      "Training [4198/10000] ..........4198 ) Loss= 0.08060538\n",
      "Training [4199/10000] ..........4199 ) Loss= 0.16514988\n",
      "Training [4200/10000] ..........4200 ) Loss= 0.11221608\n",
      "Training [4201/10000] ..........4201 ) Loss= 0.16489258\n",
      "Training [4202/10000] ..........4202 ) Loss= 0.08570247\n",
      "Training [4203/10000] ..........4203 ) Loss= 0.1420462\n",
      "Training [4204/10000] ..........4204 ) Loss= 0.13316375\n",
      "Training [4205/10000] ..........4205 ) Loss= 0.12136323\n",
      "Training [4206/10000] ..........4206 ) Loss= 0.12661473\n",
      "Training [4207/10000] ..........4207 ) Loss= 0.1966773\n",
      "Training [4208/10000] ..........4208 ) Loss= 0.092564546\n",
      "Training [4209/10000] ..........4209 ) Loss= 0.1387378\n",
      "Training [4210/10000] ..........4210 ) Loss= 0.15020935\n",
      "Training [4211/10000] ..........4211 ) Loss= 0.10073025\n",
      "Training [4212/10000] ..........4212 ) Loss= 0.08753031\n",
      "Training [4213/10000] ..........4213 ) Loss= 0.18864577\n",
      "Training [4214/10000] ..........4214 ) Loss= 0.15995511\n",
      "Training [4215/10000] ..........4215 ) Loss= 0.100399286\n",
      "Training [4216/10000] ..........4216 ) Loss= 0.10249931\n",
      "Training [4217/10000] ..........4217 ) Loss= 0.23265116\n",
      "Training [4218/10000] ..........4218 ) Loss= 0.119024545\n",
      "Training [4219/10000] ..........4219 ) Loss= 0.1537982\n",
      "Training [4220/10000] ..........4220 ) Loss= 0.111727685\n",
      "Training [4221/10000] ..........4221 ) Loss= 0.17461649\n",
      "Training [4222/10000] ..........4222 ) Loss= 0.21161044\n",
      "Training [4223/10000] ..........4223 ) Loss= 0.11935929\n",
      "Training [4224/10000] ..........4224 ) Loss= 0.15031135\n",
      "Training [4225/10000] ..........4225 ) Loss= 0.080308944\n",
      "Training [4226/10000] ..........4226 ) Loss= 0.10170106\n",
      "Training [4227/10000] ..........4227 ) Loss= 0.134232\n",
      "Training [4228/10000] ..........4228 ) Loss= 0.15059713\n",
      "Training [4229/10000] ..........4229 ) Loss= 0.09854879\n",
      "Training [4230/10000] ..........4230 ) Loss= 0.1401622\n",
      "Training [4231/10000] ..........4231 ) Loss= 0.23550528\n",
      "Training [4232/10000] ..........4232 ) Loss= 0.15180416\n",
      "Training [4233/10000] ..........4233 ) Loss= 0.10481392\n",
      "Training [4234/10000] ..........4234 ) Loss= 0.0875192\n",
      "Training [4235/10000] ..........4235 ) Loss= 0.12150271\n",
      "Training [4236/10000] ..........4236 ) Loss= 0.12964527\n",
      "Training [4237/10000] ..........4237 ) Loss= 0.15681306\n",
      "Training [4238/10000] ..........4238 ) Loss= 0.10109073\n",
      "Training [4239/10000] ..........4239 ) Loss= 0.08411433\n",
      "Training [4240/10000] ..........4240 ) Loss= 0.1054308\n",
      "Training [4241/10000] ..........4241 ) Loss= 0.09527497\n",
      "Training [4242/10000] ..........4242 ) Loss= 0.16154651\n",
      "Training [4243/10000] ..........4243 ) Loss= 0.16849382\n",
      "Training [4244/10000] ..........4244 ) Loss= 0.10004442\n",
      "Training [4245/10000] ..........4245 ) Loss= 0.14467038\n",
      "Training [4246/10000] ..........4246 ) Loss= 0.1128278\n",
      "Training [4247/10000] ..........4247 ) Loss= 0.32718894\n",
      "Training [4248/10000] ..........4248 ) Loss= 0.08634989\n",
      "Training [4249/10000] ..........4249 ) Loss= 0.09493838\n",
      "Training [4250/10000] ..........4250 ) Loss= 0.10720397\n",
      "Training [4251/10000] ..........4251 ) Loss= 0.07612034\n",
      "Training [4252/10000] ..........4252 ) Loss= 0.22309265\n",
      "Training [4253/10000] ..........4253 ) Loss= 0.10766391\n",
      "Training [4254/10000] ..........4254 ) Loss= 0.112030044\n",
      "Training [4255/10000] ..........4255 ) Loss= 0.09466136\n",
      "Training [4256/10000] ..........4256 ) Loss= 0.08073611\n",
      "Training [4257/10000] ..........4257 ) Loss= 0.073214926\n",
      "Training [4258/10000] ..........4258 ) Loss= 0.1284272\n",
      "Training [4259/10000] ..........4259 ) Loss= 0.11138618\n",
      "Training [4260/10000] ..........4260 ) Loss= 0.104209445\n",
      "Training [4261/10000] ..........4261 ) Loss= 0.14701116\n",
      "Training [4262/10000] ..........4262 ) Loss= 0.09899282\n",
      "Training [4263/10000] ..........4263 ) Loss= 0.07315937\n",
      "Training [4264/10000] ..........4264 ) Loss= 0.097205564\n",
      "Training [4265/10000] ..........4265 ) Loss= 0.11342059\n",
      "Training [4266/10000] ..........4266 ) Loss= 0.08192294\n",
      "Training [4267/10000] ..........4267 ) Loss= 0.107180715\n",
      "Training [4268/10000] ..........4268 ) Loss= 0.0652075\n",
      "Training [4269/10000] ..........4269 ) Loss= 0.14962207\n",
      "Training [4270/10000] ..........4270 ) Loss= 0.09650664\n",
      "Training [4271/10000] ..........4271 ) Loss= 0.101999454\n",
      "Training [4272/10000] ..........4272 ) Loss= 0.1407326\n",
      "Training [4273/10000] ..........4273 ) Loss= 0.3048204\n",
      "Training [4274/10000] ..........4274 ) Loss= 0.36937383\n",
      "Training [4275/10000] ..........4275 ) Loss= 0.12139962\n",
      "Training [4276/10000] ..........4276 ) Loss= 0.14168034\n",
      "Training [4277/10000] ..........4277 ) Loss= 0.15299492\n",
      "Training [4278/10000] ..........4278 ) Loss= 0.07399675\n",
      "Training [4279/10000] ..........4279 ) Loss= 0.0914329\n",
      "Training [4280/10000] ..........4280 ) Loss= 0.08738955\n",
      "Training [4281/10000] ..........4281 ) Loss= 0.3525979\n",
      "Training [4282/10000] ..........4282 ) Loss= 0.08810169\n",
      "Training [4283/10000] ..........4283 ) Loss= 0.09777314\n",
      "Training [4284/10000] ..........4284 ) Loss= 0.104737096\n",
      "Training [4285/10000] ..........4285 ) Loss= 0.35043\n",
      "Training [4286/10000] ..........4286 ) Loss= 0.07549421\n",
      "Training [4287/10000] ..........4287 ) Loss= 0.09737246\n",
      "Training [4288/10000] ..........4288 ) Loss= 0.3325604\n",
      "Training [4289/10000] ..........4289 ) Loss= 0.1706021\n",
      "Training [4290/10000] ..........4290 ) Loss= 0.09112294\n",
      "Training [4291/10000] ..........4291 ) Loss= 0.09095556\n",
      "Training [4292/10000] ..........4292 ) Loss= 0.10427545\n",
      "Training [4293/10000] ..........4293 ) Loss= 0.19723253\n",
      "Training [4294/10000] ..........4294 ) Loss= 0.27923894\n",
      "Training [4295/10000] ..........4295 ) Loss= 0.11247616\n",
      "Training [4296/10000] ..........4296 ) Loss= 0.11195593\n",
      "Training [4297/10000] ..........4297 ) Loss= 0.10501127\n",
      "Training [4298/10000] ..........4298 ) Loss= 0.13896023\n",
      "Training [4299/10000] ..........4299 ) Loss= 0.09869598\n",
      "Training [4300/10000] ..........4300 ) Loss= 0.07511268\n",
      "Training [4301/10000] ..........4301 ) Loss= 0.08765509\n",
      "Training [4302/10000] ..........4302 ) Loss= 0.16894555\n",
      "Training [4303/10000] ..........4303 ) Loss= 0.13980453\n",
      "Training [4304/10000] ..........4304 ) Loss= 0.15634628\n",
      "Training [4305/10000] ..........4305 ) Loss= 0.09468139\n",
      "Training [4306/10000] ..........4306 ) Loss= 0.09327518\n",
      "Training [4307/10000] ..........4307 ) Loss= 0.17556764\n",
      "Training [4308/10000] ..........4308 ) Loss= 0.13083844\n",
      "Training [4309/10000] ..........4309 ) Loss= 0.16138232\n",
      "Training [4310/10000] ..........4310 ) Loss= 0.120055646\n",
      "Training [4311/10000] ..........4311 ) Loss= 0.1802636\n",
      "Training [4312/10000] ..........4312 ) Loss= 0.17813496\n",
      "Training [4313/10000] ..........4313 ) Loss= 0.10558335\n",
      "Training [4314/10000] ..........4314 ) Loss= 0.10604344\n",
      "Training [4315/10000] ..........4315 ) Loss= 0.2233275\n",
      "Training [4316/10000] ..........4316 ) Loss= 0.19461231\n",
      "Training [4317/10000] ..........4317 ) Loss= 0.087316975\n",
      "Training [4318/10000] ..........4318 ) Loss= 0.096695185\n",
      "Training [4319/10000] ..........4319 ) Loss= 0.13329351\n",
      "Training [4320/10000] ..........4320 ) Loss= 0.109213494\n",
      "Training [4321/10000] ..........4321 ) Loss= 0.20799613\n",
      "Training [4322/10000] ..........4322 ) Loss= 0.6199756\n",
      "Training [4323/10000] ..........4323 ) Loss= 0.11734383\n",
      "Training [4324/10000] ..........4324 ) Loss= 0.195704\n",
      "Training [4325/10000] ..........4325 ) Loss= 0.15501945\n",
      "Training [4326/10000] ..........4326 ) Loss= 0.14784676\n",
      "Training [4327/10000] ..........4327 ) Loss= 0.2190869\n",
      "Training [4328/10000] ..........4328 ) Loss= 0.09016235\n",
      "Training [4329/10000] ..........4329 ) Loss= 0.15381628\n",
      "Training [4330/10000] ..........4330 ) Loss= 0.13646096\n",
      "Training [4331/10000] ..........4331 ) Loss= 0.2608049\n",
      "Training [4332/10000] ..........4332 ) Loss= 0.093086764\n",
      "Training [4333/10000] ..........4333 ) Loss= 0.090160765\n",
      "Training [4334/10000] ..........4334 ) Loss= 0.13894334\n",
      "Training [4335/10000] ..........4335 ) Loss= 0.10846121\n",
      "Training [4336/10000] ..........4336 ) Loss= 0.16428527\n",
      "Training [4337/10000] ..........4337 ) Loss= 0.12532346\n",
      "Training [4338/10000] ..........4338 ) Loss= 0.086633645\n",
      "Training [4339/10000] ..........4339 ) Loss= 0.08956807\n",
      "Training [4340/10000] ..........4340 ) Loss= 0.4193587\n",
      "Training [4341/10000] ..........4341 ) Loss= 0.10091198\n",
      "Training [4342/10000] ..........4342 ) Loss= 0.14126109\n",
      "Training [4343/10000] ..........4343 ) Loss= 0.1991442\n",
      "Training [4344/10000] ..........4344 ) Loss= 0.1014063\n",
      "Training [4345/10000] ..........4345 ) Loss= 0.08495958\n",
      "Training [4346/10000] ..........4346 ) Loss= 0.10003371\n",
      "Training [4347/10000] ..........4347 ) Loss= 0.46254522\n",
      "Training [4348/10000] ..........4348 ) Loss= 0.13499421\n",
      "Training [4349/10000] ..........4349 ) Loss= 0.13986377\n",
      "Training [4350/10000] ..........4350 ) Loss= 0.074000664\n",
      "Training [4351/10000] ..........4351 ) Loss= 0.08600808\n",
      "Training [4352/10000] ..........4352 ) Loss= 0.18383123\n",
      "Training [4353/10000] ..........4353 ) Loss= 0.12585011\n",
      "Training [4354/10000] ..........4354 ) Loss= 0.14464714\n",
      "Training [4355/10000] ..........4355 ) Loss= 0.13517147\n",
      "Training [4356/10000] ..........4356 ) Loss= 0.119099185\n",
      "Training [4357/10000] ..........4357 ) Loss= 0.06954459\n",
      "Training [4358/10000] ..........4358 ) Loss= 0.102605134\n",
      "Training [4359/10000] ..........4359 ) Loss= 0.121111065\n",
      "Training [4360/10000] ..........4360 ) Loss= 0.102146916\n",
      "Training [4361/10000] ..........4361 ) Loss= 0.0890451\n",
      "Training [4362/10000] ..........4362 ) Loss= 0.25015694\n",
      "Training [4363/10000] ..........4363 ) Loss= 0.19485922\n",
      "Training [4364/10000] ..........4364 ) Loss= 0.11243632\n",
      "Training [4365/10000] ..........4365 ) Loss= 0.09718185\n",
      "Training [4366/10000] ..........4366 ) Loss= 0.17136444\n",
      "Training [4367/10000] ..........4367 ) Loss= 0.10479021\n",
      "Training [4368/10000] ..........4368 ) Loss= 0.16190933\n",
      "Training [4369/10000] ..........4369 ) Loss= 0.06829388\n",
      "Training [4370/10000] ..........4370 ) Loss= 0.067532055\n",
      "Training [4371/10000] ..........4371 ) Loss= 0.21111555\n",
      "Training [4372/10000] ..........4372 ) Loss= 0.12152469\n",
      "Training [4373/10000] ..........4373 ) Loss= 0.112601735\n",
      "Training [4374/10000] ..........4374 ) Loss= 0.09496323\n",
      "Training [4375/10000] ..........4375 ) Loss= 0.12795042\n",
      "Training [4376/10000] ..........4376 ) Loss= 0.20974834\n",
      "Training [4377/10000] ..........4377 ) Loss= 0.1435115\n",
      "Training [4378/10000] ..........4378 ) Loss= 0.09146869\n",
      "Training [4379/10000] ..........4379 ) Loss= 0.10084161\n",
      "Training [4380/10000] ..........4380 ) Loss= 0.11960382\n",
      "Training [4381/10000] ..........4381 ) Loss= 0.093449146\n",
      "Training [4382/10000] ..........4382 ) Loss= 0.14078426\n",
      "Training [4383/10000] ..........4383 ) Loss= 0.09569186\n",
      "Training [4384/10000] ..........4384 ) Loss= 0.0969562\n",
      "Training [4385/10000] ..........4385 ) Loss= 0.35781237\n",
      "Training [4386/10000] ..........4386 ) Loss= 0.11463801\n",
      "Training [4387/10000] ..........4387 ) Loss= 0.1970556\n",
      "Training [4388/10000] ..........4388 ) Loss= 0.15193956\n",
      "Training [4389/10000] ..........4389 ) Loss= 0.0855149\n",
      "Training [4390/10000] ..........4390 ) Loss= 0.1271979\n",
      "Training [4391/10000] ..........4391 ) Loss= 0.19694781\n",
      "Training [4392/10000] ..........4392 ) Loss= 0.101305224\n",
      "Training [4393/10000] ..........4393 ) Loss= 0.16260931\n",
      "Training [4394/10000] ..........4394 ) Loss= 0.08330627\n",
      "Training [4395/10000] ..........4395 ) Loss= 0.14233048\n",
      "Training [4396/10000] ..........4396 ) Loss= 0.2200146\n",
      "Training [4397/10000] ..........4397 ) Loss= 0.1528237\n",
      "Training [4398/10000] ..........4398 ) Loss= 0.12360006\n",
      "Training [4399/10000] ..........4399 ) Loss= 0.16226865\n",
      "Training [4400/10000] ..........4400 ) Loss= 0.088684\n",
      "Training [4401/10000] ..........4401 ) Loss= 0.09242914\n",
      "Training [4402/10000] ..........4402 ) Loss= 0.123143956\n",
      "Training [4403/10000] ..........4403 ) Loss= 0.22080088\n",
      "Training [4404/10000] ..........4404 ) Loss= 0.10842607\n",
      "Training [4405/10000] ..........4405 ) Loss= 0.16570032\n",
      "Training [4406/10000] ..........4406 ) Loss= 0.11472834\n",
      "Training [4407/10000] ..........4407 ) Loss= 0.078358114\n",
      "Training [4408/10000] ..........4408 ) Loss= 0.1360896\n",
      "Training [4409/10000] ..........4409 ) Loss= 0.14376178\n",
      "Training [4410/10000] ..........4410 ) Loss= 0.08918469\n",
      "Training [4411/10000] ..........4411 ) Loss= 0.13655154\n",
      "Training [4412/10000] ..........4412 ) Loss= 0.20802763\n",
      "Training [4413/10000] ..........4413 ) Loss= 0.08715823\n",
      "Training [4414/10000] ..........4414 ) Loss= 0.111597285\n",
      "Training [4415/10000] ..........4415 ) Loss= 0.27746385\n",
      "Training [4416/10000] ..........4416 ) Loss= 0.09518318\n",
      "Training [4417/10000] ..........4417 ) Loss= 0.13205287\n",
      "Training [4418/10000] ..........4418 ) Loss= 0.116864756\n",
      "Training [4419/10000] ..........4419 ) Loss= 0.17024639\n",
      "Training [4420/10000] ..........4420 ) Loss= 0.1038913\n",
      "Training [4421/10000] ..........4421 ) Loss= 0.07375528\n",
      "Training [4422/10000] ..........4422 ) Loss= 0.08362979\n",
      "Training [4423/10000] ..........4423 ) Loss= 0.16465265\n",
      "Training [4424/10000] ..........4424 ) Loss= 0.2724623\n",
      "Training [4425/10000] ..........4425 ) Loss= 0.08829899\n",
      "Training [4426/10000] ..........4426 ) Loss= 0.06682259\n",
      "Training [4427/10000] ..........4427 ) Loss= 0.15169564\n",
      "Training [4428/10000] ..........4428 ) Loss= 0.08844249\n",
      "Training [4429/10000] ..........4429 ) Loss= 0.0795986\n",
      "Training [4430/10000] ..........4430 ) Loss= 0.10269563\n",
      "Training [4431/10000] ..........4431 ) Loss= 0.14507349\n",
      "Training [4432/10000] ..........4432 ) Loss= 0.11980173\n",
      "Training [4433/10000] ..........4433 ) Loss= 0.15663981\n",
      "Training [4434/10000] ..........4434 ) Loss= 0.10011862\n",
      "Training [4435/10000] ..........4435 ) Loss= 0.12768112\n",
      "Training [4436/10000] ..........4436 ) Loss= 0.14556785\n",
      "Training [4437/10000] ..........4437 ) Loss= 0.08576957\n",
      "Training [4438/10000] ..........4438 ) Loss= 0.14878042\n",
      "Training [4439/10000] ..........4439 ) Loss= 0.12354285\n",
      "Training [4440/10000] ..........4440 ) Loss= 0.11994373\n",
      "Training [4441/10000] ..........4441 ) Loss= 0.13545135\n",
      "Training [4442/10000] ..........4442 ) Loss= 0.13294202\n",
      "Training [4443/10000] ..........4443 ) Loss= 0.18124591\n",
      "Training [4444/10000] ..........4444 ) Loss= 0.15946268\n",
      "Training [4445/10000] ..........4445 ) Loss= 0.085324615\n",
      "Training [4446/10000] ..........4446 ) Loss= 0.11580029\n",
      "Training [4447/10000] ..........4447 ) Loss= 0.18535641\n",
      "Training [4448/10000] ..........4448 ) Loss= 0.07374925\n",
      "Training [4449/10000] ..........4449 ) Loss= 0.12821908\n",
      "Training [4450/10000] ..........4450 ) Loss= 0.16696747\n",
      "Training [4451/10000] ..........4451 ) Loss= 0.063105226\n",
      "Training [4452/10000] ..........4452 ) Loss= 0.1394229\n",
      "Training [4453/10000] ..........4453 ) Loss= 0.12333016\n",
      "Training [4454/10000] ..........4454 ) Loss= 0.15318628\n",
      "Training [4455/10000] ..........4455 ) Loss= 0.07841174\n",
      "Training [4456/10000] ..........4456 ) Loss= 0.225789\n",
      "Training [4457/10000] ..........4457 ) Loss= 0.10135731\n",
      "Training [4458/10000] ..........4458 ) Loss= 0.14425379\n",
      "Training [4459/10000] ..........4459 ) Loss= 0.10313161\n",
      "Training [4460/10000] ..........4460 ) Loss= 0.20553026\n",
      "Training [4461/10000] ..........4461 ) Loss= 0.10401401\n",
      "Training [4462/10000] ..........4462 ) Loss= 0.091426976\n",
      "Training [4463/10000] ..........4463 ) Loss= 0.17116362\n",
      "Training [4464/10000] ..........4464 ) Loss= 0.07145762\n",
      "Training [4465/10000] ..........4465 ) Loss= 0.14909655\n",
      "Training [4466/10000] ..........4466 ) Loss= 0.14643396\n",
      "Training [4467/10000] ..........4467 ) Loss= 0.14978366\n",
      "Training [4468/10000] ..........4468 ) Loss= 0.13755019\n",
      "Training [4469/10000] ..........4469 ) Loss= 0.14706622\n",
      "Training [4470/10000] ..........4470 ) Loss= 0.14248545\n",
      "Training [4471/10000] ..........4471 ) Loss= 0.08763415\n",
      "Training [4472/10000] ..........4472 ) Loss= 0.0905635\n",
      "Training [4473/10000] ..........4473 ) Loss= 0.15066276\n",
      "Training [4474/10000] ..........4474 ) Loss= 0.13195048\n",
      "Training [4475/10000] ..........4475 ) Loss= 0.122782014\n",
      "Training [4476/10000] ..........4476 ) Loss= 0.08157901\n",
      "Training [4477/10000] ..........4477 ) Loss= 0.09608247\n",
      "Training [4478/10000] ..........4478 ) Loss= 0.16708967\n",
      "Training [4479/10000] ..........4479 ) Loss= 0.11277066\n",
      "Training [4480/10000] ..........4480 ) Loss= 0.07804686\n",
      "Training [4481/10000] ..........4481 ) Loss= 0.0830286\n",
      "Training [4482/10000] ..........4482 ) Loss= 0.13988598\n",
      "Training [4483/10000] ..........4483 ) Loss= 0.19417332\n",
      "Training [4484/10000] ..........4484 ) Loss= 0.1875932\n",
      "Training [4485/10000] ..........4485 ) Loss= 0.13285255\n",
      "Training [4486/10000] ..........4486 ) Loss= 0.1533333\n",
      "Training [4487/10000] ..........4487 ) Loss= 0.13064894\n",
      "Training [4488/10000] ..........4488 ) Loss= 0.13840628\n",
      "Training [4489/10000] ..........4489 ) Loss= 0.12132996\n",
      "Training [4490/10000] ..........4490 ) Loss= 0.29524118\n",
      "Training [4491/10000] ..........4491 ) Loss= 0.120019\n",
      "Training [4492/10000] ..........4492 ) Loss= 0.12853284\n",
      "Training [4493/10000] ..........4493 ) Loss= 0.09428969\n",
      "Training [4494/10000] ..........4494 ) Loss= 0.15815866\n",
      "Training [4495/10000] ..........4495 ) Loss= 0.22311553\n",
      "Training [4496/10000] ..........4496 ) Loss= 0.12637225\n",
      "Training [4497/10000] ..........4497 ) Loss= 0.07490196\n",
      "Training [4498/10000] ..........4498 ) Loss= 0.077159606\n",
      "Training [4499/10000] ..........4499 ) Loss= 0.096934356\n",
      "Training [4500/10000] ..........4500 ) Loss= 0.12650022\n",
      "Training [4501/10000] ..........4501 ) Loss= 0.10125926\n",
      "Training [4502/10000] ..........4502 ) Loss= 0.12074447\n",
      "Training [4503/10000] ..........4503 ) Loss= 0.21198562\n",
      "Training [4504/10000] ..........4504 ) Loss= 0.101327695\n",
      "Training [4505/10000] ..........4505 ) Loss= 0.093509056\n",
      "Training [4506/10000] ..........4506 ) Loss= 0.09321372\n",
      "Training [4507/10000] ..........4507 ) Loss= 0.087522656\n",
      "Training [4508/10000] ..........4508 ) Loss= 0.0893028\n",
      "Training [4509/10000] ..........4509 ) Loss= 0.10236334\n",
      "Training [4510/10000] ..........4510 ) Loss= 0.16593307\n",
      "Training [4511/10000] ..........4511 ) Loss= 0.10865342\n",
      "Training [4512/10000] ..........4512 ) Loss= 0.08961061\n",
      "Training [4513/10000] ..........4513 ) Loss= 0.15879227\n",
      "Training [4514/10000] ..........4514 ) Loss= 0.14881295\n",
      "Training [4515/10000] ..........4515 ) Loss= 0.14267637\n",
      "Training [4516/10000] ..........4516 ) Loss= 0.09173301\n",
      "Training [4517/10000] ..........4517 ) Loss= 0.10424678\n",
      "Training [4518/10000] ..........4518 ) Loss= 0.15327337\n",
      "Training [4519/10000] ..........4519 ) Loss= 0.06774789\n",
      "Training [4520/10000] ..........4520 ) Loss= 0.11041579\n",
      "Training [4521/10000] ..........4521 ) Loss= 0.08795115\n",
      "Training [4522/10000] ..........4522 ) Loss= 0.20688981\n",
      "Training [4523/10000] ..........4523 ) Loss= 0.10638204\n",
      "Training [4524/10000] ..........4524 ) Loss= 0.1339014\n",
      "Training [4525/10000] ..........4525 ) Loss= 0.16007301\n",
      "Training [4526/10000] ..........4526 ) Loss= 0.16763824\n",
      "Training [4527/10000] ..........4527 ) Loss= 0.171402\n",
      "Training [4528/10000] ..........4528 ) Loss= 0.24185097\n",
      "Training [4529/10000] ..........4529 ) Loss= 0.21733314\n",
      "Training [4530/10000] ..........4530 ) Loss= 0.07606013\n",
      "Training [4531/10000] ..........4531 ) Loss= 0.112954184\n",
      "Training [4532/10000] ..........4532 ) Loss= 0.1574212\n",
      "Training [4533/10000] ..........4533 ) Loss= 0.12255992\n",
      "Training [4534/10000] ..........4534 ) Loss= 0.093304016\n",
      "Training [4535/10000] ..........4535 ) Loss= 0.09836384\n",
      "Training [4536/10000] ..........4536 ) Loss= 0.08700008\n",
      "Training [4537/10000] ..........4537 ) Loss= 0.15013216\n",
      "Training [4538/10000] ..........4538 ) Loss= 0.12563303\n",
      "Training [4539/10000] ..........4539 ) Loss= 0.13192472\n",
      "Training [4540/10000] ..........4540 ) Loss= 0.08953846\n",
      "Training [4541/10000] ..........4541 ) Loss= 0.091706485\n",
      "Training [4542/10000] ..........4542 ) Loss= 0.2989762\n",
      "Training [4543/10000] ..........4543 ) Loss= 0.12606262\n",
      "Training [4544/10000] ..........4544 ) Loss= 0.09131557\n",
      "Training [4545/10000] ..........4545 ) Loss= 0.12075859\n",
      "Training [4546/10000] ..........4546 ) Loss= 0.12591435\n",
      "Training [4547/10000] ..........4547 ) Loss= 0.20003784\n",
      "Training [4548/10000] ..........4548 ) Loss= 0.12407133\n",
      "Training [4549/10000] ..........4549 ) Loss= 0.067673385\n",
      "Training [4550/10000] ..........4550 ) Loss= 0.11744637\n",
      "Training [4551/10000] ..........4551 ) Loss= 0.109614275\n",
      "Training [4552/10000] ..........4552 ) Loss= 0.12860666\n",
      "Training [4553/10000] ..........4553 ) Loss= 0.104995586\n",
      "Training [4554/10000] ..........4554 ) Loss= 0.14228588\n",
      "Training [4555/10000] ..........4555 ) Loss= 0.10266834\n",
      "Training [4556/10000] ..........4556 ) Loss= 0.11373603\n",
      "Training [4557/10000] ..........4557 ) Loss= 0.12490081\n",
      "Training [4558/10000] ..........4558 ) Loss= 0.12177791\n",
      "Training [4559/10000] ..........4559 ) Loss= 0.1471653\n",
      "Training [4560/10000] ..........4560 ) Loss= 0.10567134\n",
      "Training [4561/10000] ..........4561 ) Loss= 0.07868563\n",
      "Training [4562/10000] ..........4562 ) Loss= 0.2261577\n",
      "Training [4563/10000] ..........4563 ) Loss= 0.10929463\n",
      "Training [4564/10000] ..........4564 ) Loss= 0.11808052\n",
      "Training [4565/10000] ..........4565 ) Loss= 0.09172631\n",
      "Training [4566/10000] ..........4566 ) Loss= 0.09260213\n",
      "Training [4567/10000] ..........4567 ) Loss= 0.14629215\n",
      "Training [4568/10000] ..........4568 ) Loss= 0.15063252\n",
      "Training [4569/10000] ..........4569 ) Loss= 0.0995349\n",
      "Training [4570/10000] ..........4570 ) Loss= 0.1162555\n",
      "Training [4571/10000] ..........4571 ) Loss= 0.07553914\n",
      "Training [4572/10000] ..........4572 ) Loss= 0.11510991\n",
      "Training [4573/10000] ..........4573 ) Loss= 0.10991462\n",
      "Training [4574/10000] ..........4574 ) Loss= 0.12643667\n",
      "Training [4575/10000] ..........4575 ) Loss= 0.1364449\n",
      "Training [4576/10000] ..........4576 ) Loss= 0.14776687\n",
      "Training [4577/10000] ..........4577 ) Loss= 0.1065421\n",
      "Training [4578/10000] ..........4578 ) Loss= 0.049201667\n",
      "Training [4579/10000] ..........4579 ) Loss= 0.057794206\n",
      "Training [4580/10000] ..........4580 ) Loss= 0.16818556\n",
      "Training [4581/10000] ..........4581 ) Loss= 0.071068525\n",
      "Training [4582/10000] ..........4582 ) Loss= 0.073644996\n",
      "Training [4583/10000] ..........4583 ) Loss= 0.19967434\n",
      "Training [4584/10000] ..........4584 ) Loss= 0.27887654\n",
      "Training [4585/10000] ..........4585 ) Loss= 0.070105016\n",
      "Training [4586/10000] ..........4586 ) Loss= 0.11355335\n",
      "Training [4587/10000] ..........4587 ) Loss= 0.095789306\n",
      "Training [4588/10000] ..........4588 ) Loss= 0.14653604\n",
      "Training [4589/10000] ..........4589 ) Loss= 0.10973533\n",
      "Training [4590/10000] ..........4590 ) Loss= 0.08887231\n",
      "Training [4591/10000] ..........4591 ) Loss= 0.07930876\n",
      "Training [4592/10000] ..........4592 ) Loss= 0.08735314\n",
      "Training [4593/10000] ..........4593 ) Loss= 0.10509299\n",
      "Training [4594/10000] ..........4594 ) Loss= 0.17458831\n",
      "Training [4595/10000] ..........4595 ) Loss= 0.06302685\n",
      "Training [4596/10000] ..........4596 ) Loss= 0.13161613\n",
      "Training [4597/10000] ..........4597 ) Loss= 0.10341974\n",
      "Training [4598/10000] ..........4598 ) Loss= 0.08254477\n",
      "Training [4599/10000] ..........4599 ) Loss= 0.13939904\n",
      "Training [4600/10000] ..........4600 ) Loss= 0.08645351\n",
      "Training [4601/10000] ..........4601 ) Loss= 0.05886651\n",
      "Training [4602/10000] ..........4602 ) Loss= 0.15653864\n",
      "Training [4603/10000] ..........4603 ) Loss= 0.12257092\n",
      "Training [4604/10000] ..........4604 ) Loss= 0.2059405\n",
      "Training [4605/10000] ..........4605 ) Loss= 0.23305506\n",
      "Training [4606/10000] ..........4606 ) Loss= 0.105725124\n",
      "Training [4607/10000] ..........4607 ) Loss= 0.08296437\n",
      "Training [4608/10000] ..........4608 ) Loss= 0.130401\n",
      "Training [4609/10000] ..........4609 ) Loss= 0.15017377\n",
      "Training [4610/10000] ..........4610 ) Loss= 0.12267362\n",
      "Training [4611/10000] ..........4611 ) Loss= 0.17640972\n",
      "Training [4612/10000] ..........4612 ) Loss= 0.12796018\n",
      "Training [4613/10000] ..........4613 ) Loss= 0.2819363\n",
      "Training [4614/10000] ..........4614 ) Loss= 0.10064592\n",
      "Training [4615/10000] ..........4615 ) Loss= 0.1483627\n",
      "Training [4616/10000] ..........4616 ) Loss= 0.11708222\n",
      "Training [4617/10000] ..........4617 ) Loss= 0.12589407\n",
      "Training [4618/10000] ..........4618 ) Loss= 0.08898833\n",
      "Training [4619/10000] ..........4619 ) Loss= 0.18075347\n",
      "Training [4620/10000] ..........4620 ) Loss= 0.08932736\n",
      "Training [4621/10000] ..........4621 ) Loss= 0.14838609\n",
      "Training [4622/10000] ..........4622 ) Loss= 0.12928563\n",
      "Training [4623/10000] ..........4623 ) Loss= 0.122330666\n",
      "Training [4624/10000] ..........4624 ) Loss= 0.13538316\n",
      "Training [4625/10000] ..........4625 ) Loss= 0.09887441\n",
      "Training [4626/10000] ..........4626 ) Loss= 0.13470432\n",
      "Training [4627/10000] ..........4627 ) Loss= 0.21057564\n",
      "Training [4628/10000] ..........4628 ) Loss= 0.13152641\n",
      "Training [4629/10000] ..........4629 ) Loss= 0.10954552\n",
      "Training [4630/10000] ..........4630 ) Loss= 0.059848424\n",
      "Training [4631/10000] ..........4631 ) Loss= 0.08062952\n",
      "Training [4632/10000] ..........4632 ) Loss= 0.19147213\n",
      "Training [4633/10000] ..........4633 ) Loss= 0.12460409\n",
      "Training [4634/10000] ..........4634 ) Loss= 0.07215148\n",
      "Training [4635/10000] ..........4635 ) Loss= 0.3197995\n",
      "Training [4636/10000] ..........4636 ) Loss= 0.08780074\n",
      "Training [4637/10000] ..........4637 ) Loss= 0.114068955\n",
      "Training [4638/10000] ..........4638 ) Loss= 0.09896536\n",
      "Training [4639/10000] ..........4639 ) Loss= 0.0718077\n",
      "Training [4640/10000] ..........4640 ) Loss= 0.11050669\n",
      "Training [4641/10000] ..........4641 ) Loss= 0.1140927\n",
      "Training [4642/10000] ..........4642 ) Loss= 0.24515422\n",
      "Training [4643/10000] ..........4643 ) Loss= 0.09845523\n",
      "Training [4644/10000] ..........4644 ) Loss= 0.20584784\n",
      "Training [4645/10000] ..........4645 ) Loss= 0.083978616\n",
      "Training [4646/10000] ..........4646 ) Loss= 0.08888001\n",
      "Training [4647/10000] ..........4647 ) Loss= 0.101881936\n",
      "Training [4648/10000] ..........4648 ) Loss= 0.112805866\n",
      "Training [4649/10000] ..........4649 ) Loss= 0.07210853\n",
      "Training [4650/10000] ..........4650 ) Loss= 0.14080116\n",
      "Training [4651/10000] ..........4651 ) Loss= 0.09270108\n",
      "Training [4652/10000] ..........4652 ) Loss= 0.15796493\n",
      "Training [4653/10000] ..........4653 ) Loss= 0.11235908\n",
      "Training [4654/10000] ..........4654 ) Loss= 0.10771474\n",
      "Training [4655/10000] ..........4655 ) Loss= 0.19522472\n",
      "Training [4656/10000] ..........4656 ) Loss= 0.13989562\n",
      "Training [4657/10000] ..........4657 ) Loss= 0.1430151\n",
      "Training [4658/10000] ..........4658 ) Loss= 0.10345939\n",
      "Training [4659/10000] ..........4659 ) Loss= 0.15399636\n",
      "Training [4660/10000] ..........4660 ) Loss= 0.1015611\n",
      "Training [4661/10000] ..........4661 ) Loss= 0.12883729\n",
      "Training [4662/10000] ..........4662 ) Loss= 0.103327036\n",
      "Training [4663/10000] ..........4663 ) Loss= 0.122214004\n",
      "Training [4664/10000] ..........4664 ) Loss= 0.17254722\n",
      "Training [4665/10000] ..........4665 ) Loss= 0.1019415\n",
      "Training [4666/10000] ..........4666 ) Loss= 0.12094919\n",
      "Training [4667/10000] ..........4667 ) Loss= 0.1102314\n",
      "Training [4668/10000] ..........4668 ) Loss= 0.15664361\n",
      "Training [4669/10000] ..........4669 ) Loss= 0.08369143\n",
      "Training [4670/10000] ..........4670 ) Loss= 0.26679075\n",
      "Training [4671/10000] ..........4671 ) Loss= 0.16844268\n",
      "Training [4672/10000] ..........4672 ) Loss= 0.19953756\n",
      "Training [4673/10000] ..........4673 ) Loss= 0.121511444\n",
      "Training [4674/10000] ..........4674 ) Loss= 0.13733433\n",
      "Training [4675/10000] ..........4675 ) Loss= 0.1164722\n",
      "Training [4676/10000] ..........4676 ) Loss= 0.118046276\n",
      "Training [4677/10000] ..........4677 ) Loss= 0.07526068\n",
      "Training [4678/10000] ..........4678 ) Loss= 0.107608624\n",
      "Training [4679/10000] ..........4679 ) Loss= 0.2990447\n",
      "Training [4680/10000] ..........4680 ) Loss= 0.11248022\n",
      "Training [4681/10000] ..........4681 ) Loss= 0.15407433\n",
      "Training [4682/10000] ..........4682 ) Loss= 0.20443751\n",
      "Training [4683/10000] ..........4683 ) Loss= 0.11768735\n",
      "Training [4684/10000] ..........4684 ) Loss= 0.06497577\n",
      "Training [4685/10000] ..........4685 ) Loss= 0.107410476\n",
      "Training [4686/10000] ..........4686 ) Loss= 0.15073043\n",
      "Training [4687/10000] ..........4687 ) Loss= 0.07646949\n",
      "Training [4688/10000] ..........4688 ) Loss= 0.17047852\n",
      "Training [4689/10000] ..........4689 ) Loss= 0.08298916\n",
      "Training [4690/10000] ..........4690 ) Loss= 0.15540686\n",
      "Training [4691/10000] ..........4691 ) Loss= 0.14899944\n",
      "Training [4692/10000] ..........4692 ) Loss= 0.20342119\n",
      "Training [4693/10000] ..........4693 ) Loss= 0.18425749\n",
      "Training [4694/10000] ..........4694 ) Loss= 0.09679323\n",
      "Training [4695/10000] ..........4695 ) Loss= 0.6802479\n",
      "Training [4696/10000] ..........4696 ) Loss= 0.0858075\n",
      "Training [4697/10000] ..........4697 ) Loss= 0.085275784\n",
      "Training [4698/10000] ..........4698 ) Loss= 0.08980317\n",
      "Training [4699/10000] ..........4699 ) Loss= 0.10409333\n",
      "Training [4700/10000] ..........4700 ) Loss= 0.14110178\n",
      "Training [4701/10000] ..........4701 ) Loss= 0.07900322\n",
      "Training [4702/10000] ..........4702 ) Loss= 0.19500235\n",
      "Training [4703/10000] ..........4703 ) Loss= 0.124565735\n",
      "Training [4704/10000] ..........4704 ) Loss= 0.08093537\n",
      "Training [4705/10000] ..........4705 ) Loss= 0.10020086\n",
      "Training [4706/10000] ..........4706 ) Loss= 0.53638345\n",
      "Training [4707/10000] ..........4707 ) Loss= 0.10873743\n",
      "Training [4708/10000] ..........4708 ) Loss= 0.13785926\n",
      "Training [4709/10000] ..........4709 ) Loss= 0.16935426\n",
      "Training [4710/10000] ..........4710 ) Loss= 0.14820524\n",
      "Training [4711/10000] ..........4711 ) Loss= 0.1156673\n",
      "Training [4712/10000] ..........4712 ) Loss= 0.13967894\n",
      "Training [4713/10000] ..........4713 ) Loss= 0.10381875\n",
      "Training [4714/10000] ..........4714 ) Loss= 0.15467614\n",
      "Training [4715/10000] ..........4715 ) Loss= 0.14020316\n",
      "Training [4716/10000] ..........4716 ) Loss= 0.08795628\n",
      "Training [4717/10000] ..........4717 ) Loss= 0.13223852\n",
      "Training [4718/10000] ..........4718 ) Loss= 0.10912951\n",
      "Training [4719/10000] ..........4719 ) Loss= 0.10884663\n",
      "Training [4720/10000] ..........4720 ) Loss= 0.14253159\n",
      "Training [4721/10000] ..........4721 ) Loss= 0.17992936\n",
      "Training [4722/10000] ..........4722 ) Loss= 0.05939745\n",
      "Training [4723/10000] ..........4723 ) Loss= 0.12906088\n",
      "Training [4724/10000] ..........4724 ) Loss= 0.18620612\n",
      "Training [4725/10000] ..........4725 ) Loss= 0.76237327\n",
      "Training [4726/10000] ..........4726 ) Loss= 0.09143632\n",
      "Training [4727/10000] ..........4727 ) Loss= 0.14035672\n",
      "Training [4728/10000] ..........4728 ) Loss= 0.08845705\n",
      "Training [4729/10000] ..........4729 ) Loss= 0.17952853\n",
      "Training [4730/10000] ..........4730 ) Loss= 0.1851365\n",
      "Training [4731/10000] ..........4731 ) Loss= 0.1341041\n",
      "Training [4732/10000] ..........4732 ) Loss= 0.11218088\n",
      "Training [4733/10000] ..........4733 ) Loss= 0.27632564\n",
      "Training [4734/10000] ..........4734 ) Loss= 0.15929349\n",
      "Training [4735/10000] ..........4735 ) Loss= 0.13037917\n",
      "Training [4736/10000] ..........4736 ) Loss= 0.14852223\n",
      "Training [4737/10000] ..........4737 ) Loss= 0.1455958\n",
      "Training [4738/10000] ..........4738 ) Loss= 0.101831235\n",
      "Training [4739/10000] ..........4739 ) Loss= 0.145017\n",
      "Training [4740/10000] ..........4740 ) Loss= 0.12033965\n",
      "Training [4741/10000] ..........4741 ) Loss= 0.0876219\n",
      "Training [4742/10000] ..........4742 ) Loss= 0.0898301\n",
      "Training [4743/10000] ..........4743 ) Loss= 0.110352166\n",
      "Training [4744/10000] ..........4744 ) Loss= 0.07124424\n",
      "Training [4745/10000] ..........4745 ) Loss= 0.08679028\n",
      "Training [4746/10000] ..........4746 ) Loss= 0.12319628\n",
      "Training [4747/10000] ..........4747 ) Loss= 0.15892103\n",
      "Training [4748/10000] ..........4748 ) Loss= 0.20095032\n",
      "Training [4749/10000] ..........4749 ) Loss= 0.11260555\n",
      "Training [4750/10000] ..........4750 ) Loss= 0.067837626\n",
      "Training [4751/10000] ..........4751 ) Loss= 0.11250056\n",
      "Training [4752/10000] ..........4752 ) Loss= 0.11941326\n",
      "Training [4753/10000] ..........4753 ) Loss= 0.075347304\n",
      "Training [4754/10000] ..........4754 ) Loss= 0.09762342\n",
      "Training [4755/10000] ..........4755 ) Loss= 0.08912251\n",
      "Training [4756/10000] ..........4756 ) Loss= 0.12573709\n",
      "Training [4757/10000] ..........4757 ) Loss= 0.13149455\n",
      "Training [4758/10000] ..........4758 ) Loss= 0.20087683\n",
      "Training [4759/10000] ..........4759 ) Loss= 0.08474902\n",
      "Training [4760/10000] ..........4760 ) Loss= 0.10621787\n",
      "Training [4761/10000] ..........4761 ) Loss= 0.1215521\n",
      "Training [4762/10000] ..........4762 ) Loss= 0.17335168\n",
      "Training [4763/10000] ..........4763 ) Loss= 0.1358302\n",
      "Training [4764/10000] ..........4764 ) Loss= 0.12189355\n",
      "Training [4765/10000] ..........4765 ) Loss= 0.07545496\n",
      "Training [4766/10000] ..........4766 ) Loss= 0.1478699\n",
      "Training [4767/10000] ..........4767 ) Loss= 0.13924287\n",
      "Training [4768/10000] ..........4768 ) Loss= 0.104261786\n",
      "Training [4769/10000] ..........4769 ) Loss= 0.1515176\n",
      "Training [4770/10000] ..........4770 ) Loss= 0.081135176\n",
      "Training [4771/10000] ..........4771 ) Loss= 0.1392841\n",
      "Training [4772/10000] ..........4772 ) Loss= 0.49975997\n",
      "Training [4773/10000] ..........4773 ) Loss= 0.08484237\n",
      "Training [4774/10000] ..........4774 ) Loss= 0.0954859\n",
      "Training [4775/10000] ..........4775 ) Loss= 0.15444368\n",
      "Training [4776/10000] ..........4776 ) Loss= 0.19491567\n",
      "Training [4777/10000] ..........4777 ) Loss= 0.11787373\n",
      "Training [4778/10000] ..........4778 ) Loss= 0.15617296\n",
      "Training [4779/10000] ..........4779 ) Loss= 0.066839874\n",
      "Training [4780/10000] ..........4780 ) Loss= 0.07977245\n",
      "Training [4781/10000] ..........4781 ) Loss= 0.19038805\n",
      "Training [4782/10000] ..........4782 ) Loss= 0.1541318\n",
      "Training [4783/10000] ..........4783 ) Loss= 0.12333073\n",
      "Training [4784/10000] ..........4784 ) Loss= 0.09832342\n",
      "Training [4785/10000] ..........4785 ) Loss= 0.10988129\n",
      "Training [4786/10000] ..........4786 ) Loss= 0.11489241\n",
      "Training [4787/10000] ..........4787 ) Loss= 0.16416563\n",
      "Training [4788/10000] ..........4788 ) Loss= 0.25528145\n",
      "Training [4789/10000] ..........4789 ) Loss= 0.07333622\n",
      "Training [4790/10000] ..........4790 ) Loss= 0.23288858\n",
      "Training [4791/10000] ..........4791 ) Loss= 0.18133922\n",
      "Training [4792/10000] ..........4792 ) Loss= 0.14306028\n",
      "Training [4793/10000] ..........4793 ) Loss= 0.10136511\n",
      "Training [4794/10000] ..........4794 ) Loss= 0.087586634\n",
      "Training [4795/10000] ..........4795 ) Loss= 0.112424426\n",
      "Training [4796/10000] ..........4796 ) Loss= 0.1128747\n",
      "Training [4797/10000] ..........4797 ) Loss= 0.11190449\n",
      "Training [4798/10000] ..........4798 ) Loss= 0.092254855\n",
      "Training [4799/10000] ..........4799 ) Loss= 0.15243809\n",
      "Training [4800/10000] ..........4800 ) Loss= 0.13512354\n",
      "Training [4801/10000] ..........4801 ) Loss= 0.13680576\n",
      "Training [4802/10000] ..........4802 ) Loss= 0.22589043\n",
      "Training [4803/10000] ..........4803 ) Loss= 0.13844156\n",
      "Training [4804/10000] ..........4804 ) Loss= 0.1008832\n",
      "Training [4805/10000] ..........4805 ) Loss= 0.130677\n",
      "Training [4806/10000] ..........4806 ) Loss= 0.057262443\n",
      "Training [4807/10000] ..........4807 ) Loss= 0.12111257\n",
      "Training [4808/10000] ..........4808 ) Loss= 0.19129027\n",
      "Training [4809/10000] ..........4809 ) Loss= 0.104284994\n",
      "Training [4810/10000] ..........4810 ) Loss= 0.13997231\n",
      "Training [4811/10000] ..........4811 ) Loss= 0.12282431\n",
      "Training [4812/10000] ..........4812 ) Loss= 0.11928986\n",
      "Training [4813/10000] ..........4813 ) Loss= 0.20749046\n",
      "Training [4814/10000] ..........4814 ) Loss= 0.08157789\n",
      "Training [4815/10000] ..........4815 ) Loss= 0.16284108\n",
      "Training [4816/10000] ..........4816 ) Loss= 0.23783565\n",
      "Training [4817/10000] ..........4817 ) Loss= 0.15026915\n",
      "Training [4818/10000] ..........4818 ) Loss= 0.16179955\n",
      "Training [4819/10000] ..........4819 ) Loss= 0.12738036\n",
      "Training [4820/10000] ..........4820 ) Loss= 0.068326\n",
      "Training [4821/10000] ..........4821 ) Loss= 0.07514056\n",
      "Training [4822/10000] ..........4822 ) Loss= 0.11517342\n",
      "Training [4823/10000] ..........4823 ) Loss= 0.1349163\n",
      "Training [4824/10000] ..........4824 ) Loss= 0.21464764\n",
      "Training [4825/10000] ..........4825 ) Loss= 0.090373464\n",
      "Training [4826/10000] ..........4826 ) Loss= 0.0666827\n",
      "Training [4827/10000] ..........4827 ) Loss= 0.07941539\n",
      "Training [4828/10000] ..........4828 ) Loss= 0.054924157\n",
      "Training [4829/10000] ..........4829 ) Loss= 0.08130386\n",
      "Training [4830/10000] ..........4830 ) Loss= 0.08987633\n",
      "Training [4831/10000] ..........4831 ) Loss= 0.1103918\n",
      "Training [4832/10000] ..........4832 ) Loss= 0.20108175\n",
      "Training [4833/10000] ..........4833 ) Loss= 0.08096642\n",
      "Training [4834/10000] ..........4834 ) Loss= 0.099951155\n",
      "Training [4835/10000] ..........4835 ) Loss= 0.0963713\n",
      "Training [4836/10000] ..........4836 ) Loss= 0.107745595\n",
      "Training [4837/10000] ..........4837 ) Loss= 0.13329667\n",
      "Training [4838/10000] ..........4838 ) Loss= 0.4134838\n",
      "Training [4839/10000] ..........4839 ) Loss= 0.07990565\n",
      "Training [4840/10000] ..........4840 ) Loss= 0.29148334\n",
      "Training [4841/10000] ..........4841 ) Loss= 0.09270634\n",
      "Training [4842/10000] ..........4842 ) Loss= 0.21329616\n",
      "Training [4843/10000] ..........4843 ) Loss= 0.1382059\n",
      "Training [4844/10000] ..........4844 ) Loss= 0.10806825\n",
      "Training [4845/10000] ..........4845 ) Loss= 0.20988236\n",
      "Training [4846/10000] ..........4846 ) Loss= 0.103165306\n",
      "Training [4847/10000] ..........4847 ) Loss= 0.09500677\n",
      "Training [4848/10000] ..........4848 ) Loss= 0.091731764\n",
      "Training [4849/10000] ..........4849 ) Loss= 0.20722441\n",
      "Training [4850/10000] ..........4850 ) Loss= 0.23898527\n",
      "Training [4851/10000] ..........4851 ) Loss= 0.06504209\n",
      "Training [4852/10000] ..........4852 ) Loss= 0.13491446\n",
      "Training [4853/10000] ..........4853 ) Loss= 0.07209902\n",
      "Training [4854/10000] ..........4854 ) Loss= 0.10479992\n",
      "Training [4855/10000] ..........4855 ) Loss= 0.1066082\n",
      "Training [4856/10000] ..........4856 ) Loss= 0.09231052\n",
      "Training [4857/10000] ..........4857 ) Loss= 0.099007204\n",
      "Training [4858/10000] ..........4858 ) Loss= 0.14709383\n",
      "Training [4859/10000] ..........4859 ) Loss= 0.081246905\n",
      "Training [4860/10000] ..........4860 ) Loss= 0.1277493\n",
      "Training [4861/10000] ..........4861 ) Loss= 0.069784775\n",
      "Training [4862/10000] ..........4862 ) Loss= 0.103878416\n",
      "Training [4863/10000] ..........4863 ) Loss= 0.23355563\n",
      "Training [4864/10000] ..........4864 ) Loss= 0.1226343\n",
      "Training [4865/10000] ..........4865 ) Loss= 0.13438125\n",
      "Training [4866/10000] ..........4866 ) Loss= 0.09811632\n",
      "Training [4867/10000] ..........4867 ) Loss= 0.11027844\n",
      "Training [4868/10000] ..........4868 ) Loss= 0.16207334\n",
      "Training [4869/10000] ..........4869 ) Loss= 0.080331594\n",
      "Training [4870/10000] ..........4870 ) Loss= 0.073404945\n",
      "Training [4871/10000] ..........4871 ) Loss= 0.10819156\n",
      "Training [4872/10000] ..........4872 ) Loss= 0.07413927\n",
      "Training [4873/10000] ..........4873 ) Loss= 0.12367994\n",
      "Training [4874/10000] ..........4874 ) Loss= 0.08438464\n",
      "Training [4875/10000] ..........4875 ) Loss= 0.09169988\n",
      "Training [4876/10000] ..........4876 ) Loss= 0.14792393\n",
      "Training [4877/10000] ..........4877 ) Loss= 0.2686171\n",
      "Training [4878/10000] ..........4878 ) Loss= 0.20587261\n",
      "Training [4879/10000] ..........4879 ) Loss= 0.08666527\n",
      "Training [4880/10000] ..........4880 ) Loss= 0.31113163\n",
      "Training [4881/10000] ..........4881 ) Loss= 0.17275052\n",
      "Training [4882/10000] ..........4882 ) Loss= 0.12467577\n",
      "Training [4883/10000] ..........4883 ) Loss= 0.17673431\n",
      "Training [4884/10000] ..........4884 ) Loss= 0.09617611\n",
      "Training [4885/10000] ..........4885 ) Loss= 0.11933213\n",
      "Training [4886/10000] ..........4886 ) Loss= 0.11182639\n",
      "Training [4887/10000] ..........4887 ) Loss= 0.48498827\n",
      "Training [4888/10000] ..........4888 ) Loss= 0.22410405\n",
      "Training [4889/10000] ..........4889 ) Loss= 0.087251544\n",
      "Training [4890/10000] ..........4890 ) Loss= 0.12868899\n",
      "Training [4891/10000] ..........4891 ) Loss= 0.0884875\n",
      "Training [4892/10000] ..........4892 ) Loss= 0.10917407\n",
      "Training [4893/10000] ..........4893 ) Loss= 0.1342993\n",
      "Training [4894/10000] ..........4894 ) Loss= 0.07480602\n",
      "Training [4895/10000] ..........4895 ) Loss= 0.16254751\n",
      "Training [4896/10000] ..........4896 ) Loss= 0.3867441\n",
      "Training [4897/10000] ..........4897 ) Loss= 0.08588355\n",
      "Training [4898/10000] ..........4898 ) Loss= 0.08482389\n",
      "Training [4899/10000] ..........4899 ) Loss= 0.0867852\n",
      "Training [4900/10000] ..........4900 ) Loss= 0.10493723\n",
      "Training [4901/10000] ..........4901 ) Loss= 0.11025673\n",
      "Training [4902/10000] ..........4902 ) Loss= 0.19810456\n",
      "Training [4903/10000] ..........4903 ) Loss= 0.116633885\n",
      "Training [4904/10000] ..........4904 ) Loss= 0.11134808\n",
      "Training [4905/10000] ..........4905 ) Loss= 0.14186828\n",
      "Training [4906/10000] ..........4906 ) Loss= 0.10148659\n",
      "Training [4907/10000] ..........4907 ) Loss= 0.1413486\n",
      "Training [4908/10000] ..........4908 ) Loss= 0.12791045\n",
      "Training [4909/10000] ..........4909 ) Loss= 0.17408733\n",
      "Training [4910/10000] ..........4910 ) Loss= 0.09705664\n",
      "Training [4911/10000] ..........4911 ) Loss= 0.10497611\n",
      "Training [4912/10000] ..........4912 ) Loss= 0.19729643\n",
      "Training [4913/10000] ..........4913 ) Loss= 0.095920965\n",
      "Training [4914/10000] ..........4914 ) Loss= 0.24360996\n",
      "Training [4915/10000] ..........4915 ) Loss= 0.12501334\n",
      "Training [4916/10000] ..........4916 ) Loss= 0.10581901\n",
      "Training [4917/10000] ..........4917 ) Loss= 0.15836419\n",
      "Training [4918/10000] ..........4918 ) Loss= 0.09392725\n",
      "Training [4919/10000] ..........4919 ) Loss= 0.32585642\n",
      "Training [4920/10000] ..........4920 ) Loss= 0.083144516\n",
      "Training [4921/10000] ..........4921 ) Loss= 0.19454893\n",
      "Training [4922/10000] ..........4922 ) Loss= 0.13256\n",
      "Training [4923/10000] ..........4923 ) Loss= 0.28763968\n",
      "Training [4924/10000] ..........4924 ) Loss= 0.06701898\n",
      "Training [4925/10000] ..........4925 ) Loss= 0.18673155\n",
      "Training [4926/10000] ..........4926 ) Loss= 0.08768636\n",
      "Training [4927/10000] ..........4927 ) Loss= 0.2237262\n",
      "Training [4928/10000] ..........4928 ) Loss= 0.15590662\n",
      "Training [4929/10000] ..........4929 ) Loss= 0.109733135\n",
      "Training [4930/10000] ..........4930 ) Loss= 0.08249364\n",
      "Training [4931/10000] ..........4931 ) Loss= 0.10888863\n",
      "Training [4932/10000] ..........4932 ) Loss= 0.15105404\n",
      "Training [4933/10000] ..........4933 ) Loss= 0.09626579\n",
      "Training [4934/10000] ..........4934 ) Loss= 0.21079198\n",
      "Training [4935/10000] ..........4935 ) Loss= 0.20337705\n",
      "Training [4936/10000] ..........4936 ) Loss= 0.09433395\n",
      "Training [4937/10000] ..........4937 ) Loss= 0.11462329\n",
      "Training [4938/10000] ..........4938 ) Loss= 0.10920175\n",
      "Training [4939/10000] ..........4939 ) Loss= 0.09777539\n",
      "Training [4940/10000] ..........4940 ) Loss= 0.10037552\n",
      "Training [4941/10000] ..........4941 ) Loss= 0.15447553\n",
      "Training [4942/10000] ..........4942 ) Loss= 0.12529317\n",
      "Training [4943/10000] ..........4943 ) Loss= 0.15642413\n",
      "Training [4944/10000] ..........4944 ) Loss= 0.1476323\n",
      "Training [4945/10000] ..........4945 ) Loss= 0.13453948\n",
      "Training [4946/10000] ..........4946 ) Loss= 0.2208751\n",
      "Training [4947/10000] ..........4947 ) Loss= 0.09130795\n",
      "Training [4948/10000] ..........4948 ) Loss= 0.08245044\n",
      "Training [4949/10000] ..........4949 ) Loss= 0.11555863\n",
      "Training [4950/10000] ..........4950 ) Loss= 0.12063359\n",
      "Training [4951/10000] ..........4951 ) Loss= 0.12497921\n",
      "Training [4952/10000] ..........4952 ) Loss= 0.08149681\n",
      "Training [4953/10000] ..........4953 ) Loss= 0.25301903\n",
      "Training [4954/10000] ..........4954 ) Loss= 0.07958397\n",
      "Training [4955/10000] ..........4955 ) Loss= 0.08910329\n",
      "Training [4956/10000] ..........4956 ) Loss= 0.08113324\n",
      "Training [4957/10000] ..........4957 ) Loss= 0.12067657\n",
      "Training [4958/10000] ..........4958 ) Loss= 0.10921088\n",
      "Training [4959/10000] ..........4959 ) Loss= 0.05036989\n",
      "Training [4960/10000] ..........4960 ) Loss= 0.12487765\n",
      "Training [4961/10000] ..........4961 ) Loss= 0.089920774\n",
      "Training [4962/10000] ..........4962 ) Loss= 0.09549236\n",
      "Training [4963/10000] ..........4963 ) Loss= 0.1237922\n",
      "Training [4964/10000] ..........4964 ) Loss= 0.12447991\n",
      "Training [4965/10000] ..........4965 ) Loss= 0.107988335\n",
      "Training [4966/10000] ..........4966 ) Loss= 0.118393004\n",
      "Training [4967/10000] ..........4967 ) Loss= 0.09990261\n",
      "Training [4968/10000] ..........4968 ) Loss= 0.08971541\n",
      "Training [4969/10000] ..........4969 ) Loss= 0.14170633\n",
      "Training [4970/10000] ..........4970 ) Loss= 0.12913246\n",
      "Training [4971/10000] ..........4971 ) Loss= 0.11861359\n",
      "Training [4972/10000] ..........4972 ) Loss= 0.3027694\n",
      "Training [4973/10000] ..........4973 ) Loss= 0.07982145\n",
      "Training [4974/10000] ..........4974 ) Loss= 0.058101304\n",
      "Training [4975/10000] ..........4975 ) Loss= 0.13573948\n",
      "Training [4976/10000] ..........4976 ) Loss= 0.10552808\n",
      "Training [4977/10000] ..........4977 ) Loss= 0.07015474\n",
      "Training [4978/10000] ..........4978 ) Loss= 0.09545175\n",
      "Training [4979/10000] ..........4979 ) Loss= 0.10260995\n",
      "Training [4980/10000] ..........4980 ) Loss= 0.20329525\n",
      "Training [4981/10000] ..........4981 ) Loss= 0.07973325\n",
      "Training [4982/10000] ..........4982 ) Loss= 0.33565983\n",
      "Training [4983/10000] ..........4983 ) Loss= 0.09385914\n",
      "Training [4984/10000] ..........4984 ) Loss= 0.08895402\n",
      "Training [4985/10000] ..........4985 ) Loss= 0.11138247\n",
      "Training [4986/10000] ..........4986 ) Loss= 0.10632659\n",
      "Training [4987/10000] ..........4987 ) Loss= 0.10392159\n",
      "Training [4988/10000] ..........4988 ) Loss= 0.1322861\n",
      "Training [4989/10000] ..........4989 ) Loss= 0.7135751\n",
      "Training [4990/10000] ..........4990 ) Loss= 0.12769707\n",
      "Training [4991/10000] ..........4991 ) Loss= 0.09123742\n",
      "Training [4992/10000] ..........4992 ) Loss= 0.102466054\n",
      "Training [4993/10000] ..........4993 ) Loss= 0.0955627\n",
      "Training [4994/10000] ..........4994 ) Loss= 0.07030642\n",
      "Training [4995/10000] ..........4995 ) Loss= 0.07550536\n",
      "Training [4996/10000] ..........4996 ) Loss= 0.12241563\n",
      "Training [4997/10000] ..........4997 ) Loss= 0.08110582\n",
      "Training [4998/10000] ..........4998 ) Loss= 0.110988475\n",
      "Training [4999/10000] ..........4999 ) Loss= 0.18763761\n",
      "Training [5000/10000] ..........5000 ) Loss= 0.1985495 - Saving Model5000.torch\n",
      "Training [5001/10000] ..........5001 ) Loss= 0.077504575\n",
      "Training [5002/10000] ..........5002 ) Loss= 0.15397778\n",
      "Training [5003/10000] ..........5003 ) Loss= 0.08132763\n",
      "Training [5004/10000] ..........5004 ) Loss= 0.10917621\n",
      "Training [5005/10000] ..........5005 ) Loss= 0.07855228\n",
      "Training [5006/10000] ..........5006 ) Loss= 0.19025883\n",
      "Training [5007/10000] ..........5007 ) Loss= 0.0772168\n",
      "Training [5008/10000] ..........5008 ) Loss= 0.10182763\n",
      "Training [5009/10000] ..........5009 ) Loss= 0.100989\n",
      "Training [5010/10000] ..........5010 ) Loss= 0.17141227\n",
      "Training [5011/10000] ..........5011 ) Loss= 0.07761079\n",
      "Training [5012/10000] ..........5012 ) Loss= 0.09624123\n",
      "Training [5013/10000] ..........5013 ) Loss= 0.13675785\n",
      "Training [5014/10000] ..........5014 ) Loss= 0.13722137\n",
      "Training [5015/10000] ..........5015 ) Loss= 0.12748678\n",
      "Training [5016/10000] ..........5016 ) Loss= 0.10289658\n",
      "Training [5017/10000] ..........5017 ) Loss= 0.103003405\n",
      "Training [5018/10000] ..........5018 ) Loss= 0.10750208\n",
      "Training [5019/10000] ..........5019 ) Loss= 0.14114963\n",
      "Training [5020/10000] ..........5020 ) Loss= 0.14020413\n",
      "Training [5021/10000] ..........5021 ) Loss= 0.21731167\n",
      "Training [5022/10000] ..........5022 ) Loss= 0.1514564\n",
      "Training [5023/10000] ..........5023 ) Loss= 0.08427936\n",
      "Training [5024/10000] ..........5024 ) Loss= 0.20857756\n",
      "Training [5025/10000] ..........5025 ) Loss= 0.08032272\n",
      "Training [5026/10000] ..........5026 ) Loss= 0.11512102\n",
      "Training [5027/10000] ..........5027 ) Loss= 0.1079312\n",
      "Training [5028/10000] ..........5028 ) Loss= 0.08267515\n",
      "Training [5029/10000] ..........5029 ) Loss= 0.16890293\n",
      "Training [5030/10000] ..........5030 ) Loss= 0.076860055\n",
      "Training [5031/10000] ..........5031 ) Loss= 0.11226444\n",
      "Training [5032/10000] ..........5032 ) Loss= 0.09976946\n",
      "Training [5033/10000] ..........5033 ) Loss= 0.14235593\n",
      "Training [5034/10000] ..........5034 ) Loss= 0.22328246\n",
      "Training [5035/10000] ..........5035 ) Loss= 0.13263424\n",
      "Training [5036/10000] ..........5036 ) Loss= 0.097151116\n",
      "Training [5037/10000] ..........5037 ) Loss= 0.08526585\n",
      "Training [5038/10000] ..........5038 ) Loss= 0.17200364\n",
      "Training [5039/10000] ..........5039 ) Loss= 0.12957446\n",
      "Training [5040/10000] ..........5040 ) Loss= 0.11468026\n",
      "Training [5041/10000] ..........5041 ) Loss= 0.12916622\n",
      "Training [5042/10000] ..........5042 ) Loss= 0.10005128\n",
      "Training [5043/10000] ..........5043 ) Loss= 0.07236361\n",
      "Training [5044/10000] ..........5044 ) Loss= 0.0709146\n",
      "Training [5045/10000] ..........5045 ) Loss= 0.19183643\n",
      "Training [5046/10000] ..........5046 ) Loss= 0.13594145\n",
      "Training [5047/10000] ..........5047 ) Loss= 0.14921515\n",
      "Training [5048/10000] ..........5048 ) Loss= 0.24777412\n",
      "Training [5049/10000] ..........5049 ) Loss= 0.28238013\n",
      "Training [5050/10000] ..........5050 ) Loss= 0.08265157\n",
      "Training [5051/10000] ..........5051 ) Loss= 0.075144805\n",
      "Training [5052/10000] ..........5052 ) Loss= 0.07332432\n",
      "Training [5053/10000] ..........5053 ) Loss= 0.053599488\n",
      "Training [5054/10000] ..........5054 ) Loss= 0.1147862\n",
      "Training [5055/10000] ..........5055 ) Loss= 0.06211752\n",
      "Training [5056/10000] ..........5056 ) Loss= 0.17525642\n",
      "Training [5057/10000] ..........5057 ) Loss= 0.19334513\n",
      "Training [5058/10000] ..........5058 ) Loss= 0.09169158\n",
      "Training [5059/10000] ..........5059 ) Loss= 0.1391167\n",
      "Training [5060/10000] ..........5060 ) Loss= 0.35031855\n",
      "Training [5061/10000] ..........5061 ) Loss= 0.13487276\n",
      "Training [5062/10000] ..........5062 ) Loss= 0.12131039\n",
      "Training [5063/10000] ..........5063 ) Loss= 0.08224196\n",
      "Training [5064/10000] ..........5064 ) Loss= 0.09518427\n",
      "Training [5065/10000] ..........5065 ) Loss= 0.10010019\n",
      "Training [5066/10000] ..........5066 ) Loss= 0.1927623\n",
      "Training [5067/10000] ..........5067 ) Loss= 0.17322794\n",
      "Training [5068/10000] ..........5068 ) Loss= 0.096838154\n",
      "Training [5069/10000] ..........5069 ) Loss= 0.07388164\n",
      "Training [5070/10000] ..........5070 ) Loss= 0.23796569\n",
      "Training [5071/10000] ..........5071 ) Loss= 0.07546611\n",
      "Training [5072/10000] ..........5072 ) Loss= 0.06878839\n",
      "Training [5073/10000] ..........5073 ) Loss= 0.110860206\n",
      "Training [5074/10000] ..........5074 ) Loss= 0.12970836\n",
      "Training [5075/10000] ..........5075 ) Loss= 0.08952013\n",
      "Training [5076/10000] ..........5076 ) Loss= 0.10624833\n",
      "Training [5077/10000] ..........5077 ) Loss= 0.6254811\n",
      "Training [5078/10000] ..........5078 ) Loss= 0.12776503\n",
      "Training [5079/10000] ..........5079 ) Loss= 0.103303\n",
      "Training [5080/10000] ..........5080 ) Loss= 0.10837512\n",
      "Training [5081/10000] ..........5081 ) Loss= 0.14620385\n",
      "Training [5082/10000] ..........5082 ) Loss= 0.081129044\n",
      "Training [5083/10000] ..........5083 ) Loss= 0.09346747\n",
      "Training [5084/10000] ..........5084 ) Loss= 0.08524979\n",
      "Training [5085/10000] ..........5085 ) Loss= 0.09004852\n",
      "Training [5086/10000] ..........5086 ) Loss= 0.13033463\n",
      "Training [5087/10000] ..........5087 ) Loss= 0.08891192\n",
      "Training [5088/10000] ..........5088 ) Loss= 0.09542159\n",
      "Training [5089/10000] ..........5089 ) Loss= 0.08058816\n",
      "Training [5090/10000] ..........5090 ) Loss= 0.11860876\n",
      "Training [5091/10000] ..........5091 ) Loss= 0.087337814\n",
      "Training [5092/10000] ..........5092 ) Loss= 0.31862414\n",
      "Training [5093/10000] ..........5093 ) Loss= 0.15100501\n",
      "Training [5094/10000] ..........5094 ) Loss= 0.08772104\n",
      "Training [5095/10000] ..........5095 ) Loss= 0.15002495\n",
      "Training [5096/10000] ..........5096 ) Loss= 0.0731009\n",
      "Training [5097/10000] ..........5097 ) Loss= 0.11065365\n",
      "Training [5098/10000] ..........5098 ) Loss= 0.13865425\n",
      "Training [5099/10000] ..........5099 ) Loss= 0.09391007\n",
      "Training [5100/10000] ..........5100 ) Loss= 0.18067881\n",
      "Training [5101/10000] ..........5101 ) Loss= 0.09563081\n",
      "Training [5102/10000] ..........5102 ) Loss= 0.075175144\n",
      "Training [5103/10000] ..........5103 ) Loss= 0.12481317\n",
      "Training [5104/10000] ..........5104 ) Loss= 0.13328835\n",
      "Training [5105/10000] ..........5105 ) Loss= 0.21457416\n",
      "Training [5106/10000] ..........5106 ) Loss= 0.14619395\n",
      "Training [5107/10000] ..........5107 ) Loss= 0.05434062\n",
      "Training [5108/10000] ..........5108 ) Loss= 0.09534207\n",
      "Training [5109/10000] ..........5109 ) Loss= 0.081120595\n",
      "Training [5110/10000] ..........5110 ) Loss= 0.12698174\n",
      "Training [5111/10000] ..........5111 ) Loss= 0.22534457\n",
      "Training [5112/10000] ..........5112 ) Loss= 0.0833209\n",
      "Training [5113/10000] ..........5113 ) Loss= 0.38791507\n",
      "Training [5114/10000] ..........5114 ) Loss= 0.1252715\n",
      "Training [5115/10000] ..........5115 ) Loss= 0.11637853\n",
      "Training [5116/10000] ..........5116 ) Loss= 0.066582166\n",
      "Training [5117/10000] ..........5117 ) Loss= 0.13781053\n",
      "Training [5118/10000] ..........5118 ) Loss= 0.10159322\n",
      "Training [5119/10000] ..........5119 ) Loss= 0.12549663\n",
      "Training [5120/10000] ..........5120 ) Loss= 0.086912155\n",
      "Training [5121/10000] ..........5121 ) Loss= 0.09040456\n",
      "Training [5122/10000] ..........5122 ) Loss= 0.09616908\n",
      "Training [5123/10000] ..........5123 ) Loss= 0.09046488\n",
      "Training [5124/10000] ..........5124 ) Loss= 0.09671902\n",
      "Training [5125/10000] ..........5125 ) Loss= 0.07740282\n",
      "Training [5126/10000] ..........5126 ) Loss= 0.10856345\n",
      "Training [5127/10000] ..........5127 ) Loss= 0.17769009\n",
      "Training [5128/10000] ..........5128 ) Loss= 0.117671296\n",
      "Training [5129/10000] ..........5129 ) Loss= 0.11996108\n",
      "Training [5130/10000] ..........5130 ) Loss= 0.119430184\n",
      "Training [5131/10000] ..........5131 ) Loss= 0.10698657\n",
      "Training [5132/10000] ..........5132 ) Loss= 0.10492838\n",
      "Training [5133/10000] ..........5133 ) Loss= 0.107716694\n",
      "Training [5134/10000] ..........5134 ) Loss= 0.09072777\n",
      "Training [5135/10000] ..........5135 ) Loss= 0.073155895\n",
      "Training [5136/10000] ..........5136 ) Loss= 0.24984573\n",
      "Training [5137/10000] ..........5137 ) Loss= 0.17458758\n",
      "Training [5138/10000] ..........5138 ) Loss= 0.12562673\n",
      "Training [5139/10000] ..........5139 ) Loss= 0.09233686\n",
      "Training [5140/10000] ..........5140 ) Loss= 0.07370124\n",
      "Training [5141/10000] ..........5141 ) Loss= 0.072808765\n",
      "Training [5142/10000] ..........5142 ) Loss= 0.073915266\n",
      "Training [5143/10000] ..........5143 ) Loss= 0.10971351\n",
      "Training [5144/10000] ..........5144 ) Loss= 0.12545724\n",
      "Training [5145/10000] ..........5145 ) Loss= 0.121177286\n",
      "Training [5146/10000] ..........5146 ) Loss= 0.08440125\n",
      "Training [5147/10000] ..........5147 ) Loss= 0.06324644\n",
      "Training [5148/10000] ..........5148 ) Loss= 0.23128438\n",
      "Training [5149/10000] ..........5149 ) Loss= 0.08777978\n",
      "Training [5150/10000] ..........5150 ) Loss= 0.2619848\n",
      "Training [5151/10000] ..........5151 ) Loss= 0.14662392\n",
      "Training [5152/10000] ..........5152 ) Loss= 0.39318025\n",
      "Training [5153/10000] ..........5153 ) Loss= 0.15125057\n",
      "Training [5154/10000] ..........5154 ) Loss= 0.09137848\n",
      "Training [5155/10000] ..........5155 ) Loss= 0.093958415\n",
      "Training [5156/10000] ..........5156 ) Loss= 0.10528721\n",
      "Training [5157/10000] ..........5157 ) Loss= 0.078628756\n",
      "Training [5158/10000] ..........5158 ) Loss= 0.11517576\n",
      "Training [5159/10000] ..........5159 ) Loss= 0.19808751\n",
      "Training [5160/10000] ..........5160 ) Loss= 0.08533762\n",
      "Training [5161/10000] ..........5161 ) Loss= 0.21258454\n",
      "Training [5162/10000] ..........5162 ) Loss= 0.08200637\n",
      "Training [5163/10000] ..........5163 ) Loss= 0.071096234\n",
      "Training [5164/10000] ..........5164 ) Loss= 0.08990912\n",
      "Training [5165/10000] ..........5165 ) Loss= 0.09259434\n",
      "Training [5166/10000] ..........5166 ) Loss= 0.15141235\n",
      "Training [5167/10000] ..........5167 ) Loss= 0.08031129\n",
      "Training [5168/10000] ..........5168 ) Loss= 0.11141236\n",
      "Training [5169/10000] ..........5169 ) Loss= 0.1989764\n",
      "Training [5170/10000] ..........5170 ) Loss= 0.15025963\n",
      "Training [5171/10000] ..........5171 ) Loss= 0.18272406\n",
      "Training [5172/10000] ..........5172 ) Loss= 0.13975577\n",
      "Training [5173/10000] ..........5173 ) Loss= 0.1378425\n",
      "Training [5174/10000] ..........5174 ) Loss= 0.13679308\n",
      "Training [5175/10000] ..........5175 ) Loss= 0.1038327\n",
      "Training [5176/10000] ..........5176 ) Loss= 0.08470564\n",
      "Training [5177/10000] ..........5177 ) Loss= 0.21910828\n",
      "Training [5178/10000] ..........5178 ) Loss= 0.26514843\n",
      "Training [5179/10000] ..........5179 ) Loss= 0.27938774\n",
      "Training [5180/10000] ..........5180 ) Loss= 0.06922645\n",
      "Training [5181/10000] ..........5181 ) Loss= 0.06475224\n",
      "Training [5182/10000] ..........5182 ) Loss= 0.06022719\n",
      "Training [5183/10000] ..........5183 ) Loss= 0.07735305\n",
      "Training [5184/10000] ..........5184 ) Loss= 0.14002931\n",
      "Training [5185/10000] ..........5185 ) Loss= 0.085017614\n",
      "Training [5186/10000] ..........5186 ) Loss= 0.095009096\n",
      "Training [5187/10000] ..........5187 ) Loss= 0.12742737\n",
      "Training [5188/10000] ..........5188 ) Loss= 0.11168752\n",
      "Training [5189/10000] ..........5189 ) Loss= 0.09242171\n",
      "Training [5190/10000] ..........5190 ) Loss= 0.3402015\n",
      "Training [5191/10000] ..........5191 ) Loss= 0.14477138\n",
      "Training [5192/10000] ..........5192 ) Loss= 0.09937915\n",
      "Training [5193/10000] ..........5193 ) Loss= 0.12823562\n",
      "Training [5194/10000] ..........5194 ) Loss= 0.132561\n",
      "Training [5195/10000] ..........5195 ) Loss= 0.093086086\n",
      "Training [5196/10000] ..........5196 ) Loss= 0.1595129\n",
      "Training [5197/10000] ..........5197 ) Loss= 0.27290136\n",
      "Training [5198/10000] ..........5198 ) Loss= 0.09053599\n",
      "Training [5199/10000] ..........5199 ) Loss= 0.081614904\n",
      "Training [5200/10000] ..........5200 ) Loss= 0.10103602\n",
      "Training [5201/10000] ..........5201 ) Loss= 0.09435786\n",
      "Training [5202/10000] ..........5202 ) Loss= 0.08981847\n",
      "Training [5203/10000] ..........5203 ) Loss= 0.10908814\n",
      "Training [5204/10000] ..........5204 ) Loss= 0.2714913\n",
      "Training [5205/10000] ..........5205 ) Loss= 0.08272007\n",
      "Training [5206/10000] ..........5206 ) Loss= 0.10086104\n",
      "Training [5207/10000] ..........5207 ) Loss= 0.11290475\n",
      "Training [5208/10000] ..........5208 ) Loss= 0.07831079\n",
      "Training [5209/10000] ..........5209 ) Loss= 0.06416583\n",
      "Training [5210/10000] ..........5210 ) Loss= 0.10620847\n",
      "Training [5211/10000] ..........5211 ) Loss= 0.11822967\n",
      "Training [5212/10000] ..........5212 ) Loss= 0.16718827\n",
      "Training [5213/10000] ..........5213 ) Loss= 0.09100424\n",
      "Training [5214/10000] ..........5214 ) Loss= 0.21191101\n",
      "Training [5215/10000] ..........5215 ) Loss= 0.09216474\n",
      "Training [5216/10000] ..........5216 ) Loss= 0.3876102\n",
      "Training [5217/10000] ..........5217 ) Loss= 0.08304867\n",
      "Training [5218/10000] ..........5218 ) Loss= 0.07036528\n",
      "Training [5219/10000] ..........5219 ) Loss= 0.24420442\n",
      "Training [5220/10000] ..........5220 ) Loss= 0.21852633\n",
      "Training [5221/10000] ..........5221 ) Loss= 0.08944261\n",
      "Training [5222/10000] ..........5222 ) Loss= 0.14454708\n",
      "Training [5223/10000] ..........5223 ) Loss= 0.11886304\n",
      "Training [5224/10000] ..........5224 ) Loss= 0.10523669\n",
      "Training [5225/10000] ..........5225 ) Loss= 0.089967735\n",
      "Training [5226/10000] ..........5226 ) Loss= 0.082084045\n",
      "Training [5227/10000] ..........5227 ) Loss= 0.09216635\n",
      "Training [5228/10000] ..........5228 ) Loss= 0.09588498\n",
      "Training [5229/10000] ..........5229 ) Loss= 0.06934421\n",
      "Training [5230/10000] ..........5230 ) Loss= 0.058066104\n",
      "Training [5231/10000] ..........5231 ) Loss= 0.07074453\n",
      "Training [5232/10000] ..........5232 ) Loss= 0.21564075\n",
      "Training [5233/10000] ..........5233 ) Loss= 0.086965606\n",
      "Training [5234/10000] ..........5234 ) Loss= 0.12391796\n",
      "Training [5235/10000] ..........5235 ) Loss= 0.09703607\n",
      "Training [5236/10000] ..........5236 ) Loss= 0.09633634\n",
      "Training [5237/10000] ..........5237 ) Loss= 0.101607464\n",
      "Training [5238/10000] ..........5238 ) Loss= 0.0870292\n",
      "Training [5239/10000] ..........5239 ) Loss= 0.10624312\n",
      "Training [5240/10000] ..........5240 ) Loss= 0.18536401\n",
      "Training [5241/10000] ..........5241 ) Loss= 0.096006356\n",
      "Training [5242/10000] ..........5242 ) Loss= 0.16171971\n",
      "Training [5243/10000] ..........5243 ) Loss= 0.07699564\n",
      "Training [5244/10000] ..........5244 ) Loss= 0.09604176\n",
      "Training [5245/10000] ..........5245 ) Loss= 0.07201072\n",
      "Training [5246/10000] ..........5246 ) Loss= 0.13713248\n",
      "Training [5247/10000] ..........5247 ) Loss= 0.12409463\n",
      "Training [5248/10000] ..........5248 ) Loss= 0.13799115\n",
      "Training [5249/10000] ..........5249 ) Loss= 0.13264775\n",
      "Training [5250/10000] ..........5250 ) Loss= 0.10746545\n",
      "Training [5251/10000] ..........5251 ) Loss= 0.09698953\n",
      "Training [5252/10000] ..........5252 ) Loss= 0.23509528\n",
      "Training [5253/10000] ..........5253 ) Loss= 0.09448401\n",
      "Training [5254/10000] ..........5254 ) Loss= 0.073353305\n",
      "Training [5255/10000] ..........5255 ) Loss= 0.12936758\n",
      "Training [5256/10000] ..........5256 ) Loss= 0.04900278\n",
      "Training [5257/10000] ..........5257 ) Loss= 0.0723927\n",
      "Training [5258/10000] ..........5258 ) Loss= 0.10758856\n",
      "Training [5259/10000] ..........5259 ) Loss= 0.1516939\n",
      "Training [5260/10000] ..........5260 ) Loss= 0.11272221\n",
      "Training [5261/10000] ..........5261 ) Loss= 0.16509116\n",
      "Training [5262/10000] ..........5262 ) Loss= 0.1789478\n",
      "Training [5263/10000] ..........5263 ) Loss= 0.12084091\n",
      "Training [5264/10000] ..........5264 ) Loss= 0.13468488\n",
      "Training [5265/10000] ..........5265 ) Loss= 0.21394219\n",
      "Training [5266/10000] ..........5266 ) Loss= 0.08042351\n",
      "Training [5267/10000] ..........5267 ) Loss= 0.09561494\n",
      "Training [5268/10000] ..........5268 ) Loss= 0.10003432\n",
      "Training [5269/10000] ..........5269 ) Loss= 0.23574936\n",
      "Training [5270/10000] ..........5270 ) Loss= 0.07994998\n",
      "Training [5271/10000] ..........5271 ) Loss= 0.124285325\n",
      "Training [5272/10000] ..........5272 ) Loss= 0.061947938\n",
      "Training [5273/10000] ..........5273 ) Loss= 0.07732635\n",
      "Training [5274/10000] ..........5274 ) Loss= 0.18776774\n",
      "Training [5275/10000] ..........5275 ) Loss= 0.117733516\n",
      "Training [5276/10000] ..........5276 ) Loss= 0.101182535\n",
      "Training [5277/10000] ..........5277 ) Loss= 0.08897817\n",
      "Training [5278/10000] ..........5278 ) Loss= 0.22396338\n",
      "Training [5279/10000] ..........5279 ) Loss= 0.17566267\n",
      "Training [5280/10000] ..........5280 ) Loss= 0.47745156\n",
      "Training [5281/10000] ..........5281 ) Loss= 0.085455276\n",
      "Training [5282/10000] ..........5282 ) Loss= 0.16330276\n",
      "Training [5283/10000] ..........5283 ) Loss= 0.24727234\n",
      "Training [5284/10000] ..........5284 ) Loss= 0.05182815\n",
      "Training [5285/10000] ..........5285 ) Loss= 0.27353778\n",
      "Training [5286/10000] ..........5286 ) Loss= 0.064307064\n",
      "Training [5287/10000] ..........5287 ) Loss= 0.10056311\n",
      "Training [5288/10000] ..........5288 ) Loss= 0.1544738\n",
      "Training [5289/10000] ..........5289 ) Loss= 0.15918395\n",
      "Training [5290/10000] ..........5290 ) Loss= 0.07749087\n",
      "Training [5291/10000] ..........5291 ) Loss= 0.117008574\n",
      "Training [5292/10000] ..........5292 ) Loss= 0.20743977\n",
      "Training [5293/10000] ..........5293 ) Loss= 0.15579072\n",
      "Training [5294/10000] ..........5294 ) Loss= 0.17722115\n",
      "Training [5295/10000] ..........5295 ) Loss= 0.38071054\n",
      "Training [5296/10000] ..........5296 ) Loss= 0.07805671\n",
      "Training [5297/10000] ..........5297 ) Loss= 0.17262824\n",
      "Training [5298/10000] ..........5298 ) Loss= 0.12870099\n",
      "Training [5299/10000] ..........5299 ) Loss= 0.1120678\n",
      "Training [5300/10000] ..........5300 ) Loss= 0.119158186\n",
      "Training [5301/10000] ..........5301 ) Loss= 0.1063278\n",
      "Training [5302/10000] ..........5302 ) Loss= 0.09599525\n",
      "Training [5303/10000] ..........5303 ) Loss= 0.09442127\n",
      "Training [5304/10000] ..........5304 ) Loss= 0.07233825\n",
      "Training [5305/10000] ..........5305 ) Loss= 0.21839324\n",
      "Training [5306/10000] ..........5306 ) Loss= 0.12665613\n",
      "Training [5307/10000] ..........5307 ) Loss= 0.10886588\n",
      "Training [5308/10000] ..........5308 ) Loss= 0.15665789\n",
      "Training [5309/10000] ..........5309 ) Loss= 0.12539278\n",
      "Training [5310/10000] ..........5310 ) Loss= 0.11263131\n",
      "Training [5311/10000] ..........5311 ) Loss= 0.079443194\n",
      "Training [5312/10000] ..........5312 ) Loss= 0.27471945\n",
      "Training [5313/10000] ..........5313 ) Loss= 0.12413446\n",
      "Training [5314/10000] ..........5314 ) Loss= 0.11429338\n",
      "Training [5315/10000] ..........5315 ) Loss= 0.114553176\n",
      "Training [5316/10000] ..........5316 ) Loss= 0.077016346\n",
      "Training [5317/10000] ..........5317 ) Loss= 0.10306356\n",
      "Training [5318/10000] ..........5318 ) Loss= 0.11281618\n",
      "Training [5319/10000] ..........5319 ) Loss= 0.09923647\n",
      "Training [5320/10000] ..........5320 ) Loss= 0.1064347\n",
      "Training [5321/10000] ..........5321 ) Loss= 0.069993265\n",
      "Training [5322/10000] ..........5322 ) Loss= 0.1262005\n",
      "Training [5323/10000] ..........5323 ) Loss= 0.16353506\n",
      "Training [5324/10000] ..........5324 ) Loss= 0.113558\n",
      "Training [5325/10000] ..........5325 ) Loss= 0.106299184\n",
      "Training [5326/10000] ..........5326 ) Loss= 0.13951306\n",
      "Training [5327/10000] ..........5327 ) Loss= 0.088103294\n",
      "Training [5328/10000] ..........5328 ) Loss= 0.11309093\n",
      "Training [5329/10000] ..........5329 ) Loss= 0.11868283\n",
      "Training [5330/10000] ..........5330 ) Loss= 0.08105527\n",
      "Training [5331/10000] ..........5331 ) Loss= 0.13889807\n",
      "Training [5332/10000] ..........5332 ) Loss= 0.079847455\n",
      "Training [5333/10000] ..........5333 ) Loss= 0.08744801\n",
      "Training [5334/10000] ..........5334 ) Loss= 0.1241615\n",
      "Training [5335/10000] ..........5335 ) Loss= 0.15037388\n",
      "Training [5336/10000] ..........5336 ) Loss= 0.26186532\n",
      "Training [5337/10000] ..........5337 ) Loss= 0.13737729\n",
      "Training [5338/10000] ..........5338 ) Loss= 0.12938963\n",
      "Training [5339/10000] ..........5339 ) Loss= 0.12330477\n",
      "Training [5340/10000] ..........5340 ) Loss= 0.16661753\n",
      "Training [5341/10000] ..........5341 ) Loss= 0.1920391\n",
      "Training [5342/10000] ..........5342 ) Loss= 0.14833352\n",
      "Training [5343/10000] ..........5343 ) Loss= 0.12279135\n",
      "Training [5344/10000] ..........5344 ) Loss= 0.054505125\n",
      "Training [5345/10000] ..........5345 ) Loss= 0.11179667\n",
      "Training [5346/10000] ..........5346 ) Loss= 0.09536296\n",
      "Training [5347/10000] ..........5347 ) Loss= 0.09780857\n",
      "Training [5348/10000] ..........5348 ) Loss= 0.1048097\n",
      "Training [5349/10000] ..........5349 ) Loss= 0.14847417\n",
      "Training [5350/10000] ..........5350 ) Loss= 0.0800874\n",
      "Training [5351/10000] ..........5351 ) Loss= 0.12194257\n",
      "Training [5352/10000] ..........5352 ) Loss= 0.05622131\n",
      "Training [5353/10000] ..........5353 ) Loss= 0.19151665\n",
      "Training [5354/10000] ..........5354 ) Loss= 0.16213882\n",
      "Training [5355/10000] ..........5355 ) Loss= 0.12439721\n",
      "Training [5356/10000] ..........5356 ) Loss= 0.091824375\n",
      "Training [5357/10000] ..........5357 ) Loss= 0.12971179\n",
      "Training [5358/10000] ..........5358 ) Loss= 0.08727715\n",
      "Training [5359/10000] ..........5359 ) Loss= 0.07315126\n",
      "Training [5360/10000] ..........5360 ) Loss= 0.060239315\n",
      "Training [5361/10000] ..........5361 ) Loss= 0.067512155\n",
      "Training [5362/10000] ..........5362 ) Loss= 0.06894347\n",
      "Training [5363/10000] ..........5363 ) Loss= 0.08785309\n",
      "Training [5364/10000] ..........5364 ) Loss= 0.072401375\n",
      "Training [5365/10000] ..........5365 ) Loss= 0.10387023\n",
      "Training [5366/10000] ..........5366 ) Loss= 0.28881052\n",
      "Training [5367/10000] ..........5367 ) Loss= 0.17403397\n",
      "Training [5368/10000] ..........5368 ) Loss= 0.24014854\n",
      "Training [5369/10000] ..........5369 ) Loss= 0.10602681\n",
      "Training [5370/10000] ..........5370 ) Loss= 0.08866794\n",
      "Training [5371/10000] ..........5371 ) Loss= 0.08900651\n",
      "Training [5372/10000] ..........5372 ) Loss= 0.10939448\n",
      "Training [5373/10000] ..........5373 ) Loss= 0.15168948\n",
      "Training [5374/10000] ..........5374 ) Loss= 0.09035738\n",
      "Training [5375/10000] ..........5375 ) Loss= 0.16621841\n",
      "Training [5376/10000] ..........5376 ) Loss= 0.13424574\n",
      "Training [5377/10000] ..........5377 ) Loss= 0.07864506\n",
      "Training [5378/10000] ..........5378 ) Loss= 0.08193043\n",
      "Training [5379/10000] ..........5379 ) Loss= 0.09550231\n",
      "Training [5380/10000] ..........5380 ) Loss= 0.09416606\n",
      "Training [5381/10000] ..........5381 ) Loss= 0.10843213\n",
      "Training [5382/10000] ..........5382 ) Loss= 0.105093986\n",
      "Training [5383/10000] ..........5383 ) Loss= 0.14023656\n",
      "Training [5384/10000] ..........5384 ) Loss= 0.07414332\n",
      "Training [5385/10000] ..........5385 ) Loss= 0.08552014\n",
      "Training [5386/10000] ..........5386 ) Loss= 0.09176205\n",
      "Training [5387/10000] ..........5387 ) Loss= 0.08928088\n",
      "Training [5388/10000] ..........5388 ) Loss= 0.121989064\n",
      "Training [5389/10000] ..........5389 ) Loss= 0.07233897\n",
      "Training [5390/10000] ..........5390 ) Loss= 0.11635763\n",
      "Training [5391/10000] ..........5391 ) Loss= 0.22772686\n",
      "Training [5392/10000] ..........5392 ) Loss= 0.10740105\n",
      "Training [5393/10000] ..........5393 ) Loss= 0.15649444\n",
      "Training [5394/10000] ..........5394 ) Loss= 0.14507152\n",
      "Training [5395/10000] ..........5395 ) Loss= 0.0907222\n",
      "Training [5396/10000] ..........5396 ) Loss= 0.082788825\n",
      "Training [5397/10000] ..........5397 ) Loss= 0.07193796\n",
      "Training [5398/10000] ..........5398 ) Loss= 0.08583321\n",
      "Training [5399/10000] ..........5399 ) Loss= 0.06840554\n",
      "Training [5400/10000] ..........5400 ) Loss= 0.10479051\n",
      "Training [5401/10000] ..........5401 ) Loss= 0.09184072\n",
      "Training [5402/10000] ..........5402 ) Loss= 0.10229284\n",
      "Training [5403/10000] ..........5403 ) Loss= 0.0856092\n",
      "Training [5404/10000] ..........5404 ) Loss= 0.090463415\n",
      "Training [5405/10000] ..........5405 ) Loss= 0.23001716\n",
      "Training [5406/10000] ..........5406 ) Loss= 0.1337636\n",
      "Training [5407/10000] ..........5407 ) Loss= 0.073936045\n",
      "Training [5408/10000] ..........5408 ) Loss= 0.109959245\n",
      "Training [5409/10000] ..........5409 ) Loss= 0.23980615\n",
      "Training [5410/10000] ..........5410 ) Loss= 0.13798098\n",
      "Training [5411/10000] ..........5411 ) Loss= 0.13572025\n",
      "Training [5412/10000] ..........5412 ) Loss= 0.11692688\n",
      "Training [5413/10000] ..........5413 ) Loss= 0.12464007\n",
      "Training [5414/10000] ..........5414 ) Loss= 0.071667515\n",
      "Training [5415/10000] ..........5415 ) Loss= 0.09076609\n",
      "Training [5416/10000] ..........5416 ) Loss= 0.14367974\n",
      "Training [5417/10000] ..........5417 ) Loss= 0.07478855\n",
      "Training [5418/10000] ..........5418 ) Loss= 0.0919539\n",
      "Training [5419/10000] ..........5419 ) Loss= 0.051793918\n",
      "Training [5420/10000] ..........5420 ) Loss= 0.10027783\n",
      "Training [5421/10000] ..........5421 ) Loss= 0.050414596\n",
      "Training [5422/10000] ..........5422 ) Loss= 0.120829836\n",
      "Training [5423/10000] ..........5423 ) Loss= 0.12013855\n",
      "Training [5424/10000] ..........5424 ) Loss= 0.20416713\n",
      "Training [5425/10000] ..........5425 ) Loss= 0.12745424\n",
      "Training [5426/10000] ..........5426 ) Loss= 0.07440098\n",
      "Training [5427/10000] ..........5427 ) Loss= 0.09387235\n",
      "Training [5428/10000] ..........5428 ) Loss= 0.09751407\n",
      "Training [5429/10000] ..........5429 ) Loss= 0.068899676\n",
      "Training [5430/10000] ..........5430 ) Loss= 0.10197611\n",
      "Training [5431/10000] ..........5431 ) Loss= 0.16688024\n",
      "Training [5432/10000] ..........5432 ) Loss= 0.17424302\n",
      "Training [5433/10000] ..........5433 ) Loss= 0.24709962\n",
      "Training [5434/10000] ..........5434 ) Loss= 0.07884114\n",
      "Training [5435/10000] ..........5435 ) Loss= 0.08116459\n",
      "Training [5436/10000] ..........5436 ) Loss= 0.10170385\n",
      "Training [5437/10000] ..........5437 ) Loss= 0.07916395\n",
      "Training [5438/10000] ..........5438 ) Loss= 0.083801456\n",
      "Training [5439/10000] ..........5439 ) Loss= 0.116788045\n",
      "Training [5440/10000] ..........5440 ) Loss= 0.09607925\n",
      "Training [5441/10000] ..........5441 ) Loss= 0.091184966\n",
      "Training [5442/10000] ..........5442 ) Loss= 0.07742979\n",
      "Training [5443/10000] ..........5443 ) Loss= 0.08462126\n",
      "Training [5444/10000] ..........5444 ) Loss= 0.049297247\n",
      "Training [5445/10000] ..........5445 ) Loss= 0.20385863\n",
      "Training [5446/10000] ..........5446 ) Loss= 0.11230421\n",
      "Training [5447/10000] ..........5447 ) Loss= 0.065540306\n",
      "Training [5448/10000] ..........5448 ) Loss= 0.093993984\n",
      "Training [5449/10000] ..........5449 ) Loss= 0.14258818\n",
      "Training [5450/10000] ..........5450 ) Loss= 0.100758836\n",
      "Training [5451/10000] ..........5451 ) Loss= 0.17704359\n",
      "Training [5452/10000] ..........5452 ) Loss= 0.07090306\n",
      "Training [5453/10000] ..........5453 ) Loss= 0.108405165\n",
      "Training [5454/10000] ..........5454 ) Loss= 0.10317505\n",
      "Training [5455/10000] ..........5455 ) Loss= 0.084783815\n",
      "Training [5456/10000] ..........5456 ) Loss= 0.08187157\n",
      "Training [5457/10000] ..........5457 ) Loss= 0.0672574\n",
      "Training [5458/10000] ..........5458 ) Loss= 0.097433485\n",
      "Training [5459/10000] ..........5459 ) Loss= 0.0709396\n",
      "Training [5460/10000] ..........5460 ) Loss= 0.06708964\n",
      "Training [5461/10000] ..........5461 ) Loss= 0.1384483\n",
      "Training [5462/10000] ..........5462 ) Loss= 0.11679845\n",
      "Training [5463/10000] ..........5463 ) Loss= 0.10066904\n",
      "Training [5464/10000] ..........5464 ) Loss= 0.14362365\n",
      "Training [5465/10000] ..........5465 ) Loss= 0.07905664\n",
      "Training [5466/10000] ..........5466 ) Loss= 0.09622147\n",
      "Training [5467/10000] ..........5467 ) Loss= 0.08522592\n",
      "Training [5468/10000] ..........5468 ) Loss= 0.068351075\n",
      "Training [5469/10000] ..........5469 ) Loss= 0.1016229\n",
      "Training [5470/10000] ..........5470 ) Loss= 0.15878737\n",
      "Training [5471/10000] ..........5471 ) Loss= 0.07581292\n",
      "Training [5472/10000] ..........5472 ) Loss= 0.052005753\n",
      "Training [5473/10000] ..........5473 ) Loss= 0.07638189\n",
      "Training [5474/10000] ..........5474 ) Loss= 0.11288309\n",
      "Training [5475/10000] ..........5475 ) Loss= 0.0848889\n",
      "Training [5476/10000] ..........5476 ) Loss= 0.098523654\n",
      "Training [5477/10000] ..........5477 ) Loss= 0.07516756\n",
      "Training [5478/10000] ..........5478 ) Loss= 0.07717763\n",
      "Training [5479/10000] ..........5479 ) Loss= 0.076323494\n",
      "Training [5480/10000] ..........5480 ) Loss= 0.07621959\n",
      "Training [5481/10000] ..........5481 ) Loss= 0.09431944\n",
      "Training [5482/10000] ..........5482 ) Loss= 0.18875916\n",
      "Training [5483/10000] ..........5483 ) Loss= 0.091352016\n",
      "Training [5484/10000] ..........5484 ) Loss= 0.090985656\n",
      "Training [5485/10000] ..........5485 ) Loss= 0.12361826\n",
      "Training [5486/10000] ..........5486 ) Loss= 0.088678926\n",
      "Training [5487/10000] ..........5487 ) Loss= 0.14928411\n",
      "Training [5488/10000] ..........5488 ) Loss= 0.07469977\n",
      "Training [5489/10000] ..........5489 ) Loss= 0.16201662\n",
      "Training [5490/10000] ..........5490 ) Loss= 0.17789072\n",
      "Training [5491/10000] ..........5491 ) Loss= 0.092835285\n",
      "Training [5492/10000] ..........5492 ) Loss= 0.15429972\n",
      "Training [5493/10000] ..........5493 ) Loss= 0.055610288\n",
      "Training [5494/10000] ..........5494 ) Loss= 0.0774267\n",
      "Training [5495/10000] ..........5495 ) Loss= 0.09001916\n",
      "Training [5496/10000] ..........5496 ) Loss= 0.067894466\n",
      "Training [5497/10000] ..........5497 ) Loss= 0.1318752\n",
      "Training [5498/10000] ..........5498 ) Loss= 0.072378054\n",
      "Training [5499/10000] ..........5499 ) Loss= 0.0948121\n",
      "Training [5500/10000] ..........5500 ) Loss= 0.239074\n",
      "Training [5501/10000] ..........5501 ) Loss= 0.07699323\n",
      "Training [5502/10000] ..........5502 ) Loss= 0.10733759\n",
      "Training [5503/10000] ..........5503 ) Loss= 0.059843294\n",
      "Training [5504/10000] ..........5504 ) Loss= 0.063676044\n",
      "Training [5505/10000] ..........5505 ) Loss= 0.07521252\n",
      "Training [5506/10000] ..........5506 ) Loss= 0.36288798\n",
      "Training [5507/10000] ..........5507 ) Loss= 0.06707154\n",
      "Training [5508/10000] ..........5508 ) Loss= 0.11319753\n",
      "Training [5509/10000] ..........5509 ) Loss= 0.11896237\n",
      "Training [5510/10000] ..........5510 ) Loss= 0.08777501\n",
      "Training [5511/10000] ..........5511 ) Loss= 0.080162436\n",
      "Training [5512/10000] ..........5512 ) Loss= 0.13142252\n",
      "Training [5513/10000] ..........5513 ) Loss= 0.22005787\n",
      "Training [5514/10000] ..........5514 ) Loss= 0.1418877\n",
      "Training [5515/10000] ..........5515 ) Loss= 0.09007096\n",
      "Training [5516/10000] ..........5516 ) Loss= 0.115051195\n",
      "Training [5517/10000] ..........5517 ) Loss= 0.052518476\n",
      "Training [5518/10000] ..........5518 ) Loss= 0.06848928\n",
      "Training [5519/10000] ..........5519 ) Loss= 0.089145124\n",
      "Training [5520/10000] ..........5520 ) Loss= 0.07908179\n",
      "Training [5521/10000] ..........5521 ) Loss= 0.10611146\n",
      "Training [5522/10000] ..........5522 ) Loss= 0.08664081\n",
      "Training [5523/10000] ..........5523 ) Loss= 0.09355209\n",
      "Training [5524/10000] ..........5524 ) Loss= 0.10482677\n",
      "Training [5525/10000] ..........5525 ) Loss= 0.117644206\n",
      "Training [5526/10000] ..........5526 ) Loss= 0.053186316\n",
      "Training [5527/10000] ..........5527 ) Loss= 0.09142255\n",
      "Training [5528/10000] ..........5528 ) Loss= 0.11711324\n",
      "Training [5529/10000] ..........5529 ) Loss= 0.20828366\n",
      "Training [5530/10000] ..........5530 ) Loss= 0.106102504\n",
      "Training [5531/10000] ..........5531 ) Loss= 0.10687321\n",
      "Training [5532/10000] ..........5532 ) Loss= 0.06033307\n",
      "Training [5533/10000] ..........5533 ) Loss= 0.14622758\n",
      "Training [5534/10000] ..........5534 ) Loss= 0.062979676\n",
      "Training [5535/10000] ..........5535 ) Loss= 0.08956503\n",
      "Training [5536/10000] ..........5536 ) Loss= 0.11796285\n",
      "Training [5537/10000] ..........5537 ) Loss= 0.17219323\n",
      "Training [5538/10000] ..........5538 ) Loss= 0.13265967\n",
      "Training [5539/10000] ..........5539 ) Loss= 0.094077036\n",
      "Training [5540/10000] ..........5540 ) Loss= 0.19064087\n",
      "Training [5541/10000] ..........5541 ) Loss= 0.090829216\n",
      "Training [5542/10000] ..........5542 ) Loss= 0.09728031\n",
      "Training [5543/10000] ..........5543 ) Loss= 0.055402312\n",
      "Training [5544/10000] ..........5544 ) Loss= 0.09348914\n",
      "Training [5545/10000] ..........5545 ) Loss= 0.077748924\n",
      "Training [5546/10000] ..........5546 ) Loss= 0.11650126\n",
      "Training [5547/10000] ..........5547 ) Loss= 0.050752524\n",
      "Training [5548/10000] ..........5548 ) Loss= 0.110875554\n",
      "Training [5549/10000] ..........5549 ) Loss= 0.07800109\n",
      "Training [5550/10000] ..........5550 ) Loss= 0.07628707\n",
      "Training [5551/10000] ..........5551 ) Loss= 0.11872585\n",
      "Training [5552/10000] ..........5552 ) Loss= 0.1274686\n",
      "Training [5553/10000] ..........5553 ) Loss= 0.055889178\n",
      "Training [5554/10000] ..........5554 ) Loss= 0.14606878\n",
      "Training [5555/10000] ..........5555 ) Loss= 0.092094235\n",
      "Training [5556/10000] ..........5556 ) Loss= 0.11235752\n",
      "Training [5557/10000] ..........5557 ) Loss= 0.10437406\n",
      "Training [5558/10000] ..........5558 ) Loss= 0.0898817\n",
      "Training [5559/10000] ..........5559 ) Loss= 0.07265096\n",
      "Training [5560/10000] ..........5560 ) Loss= 0.099168636\n",
      "Training [5561/10000] ..........5561 ) Loss= 0.05609798\n",
      "Training [5562/10000] ..........5562 ) Loss= 0.100281395\n",
      "Training [5563/10000] ..........5563 ) Loss= 0.06309929\n",
      "Training [5564/10000] ..........5564 ) Loss= 0.08518902\n",
      "Training [5565/10000] ..........5565 ) Loss= 0.10523773\n",
      "Training [5566/10000] ..........5566 ) Loss= 0.12890315\n",
      "Training [5567/10000] ..........5567 ) Loss= 0.22054811\n",
      "Training [5568/10000] ..........5568 ) Loss= 0.11723669\n",
      "Training [5569/10000] ..........5569 ) Loss= 0.08588112\n",
      "Training [5570/10000] ..........5570 ) Loss= 0.09420752\n",
      "Training [5571/10000] ..........5571 ) Loss= 0.10280395\n",
      "Training [5572/10000] ..........5572 ) Loss= 0.08884507\n",
      "Training [5573/10000] ..........5573 ) Loss= 0.067198865\n",
      "Training [5574/10000] ..........5574 ) Loss= 0.16886589\n",
      "Training [5575/10000] ..........5575 ) Loss= 0.044912938\n",
      "Training [5576/10000] ..........5576 ) Loss= 0.087444164\n",
      "Training [5577/10000] ..........5577 ) Loss= 0.114797704\n",
      "Training [5578/10000] ..........5578 ) Loss= 0.046771135\n",
      "Training [5579/10000] ..........5579 ) Loss= 0.332597\n",
      "Training [5580/10000] ..........5580 ) Loss= 0.121532224\n",
      "Training [5581/10000] ..........5581 ) Loss= 0.16605207\n",
      "Training [5582/10000] ..........5582 ) Loss= 0.17899114\n",
      "Training [5583/10000] ..........5583 ) Loss= 0.22471392\n",
      "Training [5584/10000] ..........5584 ) Loss= 0.10719852\n",
      "Training [5585/10000] ..........5585 ) Loss= 0.07488932\n",
      "Training [5586/10000] ..........5586 ) Loss= 0.06978606\n",
      "Training [5587/10000] ..........5587 ) Loss= 0.16338047\n",
      "Training [5588/10000] ..........5588 ) Loss= 0.096846886\n",
      "Training [5589/10000] ..........5589 ) Loss= 0.106216766\n",
      "Training [5590/10000] ..........5590 ) Loss= 0.068752825\n",
      "Training [5591/10000] ..........5591 ) Loss= 0.11004709\n",
      "Training [5592/10000] ..........5592 ) Loss= 0.06513489\n",
      "Training [5593/10000] ..........5593 ) Loss= 0.12413381\n",
      "Training [5594/10000] ..........5594 ) Loss= 0.088411555\n",
      "Training [5595/10000] ..........5595 ) Loss= 0.052184653\n",
      "Training [5596/10000] ..........5596 ) Loss= 0.07455723\n",
      "Training [5597/10000] ..........5597 ) Loss= 0.09227345\n",
      "Training [5598/10000] ..........5598 ) Loss= 0.1513895\n",
      "Training [5599/10000] ..........5599 ) Loss= 0.15805651\n",
      "Training [5600/10000] ..........5600 ) Loss= 0.14319493\n",
      "Training [5601/10000] ..........5601 ) Loss= 0.05997899\n",
      "Training [5602/10000] ..........5602 ) Loss= 0.09608577\n",
      "Training [5603/10000] ..........5603 ) Loss= 0.08194414\n",
      "Training [5604/10000] ..........5604 ) Loss= 0.10272172\n",
      "Training [5605/10000] ..........5605 ) Loss= 0.07155971\n",
      "Training [5606/10000] ..........5606 ) Loss= 0.09590854\n",
      "Training [5607/10000] ..........5607 ) Loss= 0.30420747\n",
      "Training [5608/10000] ..........5608 ) Loss= 0.17735353\n",
      "Training [5609/10000] ..........5609 ) Loss= 0.08991287\n",
      "Training [5610/10000] ..........5610 ) Loss= 0.088877015\n",
      "Training [5611/10000] ..........5611 ) Loss= 0.072221965\n",
      "Training [5612/10000] ..........5612 ) Loss= 0.10579474\n",
      "Training [5613/10000] ..........5613 ) Loss= 0.06504318\n",
      "Training [5614/10000] ..........5614 ) Loss= 0.07786887\n",
      "Training [5615/10000] ..........5615 ) Loss= 0.08982978\n",
      "Training [5616/10000] ..........5616 ) Loss= 0.099227205\n",
      "Training [5617/10000] ..........5617 ) Loss= 0.09565723\n",
      "Training [5618/10000] ..........5618 ) Loss= 0.16205071\n",
      "Training [5619/10000] ..........5619 ) Loss= 0.049848936\n",
      "Training [5620/10000] ..........5620 ) Loss= 0.13092893\n",
      "Training [5621/10000] ..........5621 ) Loss= 0.09052558\n",
      "Training [5622/10000] ..........5622 ) Loss= 0.097555906\n",
      "Training [5623/10000] ..........5623 ) Loss= 0.17486914\n",
      "Training [5624/10000] ..........5624 ) Loss= 0.08394725\n",
      "Training [5625/10000] ..........5625 ) Loss= 0.07378548\n",
      "Training [5626/10000] ..........5626 ) Loss= 0.073408626\n",
      "Training [5627/10000] ..........5627 ) Loss= 0.088596955\n",
      "Training [5628/10000] ..........5628 ) Loss= 0.0814245\n",
      "Training [5629/10000] ..........5629 ) Loss= 0.13205571\n",
      "Training [5630/10000] ..........5630 ) Loss= 0.08566045\n",
      "Training [5631/10000] ..........5631 ) Loss= 0.1387085\n",
      "Training [5632/10000] ..........5632 ) Loss= 0.15702933\n",
      "Training [5633/10000] ..........5633 ) Loss= 0.17936762\n",
      "Training [5634/10000] ..........5634 ) Loss= 0.07839421\n",
      "Training [5635/10000] ..........5635 ) Loss= 0.09577874\n",
      "Training [5636/10000] ..........5636 ) Loss= 0.15240306\n",
      "Training [5637/10000] ..........5637 ) Loss= 0.0743433\n",
      "Training [5638/10000] ..........5638 ) Loss= 0.1390938\n",
      "Training [5639/10000] ..........5639 ) Loss= 0.08553578\n",
      "Training [5640/10000] ..........5640 ) Loss= 0.09484329\n",
      "Training [5641/10000] ..........5641 ) Loss= 0.06185947\n",
      "Training [5642/10000] ..........5642 ) Loss= 0.096309446\n",
      "Training [5643/10000] ..........5643 ) Loss= 0.13705547\n",
      "Training [5644/10000] ..........5644 ) Loss= 0.18088803\n",
      "Training [5645/10000] ..........5645 ) Loss= 0.09027875\n",
      "Training [5646/10000] ..........5646 ) Loss= 0.12017008\n",
      "Training [5647/10000] ..........5647 ) Loss= 0.08613928\n",
      "Training [5648/10000] ..........5648 ) Loss= 0.2539745\n",
      "Training [5649/10000] ..........5649 ) Loss= 0.07936217\n",
      "Training [5650/10000] ..........5650 ) Loss= 0.24876496\n",
      "Training [5651/10000] ..........5651 ) Loss= 0.16696659\n",
      "Training [5652/10000] ..........5652 ) Loss= 0.09574035\n",
      "Training [5653/10000] ..........5653 ) Loss= 0.08914359\n",
      "Training [5654/10000] ..........5654 ) Loss= 0.05808025\n",
      "Training [5655/10000] ..........5655 ) Loss= 0.13116618\n",
      "Training [5656/10000] ..........5656 ) Loss= 0.06823839\n",
      "Training [5657/10000] ..........5657 ) Loss= 0.07601983\n",
      "Training [5658/10000] ..........5658 ) Loss= 0.08986419\n",
      "Training [5659/10000] ..........5659 ) Loss= 0.102843836\n",
      "Training [5660/10000] ..........5660 ) Loss= 0.10269089\n",
      "Training [5661/10000] ..........5661 ) Loss= 0.078583315\n",
      "Training [5662/10000] ..........5662 ) Loss= 0.07944827\n",
      "Training [5663/10000] ..........5663 ) Loss= 0.18423754\n",
      "Training [5664/10000] ..........5664 ) Loss= 0.12989037\n",
      "Training [5665/10000] ..........5665 ) Loss= 0.16229942\n",
      "Training [5666/10000] ..........5666 ) Loss= 0.113186955\n",
      "Training [5667/10000] ..........5667 ) Loss= 0.06647968\n",
      "Training [5668/10000] ..........5668 ) Loss= 0.05125201\n",
      "Training [5669/10000] ..........5669 ) Loss= 0.096676394\n",
      "Training [5670/10000] ..........5670 ) Loss= 0.065112084\n",
      "Training [5671/10000] ..........5671 ) Loss= 0.06963597\n",
      "Training [5672/10000] ..........5672 ) Loss= 0.15126003\n",
      "Training [5673/10000] ..........5673 ) Loss= 0.05697267\n",
      "Training [5674/10000] ..........5674 ) Loss= 0.09723521\n",
      "Training [5675/10000] ..........5675 ) Loss= 0.075110145\n",
      "Training [5676/10000] ..........5676 ) Loss= 0.069670804\n",
      "Training [5677/10000] ..........5677 ) Loss= 0.08433303\n",
      "Training [5678/10000] ..........5678 ) Loss= 0.06217825\n",
      "Training [5679/10000] ..........5679 ) Loss= 0.093537144\n",
      "Training [5680/10000] ..........5680 ) Loss= 0.07358662\n",
      "Training [5681/10000] ..........5681 ) Loss= 0.07299125\n",
      "Training [5682/10000] ..........5682 ) Loss= 0.111215666\n",
      "Training [5683/10000] ..........5683 ) Loss= 0.05654651\n",
      "Training [5684/10000] ..........5684 ) Loss= 0.09057138\n",
      "Training [5685/10000] ..........5685 ) Loss= 0.09149143\n",
      "Training [5686/10000] ..........5686 ) Loss= 0.06674915\n",
      "Training [5687/10000] ..........5687 ) Loss= 0.1967737\n",
      "Training [5688/10000] ..........5688 ) Loss= 0.08407347\n",
      "Training [5689/10000] ..........5689 ) Loss= 0.10400136\n",
      "Training [5690/10000] ..........5690 ) Loss= 0.13102742\n",
      "Training [5691/10000] ..........5691 ) Loss= 0.08854348\n",
      "Training [5692/10000] ..........5692 ) Loss= 0.13542002\n",
      "Training [5693/10000] ..........5693 ) Loss= 0.12855153\n",
      "Training [5694/10000] ..........5694 ) Loss= 0.08259219\n",
      "Training [5695/10000] ..........5695 ) Loss= 0.14069408\n",
      "Training [5696/10000] ..........5696 ) Loss= 0.06992282\n",
      "Training [5697/10000] ..........5697 ) Loss= 0.12963115\n",
      "Training [5698/10000] ..........5698 ) Loss= 0.13336551\n",
      "Training [5699/10000] ..........5699 ) Loss= 0.13989766\n",
      "Training [5700/10000] ..........5700 ) Loss= 0.080951616\n",
      "Training [5701/10000] ..........5701 ) Loss= 0.114401825\n",
      "Training [5702/10000] ..........5702 ) Loss= 0.27499238\n",
      "Training [5703/10000] ..........5703 ) Loss= 0.07266816\n",
      "Training [5704/10000] ..........5704 ) Loss= 0.059341747\n",
      "Training [5705/10000] ..........5705 ) Loss= 0.2828248\n",
      "Training [5706/10000] ..........5706 ) Loss= 0.1058733\n",
      "Training [5707/10000] ..........5707 ) Loss= 0.06935008\n",
      "Training [5708/10000] ..........5708 ) Loss= 0.07276883\n",
      "Training [5709/10000] ..........5709 ) Loss= 0.09064769\n",
      "Training [5710/10000] ..........5710 ) Loss= 0.071659245\n",
      "Training [5711/10000] ..........5711 ) Loss= 0.119737335\n",
      "Training [5712/10000] ..........5712 ) Loss= 0.07911991\n",
      "Training [5713/10000] ..........5713 ) Loss= 0.23531313\n",
      "Training [5714/10000] ..........5714 ) Loss= 0.06646778\n",
      "Training [5715/10000] ..........5715 ) Loss= 0.0701231\n",
      "Training [5716/10000] ..........5716 ) Loss= 0.2208194\n",
      "Training [5717/10000] ..........5717 ) Loss= 0.11679947\n",
      "Training [5718/10000] ..........5718 ) Loss= 0.09617041\n",
      "Training [5719/10000] ..........5719 ) Loss= 0.083089545\n",
      "Training [5720/10000] ..........5720 ) Loss= 0.107910596\n",
      "Training [5721/10000] ..........5721 ) Loss= 0.07792769\n",
      "Training [5722/10000] ..........5722 ) Loss= 0.093959406\n",
      "Training [5723/10000] ..........5723 ) Loss= 0.06594354\n",
      "Training [5724/10000] ..........5724 ) Loss= 0.14699346\n",
      "Training [5725/10000] ..........5725 ) Loss= 0.09379619\n",
      "Training [5726/10000] ..........5726 ) Loss= 0.08984142\n",
      "Training [5727/10000] ..........5727 ) Loss= 0.14788488\n",
      "Training [5728/10000] ..........5728 ) Loss= 0.13760929\n",
      "Training [5729/10000] ..........5729 ) Loss= 0.058318898\n",
      "Training [5730/10000] ..........5730 ) Loss= 0.08051633\n",
      "Training [5731/10000] ..........5731 ) Loss= 0.09432614\n",
      "Training [5732/10000] ..........5732 ) Loss= 0.059939515\n",
      "Training [5733/10000] ..........5733 ) Loss= 0.049994547\n",
      "Training [5734/10000] ..........5734 ) Loss= 0.10881578\n",
      "Training [5735/10000] ..........5735 ) Loss= 0.105575174\n",
      "Training [5736/10000] ..........5736 ) Loss= 0.071819745\n",
      "Training [5737/10000] ..........5737 ) Loss= 0.10057028\n",
      "Training [5738/10000] ..........5738 ) Loss= 0.057080302\n",
      "Training [5739/10000] ..........5739 ) Loss= 0.08200999\n",
      "Training [5740/10000] ..........5740 ) Loss= 0.09757767\n",
      "Training [5741/10000] ..........5741 ) Loss= 0.09640315\n",
      "Training [5742/10000] ..........5742 ) Loss= 0.07955915\n",
      "Training [5743/10000] ..........5743 ) Loss= 0.08007518\n",
      "Training [5744/10000] ..........5744 ) Loss= 0.08832569\n",
      "Training [5745/10000] ..........5745 ) Loss= 0.11029372\n",
      "Training [5746/10000] ..........5746 ) Loss= 0.17073475\n",
      "Training [5747/10000] ..........5747 ) Loss= 0.11335488\n",
      "Training [5748/10000] ..........5748 ) Loss= 0.057594713\n",
      "Training [5749/10000] ..........5749 ) Loss= 0.070843644\n",
      "Training [5750/10000] ..........5750 ) Loss= 0.08461758\n",
      "Training [5751/10000] ..........5751 ) Loss= 0.057858773\n",
      "Training [5752/10000] ..........5752 ) Loss= 0.081441164\n",
      "Training [5753/10000] ..........5753 ) Loss= 0.14807978\n",
      "Training [5754/10000] ..........5754 ) Loss= 0.13064589\n",
      "Training [5755/10000] ..........5755 ) Loss= 0.10722739\n",
      "Training [5756/10000] ..........5756 ) Loss= 0.074421436\n",
      "Training [5757/10000] ..........5757 ) Loss= 0.1374378\n",
      "Training [5758/10000] ..........5758 ) Loss= 0.06496982\n",
      "Training [5759/10000] ..........5759 ) Loss= 0.0685622\n",
      "Training [5760/10000] ..........5760 ) Loss= 0.12508461\n",
      "Training [5761/10000] ..........5761 ) Loss= 0.106647454\n",
      "Training [5762/10000] ..........5762 ) Loss= 0.07485158\n",
      "Training [5763/10000] ..........5763 ) Loss= 0.061190657\n",
      "Training [5764/10000] ..........5764 ) Loss= 0.11113485\n",
      "Training [5765/10000] ..........5765 ) Loss= 0.09864722\n",
      "Training [5766/10000] ..........5766 ) Loss= 0.20265734\n",
      "Training [5767/10000] ..........5767 ) Loss= 0.080473125\n",
      "Training [5768/10000] ..........5768 ) Loss= 0.09805415\n",
      "Training [5769/10000] ..........5769 ) Loss= 0.07305015\n",
      "Training [5770/10000] ..........5770 ) Loss= 0.08025742\n",
      "Training [5771/10000] ..........5771 ) Loss= 0.15778802\n",
      "Training [5772/10000] ..........5772 ) Loss= 0.05263898\n",
      "Training [5773/10000] ..........5773 ) Loss= 0.08976042\n",
      "Training [5774/10000] ..........5774 ) Loss= 0.07757666\n",
      "Training [5775/10000] ..........5775 ) Loss= 0.11103906\n",
      "Training [5776/10000] ..........5776 ) Loss= 0.13292232\n",
      "Training [5777/10000] ..........5777 ) Loss= 0.1722991\n",
      "Training [5778/10000] ..........5778 ) Loss= 0.165\n",
      "Training [5779/10000] ..........5779 ) Loss= 0.091506876\n",
      "Training [5780/10000] ..........5780 ) Loss= 0.059869163\n",
      "Training [5781/10000] ..........5781 ) Loss= 0.075668074\n",
      "Training [5782/10000] ..........5782 ) Loss= 0.050790053\n",
      "Training [5783/10000] ..........5783 ) Loss= 0.14229336\n",
      "Training [5784/10000] ..........5784 ) Loss= 0.080849394\n",
      "Training [5785/10000] ..........5785 ) Loss= 0.22379686\n",
      "Training [5786/10000] ..........5786 ) Loss= 0.07584029\n",
      "Training [5787/10000] ..........5787 ) Loss= 0.20557687\n",
      "Training [5788/10000] ..........5788 ) Loss= 0.11293179\n",
      "Training [5789/10000] ..........5789 ) Loss= 0.17003247\n",
      "Training [5790/10000] ..........5790 ) Loss= 0.07960565\n",
      "Training [5791/10000] ..........5791 ) Loss= 0.078541465\n",
      "Training [5792/10000] ..........5792 ) Loss= 0.15571709\n",
      "Training [5793/10000] ..........5793 ) Loss= 0.16104521\n",
      "Training [5794/10000] ..........5794 ) Loss= 0.07795257\n",
      "Training [5795/10000] ..........5795 ) Loss= 0.13234928\n",
      "Training [5796/10000] ..........5796 ) Loss= 0.22484905\n",
      "Training [5797/10000] ..........5797 ) Loss= 0.13111286\n",
      "Training [5798/10000] ..........5798 ) Loss= 0.13424793\n",
      "Training [5799/10000] ..........5799 ) Loss= 0.1109391\n",
      "Training [5800/10000] ..........5800 ) Loss= 0.06695466\n",
      "Training [5801/10000] ..........5801 ) Loss= 0.07607394\n",
      "Training [5802/10000] ..........5802 ) Loss= 0.22886208\n",
      "Training [5803/10000] ..........5803 ) Loss= 0.08756797\n",
      "Training [5804/10000] ..........5804 ) Loss= 0.09475215\n",
      "Training [5805/10000] ..........5805 ) Loss= 0.17927971\n",
      "Training [5806/10000] ..........5806 ) Loss= 0.1293466\n",
      "Training [5807/10000] ..........5807 ) Loss= 0.07234225\n",
      "Training [5808/10000] ..........5808 ) Loss= 0.07002514\n",
      "Training [5809/10000] ..........5809 ) Loss= 0.091240145\n",
      "Training [5810/10000] ..........5810 ) Loss= 0.062073715\n",
      "Training [5811/10000] ..........5811 ) Loss= 0.058916938\n",
      "Training [5812/10000] ..........5812 ) Loss= 0.20220403\n",
      "Training [5813/10000] ..........5813 ) Loss= 0.1398223\n",
      "Training [5814/10000] ..........5814 ) Loss= 0.07835467\n",
      "Training [5815/10000] ..........5815 ) Loss= 0.12700716\n",
      "Training [5816/10000] ..........5816 ) Loss= 0.10387259\n",
      "Training [5817/10000] ..........5817 ) Loss= 0.11102447\n",
      "Training [5818/10000] ..........5818 ) Loss= 0.09038887\n",
      "Training [5819/10000] ..........5819 ) Loss= 0.13694227\n",
      "Training [5820/10000] ..........5820 ) Loss= 0.080348566\n",
      "Training [5821/10000] ..........5821 ) Loss= 0.08595879\n",
      "Training [5822/10000] ..........5822 ) Loss= 0.08667573\n",
      "Training [5823/10000] ..........5823 ) Loss= 0.06767065\n",
      "Training [5824/10000] ..........5824 ) Loss= 0.09190573\n",
      "Training [5825/10000] ..........5825 ) Loss= 0.15968083\n",
      "Training [5826/10000] ..........5826 ) Loss= 0.069418944\n",
      "Training [5827/10000] ..........5827 ) Loss= 0.07689426\n",
      "Training [5828/10000] ..........5828 ) Loss= 0.07123415\n",
      "Training [5829/10000] ..........5829 ) Loss= 0.065697215\n",
      "Training [5830/10000] ..........5830 ) Loss= 0.13475238\n",
      "Training [5831/10000] ..........5831 ) Loss= 0.123550326\n",
      "Training [5832/10000] ..........5832 ) Loss= 0.18332085\n",
      "Training [5833/10000] ..........5833 ) Loss= 0.06689523\n",
      "Training [5834/10000] ..........5834 ) Loss= 0.08919327\n",
      "Training [5835/10000] ..........5835 ) Loss= 0.11278603\n",
      "Training [5836/10000] ..........5836 ) Loss= 0.17716065\n",
      "Training [5837/10000] ..........5837 ) Loss= 0.089927904\n",
      "Training [5838/10000] ..........5838 ) Loss= 0.31508154\n",
      "Training [5839/10000] ..........5839 ) Loss= 0.11711939\n",
      "Training [5840/10000] ..........5840 ) Loss= 0.06002255\n",
      "Training [5841/10000] ..........5841 ) Loss= 0.1398687\n",
      "Training [5842/10000] ..........5842 ) Loss= 0.09297119\n",
      "Training [5843/10000] ..........5843 ) Loss= 0.10227285\n",
      "Training [5844/10000] ..........5844 ) Loss= 0.08259439\n",
      "Training [5845/10000] ..........5845 ) Loss= 0.1471878\n",
      "Training [5846/10000] ..........5846 ) Loss= 0.058830608\n",
      "Training [5847/10000] ..........5847 ) Loss= 0.11024079\n",
      "Training [5848/10000] ..........5848 ) Loss= 0.106111266\n",
      "Training [5849/10000] ..........5849 ) Loss= 0.07250363\n",
      "Training [5850/10000] ..........5850 ) Loss= 0.16574362\n",
      "Training [5851/10000] ..........5851 ) Loss= 0.10765974\n",
      "Training [5852/10000] ..........5852 ) Loss= 0.060573585\n",
      "Training [5853/10000] ..........5853 ) Loss= 0.07046572\n",
      "Training [5854/10000] ..........5854 ) Loss= 0.0690729\n",
      "Training [5855/10000] ..........5855 ) Loss= 0.06681499\n",
      "Training [5856/10000] ..........5856 ) Loss= 0.08858507\n",
      "Training [5857/10000] ..........5857 ) Loss= 0.07498054\n",
      "Training [5858/10000] ..........5858 ) Loss= 0.09032958\n",
      "Training [5859/10000] ..........5859 ) Loss= 0.058164116\n",
      "Training [5860/10000] ..........5860 ) Loss= 0.057772283\n",
      "Training [5861/10000] ..........5861 ) Loss= 0.053024776\n",
      "Training [5862/10000] ..........5862 ) Loss= 0.093739256\n",
      "Training [5863/10000] ..........5863 ) Loss= 0.26219267\n",
      "Training [5864/10000] ..........5864 ) Loss= 0.09066492\n",
      "Training [5865/10000] ..........5865 ) Loss= 0.17067559\n",
      "Training [5866/10000] ..........5866 ) Loss= 0.058709174\n",
      "Training [5867/10000] ..........5867 ) Loss= 0.06997038\n",
      "Training [5868/10000] ..........5868 ) Loss= 0.17971002\n",
      "Training [5869/10000] ..........5869 ) Loss= 0.18761967\n",
      "Training [5870/10000] ..........5870 ) Loss= 0.14091131\n",
      "Training [5871/10000] ..........5871 ) Loss= 0.115325116\n",
      "Training [5872/10000] ..........5872 ) Loss= 0.07318569\n",
      "Training [5873/10000] ..........5873 ) Loss= 0.11430923\n",
      "Training [5874/10000] ..........5874 ) Loss= 0.06874157\n",
      "Training [5875/10000] ..........5875 ) Loss= 0.0992383\n",
      "Training [5876/10000] ..........5876 ) Loss= 0.08901106\n",
      "Training [5877/10000] ..........5877 ) Loss= 0.0625887\n",
      "Training [5878/10000] ..........5878 ) Loss= 0.06378798\n",
      "Training [5879/10000] ..........5879 ) Loss= 0.09158481\n",
      "Training [5880/10000] ..........5880 ) Loss= 0.1058069\n",
      "Training [5881/10000] ..........5881 ) Loss= 0.06880274\n",
      "Training [5882/10000] ..........5882 ) Loss= 0.082842365\n",
      "Training [5883/10000] ..........5883 ) Loss= 0.084120184\n",
      "Training [5884/10000] ..........5884 ) Loss= 0.075433336\n",
      "Training [5885/10000] ..........5885 ) Loss= 0.08458922\n",
      "Training [5886/10000] ..........5886 ) Loss= 0.09298861\n",
      "Training [5887/10000] ..........5887 ) Loss= 0.18772885\n",
      "Training [5888/10000] ..........5888 ) Loss= 0.07258596\n",
      "Training [5889/10000] ..........5889 ) Loss= 0.13483322\n",
      "Training [5890/10000] ..........5890 ) Loss= 0.06631127\n",
      "Training [5891/10000] ..........5891 ) Loss= 0.066031426\n",
      "Training [5892/10000] ..........5892 ) Loss= 0.094108075\n",
      "Training [5893/10000] ..........5893 ) Loss= 0.059217498\n",
      "Training [5894/10000] ..........5894 ) Loss= 0.048299674\n",
      "Training [5895/10000] ..........5895 ) Loss= 0.061466817\n",
      "Training [5896/10000] ..........5896 ) Loss= 0.050040152\n",
      "Training [5897/10000] ..........5897 ) Loss= 0.07944575\n",
      "Training [5898/10000] ..........5898 ) Loss= 0.06645291\n",
      "Training [5899/10000] ..........5899 ) Loss= 0.0642377\n",
      "Training [5900/10000] ..........5900 ) Loss= 0.07934282\n",
      "Training [5901/10000] ..........5901 ) Loss= 0.09375458\n",
      "Training [5902/10000] ..........5902 ) Loss= 0.07125614\n",
      "Training [5903/10000] ..........5903 ) Loss= 0.06643522\n",
      "Training [5904/10000] ..........5904 ) Loss= 0.059445374\n",
      "Training [5905/10000] ..........5905 ) Loss= 0.084869236\n",
      "Training [5906/10000] ..........5906 ) Loss= 0.080584906\n",
      "Training [5907/10000] ..........5907 ) Loss= 0.11010574\n",
      "Training [5908/10000] ..........5908 ) Loss= 0.05946036\n",
      "Training [5909/10000] ..........5909 ) Loss= 0.14543825\n",
      "Training [5910/10000] ..........5910 ) Loss= 0.06613125\n",
      "Training [5911/10000] ..........5911 ) Loss= 0.09945702\n",
      "Training [5912/10000] ..........5912 ) Loss= 0.08900195\n",
      "Training [5913/10000] ..........5913 ) Loss= 0.07994484\n",
      "Training [5914/10000] ..........5914 ) Loss= 0.110955596\n",
      "Training [5915/10000] ..........5915 ) Loss= 0.16442639\n",
      "Training [5916/10000] ..........5916 ) Loss= 0.10628688\n",
      "Training [5917/10000] ..........5917 ) Loss= 0.06465576\n",
      "Training [5918/10000] ..........5918 ) Loss= 0.08463833\n",
      "Training [5919/10000] ..........5919 ) Loss= 0.05165593\n",
      "Training [5920/10000] ..........5920 ) Loss= 0.090043604\n",
      "Training [5921/10000] ..........5921 ) Loss= 0.06398079\n",
      "Training [5922/10000] ..........5922 ) Loss= 0.11474109\n",
      "Training [5923/10000] ..........5923 ) Loss= 0.09806048\n",
      "Training [5924/10000] ..........5924 ) Loss= 0.14402744\n",
      "Training [5925/10000] ..........5925 ) Loss= 0.057777125\n",
      "Training [5926/10000] ..........5926 ) Loss= 0.08406254\n",
      "Training [5927/10000] ..........5927 ) Loss= 0.07259454\n",
      "Training [5928/10000] ..........5928 ) Loss= 0.065129876\n",
      "Training [5929/10000] ..........5929 ) Loss= 0.07848849\n",
      "Training [5930/10000] ..........5930 ) Loss= 0.16448173\n",
      "Training [5931/10000] ..........5931 ) Loss= 0.07865629\n",
      "Training [5932/10000] ..........5932 ) Loss= 0.1092291\n",
      "Training [5933/10000] ..........5933 ) Loss= 0.09900239\n",
      "Training [5934/10000] ..........5934 ) Loss= 0.074609265\n",
      "Training [5935/10000] ..........5935 ) Loss= 0.06583499\n",
      "Training [5936/10000] ..........5936 ) Loss= 0.10311035\n",
      "Training [5937/10000] ..........5937 ) Loss= 0.122234754\n",
      "Training [5938/10000] ..........5938 ) Loss= 0.090638936\n",
      "Training [5939/10000] ..........5939 ) Loss= 0.12861696\n",
      "Training [5940/10000] ..........5940 ) Loss= 0.07235675\n",
      "Training [5941/10000] ..........5941 ) Loss= 0.08793722\n",
      "Training [5942/10000] ..........5942 ) Loss= 0.18906167\n",
      "Training [5943/10000] ..........5943 ) Loss= 0.106752075\n",
      "Training [5944/10000] ..........5944 ) Loss= 0.05783619\n",
      "Training [5945/10000] ..........5945 ) Loss= 0.09522202\n",
      "Training [5946/10000] ..........5946 ) Loss= 0.07919444\n",
      "Training [5947/10000] ..........5947 ) Loss= 0.117809504\n",
      "Training [5948/10000] ..........5948 ) Loss= 0.06605023\n",
      "Training [5949/10000] ..........5949 ) Loss= 0.08712334\n",
      "Training [5950/10000] ..........5950 ) Loss= 0.058944456\n",
      "Training [5951/10000] ..........5951 ) Loss= 0.07220797\n",
      "Training [5952/10000] ..........5952 ) Loss= 0.17250822\n",
      "Training [5953/10000] ..........5953 ) Loss= 0.083255045\n",
      "Training [5954/10000] ..........5954 ) Loss= 0.09795585\n",
      "Training [5955/10000] ..........5955 ) Loss= 0.121866226\n",
      "Training [5956/10000] ..........5956 ) Loss= 0.077634156\n",
      "Training [5957/10000] ..........5957 ) Loss= 0.07897784\n",
      "Training [5958/10000] ..........5958 ) Loss= 0.079587966\n",
      "Training [5959/10000] ..........5959 ) Loss= 0.11425364\n",
      "Training [5960/10000] ..........5960 ) Loss= 0.10474765\n",
      "Training [5961/10000] ..........5961 ) Loss= 0.11602799\n",
      "Training [5962/10000] ..........5962 ) Loss= 0.129881\n",
      "Training [5963/10000] ..........5963 ) Loss= 0.09605314\n",
      "Training [5964/10000] ..........5964 ) Loss= 0.09631712\n",
      "Training [5965/10000] ..........5965 ) Loss= 0.10840452\n",
      "Training [5966/10000] ..........5966 ) Loss= 0.12313035\n",
      "Training [5967/10000] ..........5967 ) Loss= 0.09477923\n",
      "Training [5968/10000] ..........5968 ) Loss= 0.10632265\n",
      "Training [5969/10000] ..........5969 ) Loss= 0.12918171\n",
      "Training [5970/10000] ..........5970 ) Loss= 0.07291602\n",
      "Training [5971/10000] ..........5971 ) Loss= 0.1355562\n",
      "Training [5972/10000] ..........5972 ) Loss= 0.06779235\n",
      "Training [5973/10000] ..........5973 ) Loss= 0.06914198\n",
      "Training [5974/10000] ..........5974 ) Loss= 0.06651642\n",
      "Training [5975/10000] ..........5975 ) Loss= 0.07409122\n",
      "Training [5976/10000] ..........5976 ) Loss= 0.11576477\n",
      "Training [5977/10000] ..........5977 ) Loss= 0.1313977\n",
      "Training [5978/10000] ..........5978 ) Loss= 0.12031599\n",
      "Training [5979/10000] ..........5979 ) Loss= 0.183754\n",
      "Training [5980/10000] ..........5980 ) Loss= 0.056686435\n",
      "Training [5981/10000] ..........5981 ) Loss= 0.08129116\n",
      "Training [5982/10000] ..........5982 ) Loss= 0.0455306\n",
      "Training [5983/10000] ..........5983 ) Loss= 0.09780761\n",
      "Training [5984/10000] ..........5984 ) Loss= 0.1677907\n",
      "Training [5985/10000] ..........5985 ) Loss= 0.12977129\n",
      "Training [5986/10000] ..........5986 ) Loss= 0.06935472\n",
      "Training [5987/10000] ..........5987 ) Loss= 0.077913895\n",
      "Training [5988/10000] ..........5988 ) Loss= 0.083639346\n",
      "Training [5989/10000] ..........5989 ) Loss= 0.076174624\n",
      "Training [5990/10000] ..........5990 ) Loss= 0.11496421\n",
      "Training [5991/10000] ..........5991 ) Loss= 0.055534266\n",
      "Training [5992/10000] ..........5992 ) Loss= 0.09824108\n",
      "Training [5993/10000] ..........5993 ) Loss= 0.08469321\n",
      "Training [5994/10000] ..........5994 ) Loss= 0.20446669\n",
      "Training [5995/10000] ..........5995 ) Loss= 0.09095737\n",
      "Training [5996/10000] ..........5996 ) Loss= 0.07304943\n",
      "Training [5997/10000] ..........5997 ) Loss= 0.07550588\n",
      "Training [5998/10000] ..........5998 ) Loss= 0.14151287\n",
      "Training [5999/10000] ..........5999 ) Loss= 0.086878434\n",
      "Training [6000/10000] ..........6000 ) Loss= 0.059398536 - Saving Model6000.torch\n",
      "Training [6001/10000] ..........6001 ) Loss= 0.06571698\n",
      "Training [6002/10000] ..........6002 ) Loss= 0.05122922\n",
      "Training [6003/10000] ..........6003 ) Loss= 0.32318816\n",
      "Training [6004/10000] ..........6004 ) Loss= 0.073147535\n",
      "Training [6005/10000] ..........6005 ) Loss= 0.07930909\n",
      "Training [6006/10000] ..........6006 ) Loss= 0.08198475\n",
      "Training [6007/10000] ..........6007 ) Loss= 0.11346592\n",
      "Training [6008/10000] ..........6008 ) Loss= 0.102408595\n",
      "Training [6009/10000] ..........6009 ) Loss= 0.18797924\n",
      "Training [6010/10000] ..........6010 ) Loss= 0.11663605\n",
      "Training [6011/10000] ..........6011 ) Loss= 0.06364743\n",
      "Training [6012/10000] ..........6012 ) Loss= 0.07447213\n",
      "Training [6013/10000] ..........6013 ) Loss= 0.14217941\n",
      "Training [6014/10000] ..........6014 ) Loss= 0.070206024\n",
      "Training [6015/10000] ..........6015 ) Loss= 0.10607939\n",
      "Training [6016/10000] ..........6016 ) Loss= 0.08512805\n",
      "Training [6017/10000] ..........6017 ) Loss= 0.1241352\n",
      "Training [6018/10000] ..........6018 ) Loss= 0.08436368\n",
      "Training [6019/10000] ..........6019 ) Loss= 0.06935367\n",
      "Training [6020/10000] ..........6020 ) Loss= 0.08445318\n",
      "Training [6021/10000] ..........6021 ) Loss= 0.09625863\n",
      "Training [6022/10000] ..........6022 ) Loss= 0.059732474\n",
      "Training [6023/10000] ..........6023 ) Loss= 0.054128807\n",
      "Training [6024/10000] ..........6024 ) Loss= 0.05481271\n",
      "Training [6025/10000] ..........6025 ) Loss= 0.07854077\n",
      "Training [6026/10000] ..........6026 ) Loss= 0.12810668\n",
      "Training [6027/10000] ..........6027 ) Loss= 0.066500574\n",
      "Training [6028/10000] ..........6028 ) Loss= 0.12080734\n",
      "Training [6029/10000] ..........6029 ) Loss= 0.101536945\n",
      "Training [6030/10000] ..........6030 ) Loss= 0.12776545\n",
      "Training [6031/10000] ..........6031 ) Loss= 0.10840045\n",
      "Training [6032/10000] ..........6032 ) Loss= 0.08254513\n",
      "Training [6033/10000] ..........6033 ) Loss= 0.058405984\n",
      "Training [6034/10000] ..........6034 ) Loss= 0.13648102\n",
      "Training [6035/10000] ..........6035 ) Loss= 0.108020194\n",
      "Training [6036/10000] ..........6036 ) Loss= 0.08273965\n",
      "Training [6037/10000] ..........6037 ) Loss= 0.17135976\n",
      "Training [6038/10000] ..........6038 ) Loss= 0.06061516\n",
      "Training [6039/10000] ..........6039 ) Loss= 0.12881155\n",
      "Training [6040/10000] ..........6040 ) Loss= 0.1645598\n",
      "Training [6041/10000] ..........6041 ) Loss= 0.062002987\n",
      "Training [6042/10000] ..........6042 ) Loss= 0.0845583\n",
      "Training [6043/10000] ..........6043 ) Loss= 0.04839873\n",
      "Training [6044/10000] ..........6044 ) Loss= 0.076008886\n",
      "Training [6045/10000] ..........6045 ) Loss= 0.124426365\n",
      "Training [6046/10000] ..........6046 ) Loss= 0.09125541\n",
      "Training [6047/10000] ..........6047 ) Loss= 0.080845915\n",
      "Training [6048/10000] ..........6048 ) Loss= 0.09398677\n",
      "Training [6049/10000] ..........6049 ) Loss= 0.09053027\n",
      "Training [6050/10000] ..........6050 ) Loss= 0.11577337\n",
      "Training [6051/10000] ..........6051 ) Loss= 0.09466714\n",
      "Training [6052/10000] ..........6052 ) Loss= 0.14259169\n",
      "Training [6053/10000] ..........6053 ) Loss= 0.068202935\n",
      "Training [6054/10000] ..........6054 ) Loss= 0.06201451\n",
      "Training [6055/10000] ..........6055 ) Loss= 0.10309128\n",
      "Training [6056/10000] ..........6056 ) Loss= 0.109570906\n",
      "Training [6057/10000] ..........6057 ) Loss= 0.13415127\n",
      "Training [6058/10000] ..........6058 ) Loss= 0.11845087\n",
      "Training [6059/10000] ..........6059 ) Loss= 0.061062094\n",
      "Training [6060/10000] ..........6060 ) Loss= 0.11187548\n",
      "Training [6061/10000] ..........6061 ) Loss= 0.1150166\n",
      "Training [6062/10000] ..........6062 ) Loss= 0.055959716\n",
      "Training [6063/10000] ..........6063 ) Loss= 0.110462904\n",
      "Training [6064/10000] ..........6064 ) Loss= 0.17067114\n",
      "Training [6065/10000] ..........6065 ) Loss= 0.06105258\n",
      "Training [6066/10000] ..........6066 ) Loss= 0.09862396\n",
      "Training [6067/10000] ..........6067 ) Loss= 0.04483419\n",
      "Training [6068/10000] ..........6068 ) Loss= 0.091113895\n",
      "Training [6069/10000] ..........6069 ) Loss= 0.0874904\n",
      "Training [6070/10000] ..........6070 ) Loss= 0.11519029\n",
      "Training [6071/10000] ..........6071 ) Loss= 0.081090614\n",
      "Training [6072/10000] ..........6072 ) Loss= 0.074646495\n",
      "Training [6073/10000] ..........6073 ) Loss= 0.09427231\n",
      "Training [6074/10000] ..........6074 ) Loss= 0.0675729\n",
      "Training [6075/10000] ..........6075 ) Loss= 0.14227559\n",
      "Training [6076/10000] ..........6076 ) Loss= 0.096278764\n",
      "Training [6077/10000] ..........6077 ) Loss= 0.105736226\n",
      "Training [6078/10000] ..........6078 ) Loss= 0.0841952\n",
      "Training [6079/10000] ..........6079 ) Loss= 0.0589871\n",
      "Training [6080/10000] ..........6080 ) Loss= 0.105196066\n",
      "Training [6081/10000] ..........6081 ) Loss= 0.04098802\n",
      "Training [6082/10000] ..........6082 ) Loss= 0.096073575\n",
      "Training [6083/10000] ..........6083 ) Loss= 0.063606076\n",
      "Training [6084/10000] ..........6084 ) Loss= 0.17115763\n",
      "Training [6085/10000] ..........6085 ) Loss= 0.10970003\n",
      "Training [6086/10000] ..........6086 ) Loss= 0.08261295\n",
      "Training [6087/10000] ..........6087 ) Loss= 0.059352685\n",
      "Training [6088/10000] ..........6088 ) Loss= 0.08107217\n",
      "Training [6089/10000] ..........6089 ) Loss= 0.12014302\n",
      "Training [6090/10000] ..........6090 ) Loss= 0.07987529\n",
      "Training [6091/10000] ..........6091 ) Loss= 0.09397613\n",
      "Training [6092/10000] ..........6092 ) Loss= 0.08214409\n",
      "Training [6093/10000] ..........6093 ) Loss= 0.064418666\n",
      "Training [6094/10000] ..........6094 ) Loss= 0.082012385\n",
      "Training [6095/10000] ..........6095 ) Loss= 0.05601854\n",
      "Training [6096/10000] ..........6096 ) Loss= 0.09104892\n",
      "Training [6097/10000] ..........6097 ) Loss= 0.2912265\n",
      "Training [6098/10000] ..........6098 ) Loss= 0.084288135\n",
      "Training [6099/10000] ..........6099 ) Loss= 0.06273903\n",
      "Training [6100/10000] ..........6100 ) Loss= 0.09407484\n",
      "Training [6101/10000] ..........6101 ) Loss= 0.049624544\n",
      "Training [6102/10000] ..........6102 ) Loss= 0.12288158\n",
      "Training [6103/10000] ..........6103 ) Loss= 0.06058604\n",
      "Training [6104/10000] ..........6104 ) Loss= 0.109327875\n",
      "Training [6105/10000] ..........6105 ) Loss= 0.063588515\n",
      "Training [6106/10000] ..........6106 ) Loss= 0.05156065\n",
      "Training [6107/10000] ..........6107 ) Loss= 0.09935363\n",
      "Training [6108/10000] ..........6108 ) Loss= 0.1493058\n",
      "Training [6109/10000] ..........6109 ) Loss= 0.19904903\n",
      "Training [6110/10000] ..........6110 ) Loss= 0.11307874\n",
      "Training [6111/10000] ..........6111 ) Loss= 0.109881386\n",
      "Training [6112/10000] ..........6112 ) Loss= 0.14337058\n",
      "Training [6113/10000] ..........6113 ) Loss= 0.06496444\n",
      "Training [6114/10000] ..........6114 ) Loss= 0.0883972\n",
      "Training [6115/10000] ..........6115 ) Loss= 0.06315016\n",
      "Training [6116/10000] ..........6116 ) Loss= 0.07243951\n",
      "Training [6117/10000] ..........6117 ) Loss= 0.06962894\n",
      "Training [6118/10000] ..........6118 ) Loss= 0.059733346\n",
      "Training [6119/10000] ..........6119 ) Loss= 0.10896509\n",
      "Training [6120/10000] ..........6120 ) Loss= 0.18007343\n",
      "Training [6121/10000] ..........6121 ) Loss= 0.14224954\n",
      "Training [6122/10000] ..........6122 ) Loss= 0.10296608\n",
      "Training [6123/10000] ..........6123 ) Loss= 0.041190542\n",
      "Training [6124/10000] ..........6124 ) Loss= 0.09771949\n",
      "Training [6125/10000] ..........6125 ) Loss= 0.09608791\n",
      "Training [6126/10000] ..........6126 ) Loss= 0.14673226\n",
      "Training [6127/10000] ..........6127 ) Loss= 0.07238367\n",
      "Training [6128/10000] ..........6128 ) Loss= 0.113164455\n",
      "Training [6129/10000] ..........6129 ) Loss= 0.12777315\n",
      "Training [6130/10000] ..........6130 ) Loss= 0.12727192\n",
      "Training [6131/10000] ..........6131 ) Loss= 0.1107275\n",
      "Training [6132/10000] ..........6132 ) Loss= 0.07211032\n",
      "Training [6133/10000] ..........6133 ) Loss= 0.059700496\n",
      "Training [6134/10000] ..........6134 ) Loss= 0.08412121\n",
      "Training [6135/10000] ..........6135 ) Loss= 0.0719262\n",
      "Training [6136/10000] ..........6136 ) Loss= 0.12466825\n",
      "Training [6137/10000] ..........6137 ) Loss= 0.07451168\n",
      "Training [6138/10000] ..........6138 ) Loss= 0.06864619\n",
      "Training [6139/10000] ..........6139 ) Loss= 0.1242595\n",
      "Training [6140/10000] ..........6140 ) Loss= 0.07463457\n",
      "Training [6141/10000] ..........6141 ) Loss= 0.08264035\n",
      "Training [6142/10000] ..........6142 ) Loss= 0.08147992\n",
      "Training [6143/10000] ..........6143 ) Loss= 0.07446393\n",
      "Training [6144/10000] ..........6144 ) Loss= 0.109149665\n",
      "Training [6145/10000] ..........6145 ) Loss= 0.08316727\n",
      "Training [6146/10000] ..........6146 ) Loss= 0.12852834\n",
      "Training [6147/10000] ..........6147 ) Loss= 0.09712126\n",
      "Training [6148/10000] ..........6148 ) Loss= 0.09326252\n",
      "Training [6149/10000] ..........6149 ) Loss= 0.09281381\n",
      "Training [6150/10000] ..........6150 ) Loss= 0.0752755\n",
      "Training [6151/10000] ..........6151 ) Loss= 0.11167781\n",
      "Training [6152/10000] ..........6152 ) Loss= 0.08448782\n",
      "Training [6153/10000] ..........6153 ) Loss= 0.09507352\n",
      "Training [6154/10000] ..........6154 ) Loss= 0.06887463\n",
      "Training [6155/10000] ..........6155 ) Loss= 0.0672631\n",
      "Training [6156/10000] ..........6156 ) Loss= 0.063381374\n",
      "Training [6157/10000] ..........6157 ) Loss= 0.052975796\n",
      "Training [6158/10000] ..........6158 ) Loss= 0.085519984\n",
      "Training [6159/10000] ..........6159 ) Loss= 0.055905286\n",
      "Training [6160/10000] ..........6160 ) Loss= 0.08096004\n",
      "Training [6161/10000] ..........6161 ) Loss= 0.09029434\n",
      "Training [6162/10000] ..........6162 ) Loss= 0.09839061\n",
      "Training [6163/10000] ..........6163 ) Loss= 0.2506148\n",
      "Training [6164/10000] ..........6164 ) Loss= 0.06884252\n",
      "Training [6165/10000] ..........6165 ) Loss= 0.11597111\n",
      "Training [6166/10000] ..........6166 ) Loss= 0.18850553\n",
      "Training [6167/10000] ..........6167 ) Loss= 0.07126589\n",
      "Training [6168/10000] ..........6168 ) Loss= 0.10557434\n",
      "Training [6169/10000] ..........6169 ) Loss= 0.04390864\n",
      "Training [6170/10000] ..........6170 ) Loss= 0.117498964\n",
      "Training [6171/10000] ..........6171 ) Loss= 0.08704106\n",
      "Training [6172/10000] ..........6172 ) Loss= 0.07028728\n",
      "Training [6173/10000] ..........6173 ) Loss= 0.05802972\n",
      "Training [6174/10000] ..........6174 ) Loss= 0.06259686\n",
      "Training [6175/10000] ..........6175 ) Loss= 0.05391839\n",
      "Training [6176/10000] ..........6176 ) Loss= 0.09643341\n",
      "Training [6177/10000] ..........6177 ) Loss= 0.11642018\n",
      "Training [6178/10000] ..........6178 ) Loss= 0.08405329\n",
      "Training [6179/10000] ..........6179 ) Loss= 0.14363535\n",
      "Training [6180/10000] ..........6180 ) Loss= 0.15367503\n",
      "Training [6181/10000] ..........6181 ) Loss= 0.08523885\n",
      "Training [6182/10000] ..........6182 ) Loss= 0.07792651\n",
      "Training [6183/10000] ..........6183 ) Loss= 0.058525007\n",
      "Training [6184/10000] ..........6184 ) Loss= 0.061438948\n",
      "Training [6185/10000] ..........6185 ) Loss= 0.08668458\n",
      "Training [6186/10000] ..........6186 ) Loss= 0.06913408\n",
      "Training [6187/10000] ..........6187 ) Loss= 0.10874656\n",
      "Training [6188/10000] ..........6188 ) Loss= 0.065008335\n",
      "Training [6189/10000] ..........6189 ) Loss= 0.13109836\n",
      "Training [6190/10000] ..........6190 ) Loss= 0.13121839\n",
      "Training [6191/10000] ..........6191 ) Loss= 0.11158017\n",
      "Training [6192/10000] ..........6192 ) Loss= 0.10790459\n",
      "Training [6193/10000] ..........6193 ) Loss= 0.09325373\n",
      "Training [6194/10000] ..........6194 ) Loss= 0.046374314\n",
      "Training [6195/10000] ..........6195 ) Loss= 0.09867389\n",
      "Training [6196/10000] ..........6196 ) Loss= 0.074259795\n",
      "Training [6197/10000] ..........6197 ) Loss= 0.0646476\n",
      "Training [6198/10000] ..........6198 ) Loss= 0.072581574\n",
      "Training [6199/10000] ..........6199 ) Loss= 0.09308154\n",
      "Training [6200/10000] ..........6200 ) Loss= 0.05004769\n",
      "Training [6201/10000] ..........6201 ) Loss= 0.11430221\n",
      "Training [6202/10000] ..........6202 ) Loss= 0.07624066\n",
      "Training [6203/10000] ..........6203 ) Loss= 0.079571314\n",
      "Training [6204/10000] ..........6204 ) Loss= 0.07790612\n",
      "Training [6205/10000] ..........6205 ) Loss= 0.076759435\n",
      "Training [6206/10000] ..........6206 ) Loss= 0.2786951\n",
      "Training [6207/10000] ..........6207 ) Loss= 0.065782264\n",
      "Training [6208/10000] ..........6208 ) Loss= 0.098486006\n",
      "Training [6209/10000] ..........6209 ) Loss= 0.09987476\n",
      "Training [6210/10000] ..........6210 ) Loss= 0.057812687\n",
      "Training [6211/10000] ..........6211 ) Loss= 0.11105024\n",
      "Training [6212/10000] ..........6212 ) Loss= 0.13947184\n",
      "Training [6213/10000] ..........6213 ) Loss= 0.07374815\n",
      "Training [6214/10000] ..........6214 ) Loss= 0.10184585\n",
      "Training [6215/10000] ..........6215 ) Loss= 0.07683351\n",
      "Training [6216/10000] ..........6216 ) Loss= 0.048023496\n",
      "Training [6217/10000] ..........6217 ) Loss= 0.06288554\n",
      "Training [6218/10000] ..........6218 ) Loss= 0.12419398\n",
      "Training [6219/10000] ..........6219 ) Loss= 0.059136603\n",
      "Training [6220/10000] ..........6220 ) Loss= 0.045603756\n",
      "Training [6221/10000] ..........6221 ) Loss= 0.053483654\n",
      "Training [6222/10000] ..........6222 ) Loss= 0.10714727\n",
      "Training [6223/10000] ..........6223 ) Loss= 0.06861184\n",
      "Training [6224/10000] ..........6224 ) Loss= 0.10858209\n",
      "Training [6225/10000] ..........6225 ) Loss= 0.059389826\n",
      "Training [6226/10000] ..........6226 ) Loss= 0.09925771\n",
      "Training [6227/10000] ..........6227 ) Loss= 0.085671015\n",
      "Training [6228/10000] ..........6228 ) Loss= 0.045181435\n",
      "Training [6229/10000] ..........6229 ) Loss= 0.07984613\n",
      "Training [6230/10000] ..........6230 ) Loss= 0.105615534\n",
      "Training [6231/10000] ..........6231 ) Loss= 0.06496986\n",
      "Training [6232/10000] ..........6232 ) Loss= 0.13901357\n",
      "Training [6233/10000] ..........6233 ) Loss= 0.058697134\n",
      "Training [6234/10000] ..........6234 ) Loss= 0.13467336\n",
      "Training [6235/10000] ..........6235 ) Loss= 0.06716161\n",
      "Training [6236/10000] ..........6236 ) Loss= 0.23756787\n",
      "Training [6237/10000] ..........6237 ) Loss= 0.061802603\n",
      "Training [6238/10000] ..........6238 ) Loss= 0.0686232\n",
      "Training [6239/10000] ..........6239 ) Loss= 0.07216747\n",
      "Training [6240/10000] ..........6240 ) Loss= 0.106097214\n",
      "Training [6241/10000] ..........6241 ) Loss= 0.06964051\n",
      "Training [6242/10000] ..........6242 ) Loss= 0.16115785\n",
      "Training [6243/10000] ..........6243 ) Loss= 0.04254527\n",
      "Training [6244/10000] ..........6244 ) Loss= 0.06462856\n",
      "Training [6245/10000] ..........6245 ) Loss= 0.10991831\n",
      "Training [6246/10000] ..........6246 ) Loss= 0.077833466\n",
      "Training [6247/10000] ..........6247 ) Loss= 0.10119793\n",
      "Training [6248/10000] ..........6248 ) Loss= 0.075769156\n",
      "Training [6249/10000] ..........6249 ) Loss= 0.087150216\n",
      "Training [6250/10000] ..........6250 ) Loss= 0.07146922\n",
      "Training [6251/10000] ..........6251 ) Loss= 0.082815014\n",
      "Training [6252/10000] ..........6252 ) Loss= 0.14700729\n",
      "Training [6253/10000] ..........6253 ) Loss= 0.12715296\n",
      "Training [6254/10000] ..........6254 ) Loss= 0.070810154\n",
      "Training [6255/10000] ..........6255 ) Loss= 0.06258377\n",
      "Training [6256/10000] ..........6256 ) Loss= 0.07583563\n",
      "Training [6257/10000] ..........6257 ) Loss= 0.07353283\n",
      "Training [6258/10000] ..........6258 ) Loss= 0.08611748\n",
      "Training [6259/10000] ..........6259 ) Loss= 0.067915514\n",
      "Training [6260/10000] ..........6260 ) Loss= 0.076407276\n",
      "Training [6261/10000] ..........6261 ) Loss= 0.07889391\n",
      "Training [6262/10000] ..........6262 ) Loss= 0.07948904\n",
      "Training [6263/10000] ..........6263 ) Loss= 0.07354614\n",
      "Training [6264/10000] ..........6264 ) Loss= 0.085288465\n",
      "Training [6265/10000] ..........6265 ) Loss= 0.05521703\n",
      "Training [6266/10000] ..........6266 ) Loss= 0.15947478\n",
      "Training [6267/10000] ..........6267 ) Loss= 0.08231456\n",
      "Training [6268/10000] ..........6268 ) Loss= 0.078201175\n",
      "Training [6269/10000] ..........6269 ) Loss= 0.109952554\n",
      "Training [6270/10000] ..........6270 ) Loss= 0.05978092\n",
      "Training [6271/10000] ..........6271 ) Loss= 0.05112801\n",
      "Training [6272/10000] ..........6272 ) Loss= 0.077672444\n",
      "Training [6273/10000] ..........6273 ) Loss= 0.68250906\n",
      "Training [6274/10000] ..........6274 ) Loss= 0.24181409\n",
      "Training [6275/10000] ..........6275 ) Loss= 0.07993265\n",
      "Training [6276/10000] ..........6276 ) Loss= 0.06897555\n",
      "Training [6277/10000] ..........6277 ) Loss= 0.04941532\n",
      "Training [6278/10000] ..........6278 ) Loss= 0.08010835\n",
      "Training [6279/10000] ..........6279 ) Loss= 0.090048485\n",
      "Training [6280/10000] ..........6280 ) Loss= 0.07506494\n",
      "Training [6281/10000] ..........6281 ) Loss= 0.10554858\n",
      "Training [6282/10000] ..........6282 ) Loss= 0.07316022\n",
      "Training [6283/10000] ..........6283 ) Loss= 0.08958283\n",
      "Training [6284/10000] ..........6284 ) Loss= 0.08563716\n",
      "Training [6285/10000] ..........6285 ) Loss= 0.060442403\n",
      "Training [6286/10000] ..........6286 ) Loss= 0.12050283\n",
      "Training [6287/10000] ..........6287 ) Loss= 0.07989333\n",
      "Training [6288/10000] ..........6288 ) Loss= 0.09740872\n",
      "Training [6289/10000] ..........6289 ) Loss= 0.13990885\n",
      "Training [6290/10000] ..........6290 ) Loss= 0.12022155\n",
      "Training [6291/10000] ..........6291 ) Loss= 0.058935743\n",
      "Training [6292/10000] ..........6292 ) Loss= 0.12271222\n",
      "Training [6293/10000] ..........6293 ) Loss= 0.06578183\n",
      "Training [6294/10000] ..........6294 ) Loss= 0.111220576\n",
      "Training [6295/10000] ..........6295 ) Loss= 0.07281914\n",
      "Training [6296/10000] ..........6296 ) Loss= 0.07981032\n",
      "Training [6297/10000] ..........6297 ) Loss= 0.08738923\n",
      "Training [6298/10000] ..........6298 ) Loss= 0.13304047\n",
      "Training [6299/10000] ..........6299 ) Loss= 0.065273024\n",
      "Training [6300/10000] ..........6300 ) Loss= 0.14635433\n",
      "Training [6301/10000] ..........6301 ) Loss= 0.06325597\n",
      "Training [6302/10000] ..........6302 ) Loss= 0.051809058\n",
      "Training [6303/10000] ..........6303 ) Loss= 0.072858416\n",
      "Training [6304/10000] ..........6304 ) Loss= 0.06646581\n",
      "Training [6305/10000] ..........6305 ) Loss= 0.1000717\n",
      "Training [6306/10000] ..........6306 ) Loss= 0.11525307\n",
      "Training [6307/10000] ..........6307 ) Loss= 0.07204149\n",
      "Training [6308/10000] ..........6308 ) Loss= 0.043362394\n",
      "Training [6309/10000] ..........6309 ) Loss= 0.1504124\n",
      "Training [6310/10000] ..........6310 ) Loss= 0.080045156\n",
      "Training [6311/10000] ..........6311 ) Loss= 0.1458822\n",
      "Training [6312/10000] ..........6312 ) Loss= 0.061056014\n",
      "Training [6313/10000] ..........6313 ) Loss= 0.08088868\n",
      "Training [6314/10000] ..........6314 ) Loss= 0.11246051\n",
      "Training [6315/10000] ..........6315 ) Loss= 0.14348911\n",
      "Training [6316/10000] ..........6316 ) Loss= 0.1123538\n",
      "Training [6317/10000] ..........6317 ) Loss= 0.08924909\n",
      "Training [6318/10000] ..........6318 ) Loss= 0.13967194\n",
      "Training [6319/10000] ..........6319 ) Loss= 0.05436126\n",
      "Training [6320/10000] ..........6320 ) Loss= 0.056497257\n",
      "Training [6321/10000] ..........6321 ) Loss= 0.09915908\n",
      "Training [6322/10000] ..........6322 ) Loss= 0.10524975\n",
      "Training [6323/10000] ..........6323 ) Loss= 0.09142884\n",
      "Training [6324/10000] ..........6324 ) Loss= 0.13129942\n",
      "Training [6325/10000] ..........6325 ) Loss= 0.15501516\n",
      "Training [6326/10000] ..........6326 ) Loss= 0.08601414\n",
      "Training [6327/10000] ..........6327 ) Loss= 0.12556843\n",
      "Training [6328/10000] ..........6328 ) Loss= 0.15274891\n",
      "Training [6329/10000] ..........6329 ) Loss= 0.072809204\n",
      "Training [6330/10000] ..........6330 ) Loss= 0.11061556\n",
      "Training [6331/10000] ..........6331 ) Loss= 0.15999167\n",
      "Training [6332/10000] ..........6332 ) Loss= 0.10430014\n",
      "Training [6333/10000] ..........6333 ) Loss= 0.059265412\n",
      "Training [6334/10000] ..........6334 ) Loss= 0.08114686\n",
      "Training [6335/10000] ..........6335 ) Loss= 0.08615331\n",
      "Training [6336/10000] ..........6336 ) Loss= 0.12457475\n",
      "Training [6337/10000] ..........6337 ) Loss= 0.078656465\n",
      "Training [6338/10000] ..........6338 ) Loss= 0.078741916\n",
      "Training [6339/10000] ..........6339 ) Loss= 0.06342911\n",
      "Training [6340/10000] ..........6340 ) Loss= 0.082290776\n",
      "Training [6341/10000] ..........6341 ) Loss= 0.08291072\n",
      "Training [6342/10000] ..........6342 ) Loss= 0.08229896\n",
      "Training [6343/10000] ..........6343 ) Loss= 0.073351435\n",
      "Training [6344/10000] ..........6344 ) Loss= 0.108913705\n",
      "Training [6345/10000] ..........6345 ) Loss= 0.07614119\n",
      "Training [6346/10000] ..........6346 ) Loss= 0.14379036\n",
      "Training [6347/10000] ..........6347 ) Loss= 0.109773934\n",
      "Training [6348/10000] ..........6348 ) Loss= 0.04496687\n",
      "Training [6349/10000] ..........6349 ) Loss= 0.08035531\n",
      "Training [6350/10000] ..........6350 ) Loss= 0.07344813\n",
      "Training [6351/10000] ..........6351 ) Loss= 0.0759477\n",
      "Training [6352/10000] ..........6352 ) Loss= 0.045784406\n",
      "Training [6353/10000] ..........6353 ) Loss= 0.12270405\n",
      "Training [6354/10000] ..........6354 ) Loss= 0.13750848\n",
      "Training [6355/10000] ..........6355 ) Loss= 0.0923738\n",
      "Training [6356/10000] ..........6356 ) Loss= 0.07969391\n",
      "Training [6357/10000] ..........6357 ) Loss= 0.088770404\n",
      "Training [6358/10000] ..........6358 ) Loss= 0.22847304\n",
      "Training [6359/10000] ..........6359 ) Loss= 0.120962985\n",
      "Training [6360/10000] ..........6360 ) Loss= 0.07022186\n",
      "Training [6361/10000] ..........6361 ) Loss= 0.0789012\n",
      "Training [6362/10000] ..........6362 ) Loss= 0.061224908\n",
      "Training [6363/10000] ..........6363 ) Loss= 0.039527036\n",
      "Training [6364/10000] ..........6364 ) Loss= 0.04977971\n",
      "Training [6365/10000] ..........6365 ) Loss= 0.12729776\n",
      "Training [6366/10000] ..........6366 ) Loss= 0.2639038\n",
      "Training [6367/10000] ..........6367 ) Loss= 0.13612364\n",
      "Training [6368/10000] ..........6368 ) Loss= 0.10068108\n",
      "Training [6369/10000] ..........6369 ) Loss= 0.11002057\n",
      "Training [6370/10000] ..........6370 ) Loss= 0.09551603\n",
      "Training [6371/10000] ..........6371 ) Loss= 0.06978298\n",
      "Training [6372/10000] ..........6372 ) Loss= 0.17390887\n",
      "Training [6373/10000] ..........6373 ) Loss= 0.0650531\n",
      "Training [6374/10000] ..........6374 ) Loss= 0.11041021\n",
      "Training [6375/10000] ..........6375 ) Loss= 0.08956478\n",
      "Training [6376/10000] ..........6376 ) Loss= 0.10383922\n",
      "Training [6377/10000] ..........6377 ) Loss= 0.14537609\n",
      "Training [6378/10000] ..........6378 ) Loss= 0.07641035\n",
      "Training [6379/10000] ..........6379 ) Loss= 0.06235183\n",
      "Training [6380/10000] ..........6380 ) Loss= 0.099628806\n",
      "Training [6381/10000] ..........6381 ) Loss= 0.1025132\n",
      "Training [6382/10000] ..........6382 ) Loss= 0.04369151\n",
      "Training [6383/10000] ..........6383 ) Loss= 0.07527575\n",
      "Training [6384/10000] ..........6384 ) Loss= 0.10775951\n",
      "Training [6385/10000] ..........6385 ) Loss= 0.2795457\n",
      "Training [6386/10000] ..........6386 ) Loss= 0.083077766\n",
      "Training [6387/10000] ..........6387 ) Loss= 0.07138072\n",
      "Training [6388/10000] ..........6388 ) Loss= 0.07019247\n",
      "Training [6389/10000] ..........6389 ) Loss= 0.07136299\n",
      "Training [6390/10000] ..........6390 ) Loss= 0.09840931\n",
      "Training [6391/10000] ..........6391 ) Loss= 0.045975175\n",
      "Training [6392/10000] ..........6392 ) Loss= 0.060752474\n",
      "Training [6393/10000] ..........6393 ) Loss= 0.07450089\n",
      "Training [6394/10000] ..........6394 ) Loss= 0.083095394\n",
      "Training [6395/10000] ..........6395 ) Loss= 0.060340032\n",
      "Training [6396/10000] ..........6396 ) Loss= 0.07424196\n",
      "Training [6397/10000] ..........6397 ) Loss= 0.08069803\n",
      "Training [6398/10000] ..........6398 ) Loss= 0.08465959\n",
      "Training [6399/10000] ..........6399 ) Loss= 0.07758758\n",
      "Training [6400/10000] ..........6400 ) Loss= 0.073421136\n",
      "Training [6401/10000] ..........6401 ) Loss= 0.0734371\n",
      "Training [6402/10000] ..........6402 ) Loss= 0.105922915\n",
      "Training [6403/10000] ..........6403 ) Loss= 0.08637106\n",
      "Training [6404/10000] ..........6404 ) Loss= 0.062013865\n",
      "Training [6405/10000] ..........6405 ) Loss= 0.10055695\n",
      "Training [6406/10000] ..........6406 ) Loss= 0.08139167\n",
      "Training [6407/10000] ..........6407 ) Loss= 0.13775176\n",
      "Training [6408/10000] ..........6408 ) Loss= 0.08123196\n",
      "Training [6409/10000] ..........6409 ) Loss= 0.10513955\n",
      "Training [6410/10000] ..........6410 ) Loss= 0.14251357\n",
      "Training [6411/10000] ..........6411 ) Loss= 0.1572714\n",
      "Training [6412/10000] ..........6412 ) Loss= 0.095303565\n",
      "Training [6413/10000] ..........6413 ) Loss= 0.12152546\n",
      "Training [6414/10000] ..........6414 ) Loss= 0.14156911\n",
      "Training [6415/10000] ..........6415 ) Loss= 0.05812686\n",
      "Training [6416/10000] ..........6416 ) Loss= 0.12932087\n",
      "Training [6417/10000] ..........6417 ) Loss= 0.08533071\n",
      "Training [6418/10000] ..........6418 ) Loss= 0.10945545\n",
      "Training [6419/10000] ..........6419 ) Loss= 0.07682177\n",
      "Training [6420/10000] ..........6420 ) Loss= 0.07626746\n",
      "Training [6421/10000] ..........6421 ) Loss= 0.06461214\n",
      "Training [6422/10000] ..........6422 ) Loss= 0.14175999\n",
      "Training [6423/10000] ..........6423 ) Loss= 0.054228585\n",
      "Training [6424/10000] ..........6424 ) Loss= 0.09449062\n",
      "Training [6425/10000] ..........6425 ) Loss= 0.18552408\n",
      "Training [6426/10000] ..........6426 ) Loss= 0.07029874\n",
      "Training [6427/10000] ..........6427 ) Loss= 0.15549792\n",
      "Training [6428/10000] ..........6428 ) Loss= 0.08953169\n",
      "Training [6429/10000] ..........6429 ) Loss= 0.12859549\n",
      "Training [6430/10000] ..........6430 ) Loss= 0.1265152\n",
      "Training [6431/10000] ..........6431 ) Loss= 0.06807851\n",
      "Training [6432/10000] ..........6432 ) Loss= 0.3593054\n",
      "Training [6433/10000] ..........6433 ) Loss= 0.17782581\n",
      "Training [6434/10000] ..........6434 ) Loss= 0.049086723\n",
      "Training [6435/10000] ..........6435 ) Loss= 0.065139875\n",
      "Training [6436/10000] ..........6436 ) Loss= 0.09905169\n",
      "Training [6437/10000] ..........6437 ) Loss= 0.13030335\n",
      "Training [6438/10000] ..........6438 ) Loss= 0.06373885\n",
      "Training [6439/10000] ..........6439 ) Loss= 0.0793417\n",
      "Training [6440/10000] ..........6440 ) Loss= 0.11577952\n",
      "Training [6441/10000] ..........6441 ) Loss= 0.21093728\n",
      "Training [6442/10000] ..........6442 ) Loss= 0.12518315\n",
      "Training [6443/10000] ..........6443 ) Loss= 0.0937158\n",
      "Training [6444/10000] ..........6444 ) Loss= 0.06397211\n",
      "Training [6445/10000] ..........6445 ) Loss= 0.08093643\n",
      "Training [6446/10000] ..........6446 ) Loss= 0.08680909\n",
      "Training [6447/10000] ..........6447 ) Loss= 0.08559729\n",
      "Training [6448/10000] ..........6448 ) Loss= 0.16779435\n",
      "Training [6449/10000] ..........6449 ) Loss= 0.13778248\n",
      "Training [6450/10000] ..........6450 ) Loss= 0.057623204\n",
      "Training [6451/10000] ..........6451 ) Loss= 0.12701331\n",
      "Training [6452/10000] ..........6452 ) Loss= 0.06475206\n",
      "Training [6453/10000] ..........6453 ) Loss= 0.06211108\n",
      "Training [6454/10000] ..........6454 ) Loss= 0.08204573\n",
      "Training [6455/10000] ..........6455 ) Loss= 0.11693193\n",
      "Training [6456/10000] ..........6456 ) Loss= 0.09726505\n",
      "Training [6457/10000] ..........6457 ) Loss= 0.12055416\n",
      "Training [6458/10000] ..........6458 ) Loss= 0.09451789\n",
      "Training [6459/10000] ..........6459 ) Loss= 0.16829854\n",
      "Training [6460/10000] ..........6460 ) Loss= 0.069354884\n",
      "Training [6461/10000] ..........6461 ) Loss= 0.066445395\n",
      "Training [6462/10000] ..........6462 ) Loss= 0.08421744\n",
      "Training [6463/10000] ..........6463 ) Loss= 0.040340275\n",
      "Training [6464/10000] ..........6464 ) Loss= 0.14426002\n",
      "Training [6465/10000] ..........6465 ) Loss= 0.22175673\n",
      "Training [6466/10000] ..........6466 ) Loss= 0.048557118\n",
      "Training [6467/10000] ..........6467 ) Loss= 0.07964621\n",
      "Training [6468/10000] ..........6468 ) Loss= 0.05936391\n",
      "Training [6469/10000] ..........6469 ) Loss= 0.11145823\n",
      "Training [6470/10000] ..........6470 ) Loss= 0.08508513\n",
      "Training [6471/10000] ..........6471 ) Loss= 0.07664613\n",
      "Training [6472/10000] ..........6472 ) Loss= 0.11737385\n",
      "Training [6473/10000] ..........6473 ) Loss= 0.103636496\n",
      "Training [6474/10000] ..........6474 ) Loss= 0.056355774\n",
      "Training [6475/10000] ..........6475 ) Loss= 0.11189084\n",
      "Training [6476/10000] ..........6476 ) Loss= 0.06766407\n",
      "Training [6477/10000] ..........6477 ) Loss= 0.08764783\n",
      "Training [6478/10000] ..........6478 ) Loss= 0.1584237\n",
      "Training [6479/10000] ..........6479 ) Loss= 0.10436551\n",
      "Training [6480/10000] ..........6480 ) Loss= 0.079663604\n",
      "Training [6481/10000] ..........6481 ) Loss= 0.089245155\n",
      "Training [6482/10000] ..........6482 ) Loss= 0.111779116\n",
      "Training [6483/10000] ..........6483 ) Loss= 0.10096755\n",
      "Training [6484/10000] ..........6484 ) Loss= 0.057554003\n",
      "Training [6485/10000] ..........6485 ) Loss= 0.070226796\n",
      "Training [6486/10000] ..........6486 ) Loss= 0.08445635\n",
      "Training [6487/10000] ..........6487 ) Loss= 0.100232214\n",
      "Training [6488/10000] ..........6488 ) Loss= 0.12710784\n",
      "Training [6489/10000] ..........6489 ) Loss= 0.09188908\n",
      "Training [6490/10000] ..........6490 ) Loss= 0.22023241\n",
      "Training [6491/10000] ..........6491 ) Loss= 0.15243831\n",
      "Training [6492/10000] ..........6492 ) Loss= 0.09780873\n",
      "Training [6493/10000] ..........6493 ) Loss= 0.081039004\n",
      "Training [6494/10000] ..........6494 ) Loss= 0.0665639\n",
      "Training [6495/10000] ..........6495 ) Loss= 0.0477562\n",
      "Training [6496/10000] ..........6496 ) Loss= 0.04899027\n",
      "Training [6497/10000] ..........6497 ) Loss= 0.079276145\n",
      "Training [6498/10000] ..........6498 ) Loss= 0.10475303\n",
      "Training [6499/10000] ..........6499 ) Loss= 0.081857316\n",
      "Training [6500/10000] ..........6500 ) Loss= 0.10382121\n",
      "Training [6501/10000] ..........6501 ) Loss= 0.13893397\n",
      "Training [6502/10000] ..........6502 ) Loss= 0.10826056\n",
      "Training [6503/10000] ..........6503 ) Loss= 0.16974173\n",
      "Training [6504/10000] ..........6504 ) Loss= 0.06827609\n",
      "Training [6505/10000] ..........6505 ) Loss= 0.065533094\n",
      "Training [6506/10000] ..........6506 ) Loss= 0.12452\n",
      "Training [6507/10000] ..........6507 ) Loss= 0.061702054\n",
      "Training [6508/10000] ..........6508 ) Loss= 0.070949554\n",
      "Training [6509/10000] ..........6509 ) Loss= 0.08556723\n",
      "Training [6510/10000] ..........6510 ) Loss= 0.09364727\n",
      "Training [6511/10000] ..........6511 ) Loss= 0.06677761\n",
      "Training [6512/10000] ..........6512 ) Loss= 0.07542316\n",
      "Training [6513/10000] ..........6513 ) Loss= 0.05335183\n",
      "Training [6514/10000] ..........6514 ) Loss= 0.12881747\n",
      "Training [6515/10000] ..........6515 ) Loss= 0.20145698\n",
      "Training [6516/10000] ..........6516 ) Loss= 0.09288553\n",
      "Training [6517/10000] ..........6517 ) Loss= 0.21471846\n",
      "Training [6518/10000] ..........6518 ) Loss= 0.10051023\n",
      "Training [6519/10000] ..........6519 ) Loss= 0.107654676\n",
      "Training [6520/10000] ..........6520 ) Loss= 0.10501154\n",
      "Training [6521/10000] ..........6521 ) Loss= 0.058277916\n",
      "Training [6522/10000] ..........6522 ) Loss= 0.14178786\n",
      "Training [6523/10000] ..........6523 ) Loss= 0.11162052\n",
      "Training [6524/10000] ..........6524 ) Loss= 0.12442091\n",
      "Training [6525/10000] ..........6525 ) Loss= 0.085574135\n",
      "Training [6526/10000] ..........6526 ) Loss= 0.09648548\n",
      "Training [6527/10000] ..........6527 ) Loss= 0.070270285\n",
      "Training [6528/10000] ..........6528 ) Loss= 0.108778134\n",
      "Training [6529/10000] ..........6529 ) Loss= 0.10112474\n",
      "Training [6530/10000] ..........6530 ) Loss= 0.08226675\n",
      "Training [6531/10000] ..........6531 ) Loss= 0.08672193\n",
      "Training [6532/10000] ..........6532 ) Loss= 0.17906007\n",
      "Training [6533/10000] ..........6533 ) Loss= 0.086724706\n",
      "Training [6534/10000] ..........6534 ) Loss= 0.09040574\n",
      "Training [6535/10000] ..........6535 ) Loss= 0.073439\n",
      "Training [6536/10000] ..........6536 ) Loss= 0.16157429\n",
      "Training [6537/10000] ..........6537 ) Loss= 0.079465665\n",
      "Training [6538/10000] ..........6538 ) Loss= 0.091235995\n",
      "Training [6539/10000] ..........6539 ) Loss= 0.1656746\n",
      "Training [6540/10000] ..........6540 ) Loss= 0.10720078\n",
      "Training [6541/10000] ..........6541 ) Loss= 0.14746553\n",
      "Training [6542/10000] ..........6542 ) Loss= 0.08498514\n",
      "Training [6543/10000] ..........6543 ) Loss= 0.09014525\n",
      "Training [6544/10000] ..........6544 ) Loss= 0.06413848\n",
      "Training [6545/10000] ..........6545 ) Loss= 0.13679463\n",
      "Training [6546/10000] ..........6546 ) Loss= 0.18221305\n",
      "Training [6547/10000] ..........6547 ) Loss= 0.09490967\n",
      "Training [6548/10000] ..........6548 ) Loss= 0.10888106\n",
      "Training [6549/10000] ..........6549 ) Loss= 0.09957761\n",
      "Training [6550/10000] ..........6550 ) Loss= 0.11445979\n",
      "Training [6551/10000] ..........6551 ) Loss= 0.061127204\n",
      "Training [6552/10000] ..........6552 ) Loss= 0.09750719\n",
      "Training [6553/10000] ..........6553 ) Loss= 0.13947988\n",
      "Training [6554/10000] ..........6554 ) Loss= 0.05775863\n",
      "Training [6555/10000] ..........6555 ) Loss= 0.07746032\n",
      "Training [6556/10000] ..........6556 ) Loss= 0.09979031\n",
      "Training [6557/10000] ..........6557 ) Loss= 0.051613815\n",
      "Training [6558/10000] ..........6558 ) Loss= 0.086080946\n",
      "Training [6559/10000] ..........6559 ) Loss= 0.10966894\n",
      "Training [6560/10000] ..........6560 ) Loss= 0.047200374\n",
      "Training [6561/10000] ..........6561 ) Loss= 0.06955952\n",
      "Training [6562/10000] ..........6562 ) Loss= 0.08464317\n",
      "Training [6563/10000] ..........6563 ) Loss= 0.07512249\n",
      "Training [6564/10000] ..........6564 ) Loss= 0.16448933\n",
      "Training [6565/10000] ..........6565 ) Loss= 0.070696235\n",
      "Training [6566/10000] ..........6566 ) Loss= 0.10459321\n",
      "Training [6567/10000] ..........6567 ) Loss= 0.05452211\n",
      "Training [6568/10000] ..........6568 ) Loss= 0.09067061\n",
      "Training [6569/10000] ..........6569 ) Loss= 0.05311224\n",
      "Training [6570/10000] ..........6570 ) Loss= 0.07179798\n",
      "Training [6571/10000] ..........6571 ) Loss= 0.09378949\n",
      "Training [6572/10000] ..........6572 ) Loss= 0.10176416\n",
      "Training [6573/10000] ..........6573 ) Loss= 0.15005329\n",
      "Training [6574/10000] ..........6574 ) Loss= 0.04876897\n",
      "Training [6575/10000] ..........6575 ) Loss= 0.19428112\n",
      "Training [6576/10000] ..........6576 ) Loss= 0.1342229\n",
      "Training [6577/10000] ..........6577 ) Loss= 0.1029334\n",
      "Training [6578/10000] ..........6578 ) Loss= 0.07015315\n",
      "Training [6579/10000] ..........6579 ) Loss= 0.095724255\n",
      "Training [6580/10000] ..........6580 ) Loss= 0.075585954\n",
      "Training [6581/10000] ..........6581 ) Loss= 0.1316223\n",
      "Training [6582/10000] ..........6582 ) Loss= 0.0732214\n",
      "Training [6583/10000] ..........6583 ) Loss= 0.08936643\n",
      "Training [6584/10000] ..........6584 ) Loss= 0.10027603\n",
      "Training [6585/10000] ..........6585 ) Loss= 0.08403978\n",
      "Training [6586/10000] ..........6586 ) Loss= 0.08426187\n",
      "Training [6587/10000] ..........6587 ) Loss= 0.060113188\n",
      "Training [6588/10000] ..........6588 ) Loss= 0.24328718\n",
      "Training [6589/10000] ..........6589 ) Loss= 0.14874583\n",
      "Training [6590/10000] ..........6590 ) Loss= 0.08056193\n",
      "Training [6591/10000] ..........6591 ) Loss= 0.12510145\n",
      "Training [6592/10000] ..........6592 ) Loss= 0.10788528\n",
      "Training [6593/10000] ..........6593 ) Loss= 0.21472833\n",
      "Training [6594/10000] ..........6594 ) Loss= 0.26094943\n",
      "Training [6595/10000] ..........6595 ) Loss= 0.07097238\n",
      "Training [6596/10000] ..........6596 ) Loss= 0.16638468\n",
      "Training [6597/10000] ..........6597 ) Loss= 0.07092416\n",
      "Training [6598/10000] ..........6598 ) Loss= 0.06920538\n",
      "Training [6599/10000] ..........6599 ) Loss= 0.05781767\n",
      "Training [6600/10000] ..........6600 ) Loss= 0.08614122\n",
      "Training [6601/10000] ..........6601 ) Loss= 0.13232268\n",
      "Training [6602/10000] ..........6602 ) Loss= 0.05873824\n",
      "Training [6603/10000] ..........6603 ) Loss= 0.0639085\n",
      "Training [6604/10000] ..........6604 ) Loss= 0.066736415\n",
      "Training [6605/10000] ..........6605 ) Loss= 0.049888484\n",
      "Training [6606/10000] ..........6606 ) Loss= 0.10732553\n",
      "Training [6607/10000] ..........6607 ) Loss= 0.13654806\n",
      "Training [6608/10000] ..........6608 ) Loss= 0.1267684\n",
      "Training [6609/10000] ..........6609 ) Loss= 0.07647523\n",
      "Training [6610/10000] ..........6610 ) Loss= 0.06016618\n",
      "Training [6611/10000] ..........6611 ) Loss= 0.07201691\n",
      "Training [6612/10000] ..........6612 ) Loss= 0.09111289\n",
      "Training [6613/10000] ..........6613 ) Loss= 0.08697438\n",
      "Training [6614/10000] ..........6614 ) Loss= 0.14240734\n",
      "Training [6615/10000] ..........6615 ) Loss= 0.14015462\n",
      "Training [6616/10000] ..........6616 ) Loss= 0.061033756\n",
      "Training [6617/10000] ..........6617 ) Loss= 0.0638181\n",
      "Training [6618/10000] ..........6618 ) Loss= 0.08601321\n",
      "Training [6619/10000] ..........6619 ) Loss= 0.09179972\n",
      "Training [6620/10000] ..........6620 ) Loss= 0.06574281\n",
      "Training [6621/10000] ..........6621 ) Loss= 0.039755493\n",
      "Training [6622/10000] ..........6622 ) Loss= 0.048564628\n",
      "Training [6623/10000] ..........6623 ) Loss= 0.12758762\n",
      "Training [6624/10000] ..........6624 ) Loss= 0.07197511\n",
      "Training [6625/10000] ..........6625 ) Loss= 0.07775017\n",
      "Training [6626/10000] ..........6626 ) Loss= 0.0746197\n",
      "Training [6627/10000] ..........6627 ) Loss= 0.09282113\n",
      "Training [6628/10000] ..........6628 ) Loss= 0.12073515\n",
      "Training [6629/10000] ..........6629 ) Loss= 0.083736375\n",
      "Training [6630/10000] ..........6630 ) Loss= 0.0663107\n",
      "Training [6631/10000] ..........6631 ) Loss= 0.088051185\n",
      "Training [6632/10000] ..........6632 ) Loss= 0.08490026\n",
      "Training [6633/10000] ..........6633 ) Loss= 0.10206719\n",
      "Training [6634/10000] ..........6634 ) Loss= 0.09001766\n",
      "Training [6635/10000] ..........6635 ) Loss= 0.055822175\n",
      "Training [6636/10000] ..........6636 ) Loss= 0.07448255\n",
      "Training [6637/10000] ..........6637 ) Loss= 0.12553926\n",
      "Training [6638/10000] ..........6638 ) Loss= 0.1070508\n",
      "Training [6639/10000] ..........6639 ) Loss= 0.23065054\n",
      "Training [6640/10000] ..........6640 ) Loss= 0.05863036\n",
      "Training [6641/10000] ..........6641 ) Loss= 0.072187155\n",
      "Training [6642/10000] ..........6642 ) Loss= 0.075009264\n",
      "Training [6643/10000] ..........6643 ) Loss= 0.12134064\n",
      "Training [6644/10000] ..........6644 ) Loss= 0.06825765\n",
      "Training [6645/10000] ..........6645 ) Loss= 0.056138434\n",
      "Training [6646/10000] ..........6646 ) Loss= 0.077100486\n",
      "Training [6647/10000] ..........6647 ) Loss= 0.04939866\n",
      "Training [6648/10000] ..........6648 ) Loss= 0.06347306\n",
      "Training [6649/10000] ..........6649 ) Loss= 0.07669194\n",
      "Training [6650/10000] ..........6650 ) Loss= 0.08039297\n",
      "Training [6651/10000] ..........6651 ) Loss= 0.10481633\n",
      "Training [6652/10000] ..........6652 ) Loss= 0.05925743\n",
      "Training [6653/10000] ..........6653 ) Loss= 0.092760675\n",
      "Training [6654/10000] ..........6654 ) Loss= 0.2731372\n",
      "Training [6655/10000] ..........6655 ) Loss= 0.0779662\n",
      "Training [6656/10000] ..........6656 ) Loss= 0.06470907\n",
      "Training [6657/10000] ..........6657 ) Loss= 0.14337152\n",
      "Training [6658/10000] ..........6658 ) Loss= 0.09180044\n",
      "Training [6659/10000] ..........6659 ) Loss= 0.05576445\n",
      "Training [6660/10000] ..........6660 ) Loss= 0.11967476\n",
      "Training [6661/10000] ..........6661 ) Loss= 0.08895436\n",
      "Training [6662/10000] ..........6662 ) Loss= 0.06267109\n",
      "Training [6663/10000] ..........6663 ) Loss= 0.1249586\n",
      "Training [6664/10000] ..........6664 ) Loss= 0.0852451\n",
      "Training [6665/10000] ..........6665 ) Loss= 0.0712853\n",
      "Training [6666/10000] ..........6666 ) Loss= 0.07723159\n",
      "Training [6667/10000] ..........6667 ) Loss= 0.09293817\n",
      "Training [6668/10000] ..........6668 ) Loss= 0.16922158\n",
      "Training [6669/10000] ..........6669 ) Loss= 0.050051857\n",
      "Training [6670/10000] ..........6670 ) Loss= 0.05621749\n",
      "Training [6671/10000] ..........6671 ) Loss= 0.05913425\n",
      "Training [6672/10000] ..........6672 ) Loss= 0.08110698\n",
      "Training [6673/10000] ..........6673 ) Loss= 0.07318042\n",
      "Training [6674/10000] ..........6674 ) Loss= 0.09405597\n",
      "Training [6675/10000] ..........6675 ) Loss= 0.107864745\n",
      "Training [6676/10000] ..........6676 ) Loss= 0.05582939\n",
      "Training [6677/10000] ..........6677 ) Loss= 0.093206644\n",
      "Training [6678/10000] ..........6678 ) Loss= 0.04900533\n",
      "Training [6679/10000] ..........6679 ) Loss= 0.061897673\n",
      "Training [6680/10000] ..........6680 ) Loss= 0.09451772\n",
      "Training [6681/10000] ..........6681 ) Loss= 0.07011883\n",
      "Training [6682/10000] ..........6682 ) Loss= 0.08757753\n",
      "Training [6683/10000] ..........6683 ) Loss= 0.09054745\n",
      "Training [6684/10000] ..........6684 ) Loss= 0.06466862\n",
      "Training [6685/10000] ..........6685 ) Loss= 0.09109559\n",
      "Training [6686/10000] ..........6686 ) Loss= 0.07919545\n",
      "Training [6687/10000] ..........6687 ) Loss= 0.1086403\n",
      "Training [6688/10000] ..........6688 ) Loss= 0.10798969\n",
      "Training [6689/10000] ..........6689 ) Loss= 0.0689923\n",
      "Training [6690/10000] ..........6690 ) Loss= 0.06973149\n",
      "Training [6691/10000] ..........6691 ) Loss= 0.077641115\n",
      "Training [6692/10000] ..........6692 ) Loss= 0.15331124\n",
      "Training [6693/10000] ..........6693 ) Loss= 0.11226055\n",
      "Training [6694/10000] ..........6694 ) Loss= 0.105117984\n",
      "Training [6695/10000] ..........6695 ) Loss= 0.112971835\n",
      "Training [6696/10000] ..........6696 ) Loss= 0.046371512\n",
      "Training [6697/10000] ..........6697 ) Loss= 0.06843257\n",
      "Training [6698/10000] ..........6698 ) Loss= 0.07624315\n",
      "Training [6699/10000] ..........6699 ) Loss= 0.09825106\n",
      "Training [6700/10000] ..........6700 ) Loss= 0.0909026\n",
      "Training [6701/10000] ..........6701 ) Loss= 0.05472901\n",
      "Training [6702/10000] ..........6702 ) Loss= 0.10766069\n",
      "Training [6703/10000] ..........6703 ) Loss= 0.0715106\n",
      "Training [6704/10000] ..........6704 ) Loss= 0.13966657\n",
      "Training [6705/10000] ..........6705 ) Loss= 0.056520212\n",
      "Training [6706/10000] ..........6706 ) Loss= 0.06612168\n",
      "Training [6707/10000] ..........6707 ) Loss= 0.10868263\n",
      "Training [6708/10000] ..........6708 ) Loss= 0.10652162\n",
      "Training [6709/10000] ..........6709 ) Loss= 0.07638657\n",
      "Training [6710/10000] ..........6710 ) Loss= 0.05425537\n",
      "Training [6711/10000] ..........6711 ) Loss= 0.06915194\n",
      "Training [6712/10000] ..........6712 ) Loss= 0.08122118\n",
      "Training [6713/10000] ..........6713 ) Loss= 0.20669802\n",
      "Training [6714/10000] ..........6714 ) Loss= 0.16103639\n",
      "Training [6715/10000] ..........6715 ) Loss= 0.099499986\n",
      "Training [6716/10000] ..........6716 ) Loss= 0.10227605\n",
      "Training [6717/10000] ..........6717 ) Loss= 0.080232844\n",
      "Training [6718/10000] ..........6718 ) Loss= 0.07006111\n",
      "Training [6719/10000] ..........6719 ) Loss= 0.08858422\n",
      "Training [6720/10000] ..........6720 ) Loss= 0.098564826\n",
      "Training [6721/10000] ..........6721 ) Loss= 0.055860177\n",
      "Training [6722/10000] ..........6722 ) Loss= 0.05549414\n",
      "Training [6723/10000] ..........6723 ) Loss= 0.1069389\n",
      "Training [6724/10000] ..........6724 ) Loss= 0.076071166\n",
      "Training [6725/10000] ..........6725 ) Loss= 0.07569095\n",
      "Training [6726/10000] ..........6726 ) Loss= 0.056364264\n",
      "Training [6727/10000] ..........6727 ) Loss= 0.040667135\n",
      "Training [6728/10000] ..........6728 ) Loss= 0.052852605\n",
      "Training [6729/10000] ..........6729 ) Loss= 0.15483174\n",
      "Training [6730/10000] ..........6730 ) Loss= 0.051801775\n",
      "Training [6731/10000] ..........6731 ) Loss= 0.113848455\n",
      "Training [6732/10000] ..........6732 ) Loss= 0.11652063\n",
      "Training [6733/10000] ..........6733 ) Loss= 0.0754449\n",
      "Training [6734/10000] ..........6734 ) Loss= 0.08194965\n",
      "Training [6735/10000] ..........6735 ) Loss= 0.056155708\n",
      "Training [6736/10000] ..........6736 ) Loss= 0.05885678\n",
      "Training [6737/10000] ..........6737 ) Loss= 0.04733707\n",
      "Training [6738/10000] ..........6738 ) Loss= 0.085377865\n",
      "Training [6739/10000] ..........6739 ) Loss= 0.1403376\n",
      "Training [6740/10000] ..........6740 ) Loss= 0.0565054\n",
      "Training [6741/10000] ..........6741 ) Loss= 0.08367119\n",
      "Training [6742/10000] ..........6742 ) Loss= 0.085050635\n",
      "Training [6743/10000] ..........6743 ) Loss= 0.07231324\n",
      "Training [6744/10000] ..........6744 ) Loss= 0.06574879\n",
      "Training [6745/10000] ..........6745 ) Loss= 0.08221556\n",
      "Training [6746/10000] ..........6746 ) Loss= 0.074716106\n",
      "Training [6747/10000] ..........6747 ) Loss= 0.0848926\n",
      "Training [6748/10000] ..........6748 ) Loss= 0.06493795\n",
      "Training [6749/10000] ..........6749 ) Loss= 0.10869936\n",
      "Training [6750/10000] ..........6750 ) Loss= 0.0948052\n",
      "Training [6751/10000] ..........6751 ) Loss= 0.116305836\n",
      "Training [6752/10000] ..........6752 ) Loss= 0.07059406\n",
      "Training [6753/10000] ..........6753 ) Loss= 0.07634628\n",
      "Training [6754/10000] ..........6754 ) Loss= 0.0811831\n",
      "Training [6755/10000] ..........6755 ) Loss= 0.06745706\n",
      "Training [6756/10000] ..........6756 ) Loss= 0.08731012\n",
      "Training [6757/10000] ..........6757 ) Loss= 0.10738124\n",
      "Training [6758/10000] ..........6758 ) Loss= 0.10086766\n",
      "Training [6759/10000] ..........6759 ) Loss= 0.05214963\n",
      "Training [6760/10000] ..........6760 ) Loss= 0.0908628\n",
      "Training [6761/10000] ..........6761 ) Loss= 0.07621245\n",
      "Training [6762/10000] ..........6762 ) Loss= 0.11761722\n",
      "Training [6763/10000] ..........6763 ) Loss= 0.10567178\n",
      "Training [6764/10000] ..........6764 ) Loss= 0.07521796\n",
      "Training [6765/10000] ..........6765 ) Loss= 0.080052875\n",
      "Training [6766/10000] ..........6766 ) Loss= 0.091863506\n",
      "Training [6767/10000] ..........6767 ) Loss= 0.07481545\n",
      "Training [6768/10000] ..........6768 ) Loss= 0.13276286\n",
      "Training [6769/10000] ..........6769 ) Loss= 0.06906399\n",
      "Training [6770/10000] ..........6770 ) Loss= 0.06950002\n",
      "Training [6771/10000] ..........6771 ) Loss= 0.1179368\n",
      "Training [6772/10000] ..........6772 ) Loss= 0.118066244\n",
      "Training [6773/10000] ..........6773 ) Loss= 0.12098552\n",
      "Training [6774/10000] ..........6774 ) Loss= 0.044952072\n",
      "Training [6775/10000] ..........6775 ) Loss= 0.084453166\n",
      "Training [6776/10000] ..........6776 ) Loss= 0.056591067\n",
      "Training [6777/10000] ..........6777 ) Loss= 0.08146853\n",
      "Training [6778/10000] ..........6778 ) Loss= 0.13392384\n",
      "Training [6779/10000] ..........6779 ) Loss= 0.084986866\n",
      "Training [6780/10000] ..........6780 ) Loss= 0.07190197\n",
      "Training [6781/10000] ..........6781 ) Loss= 0.07574802\n",
      "Training [6782/10000] ..........6782 ) Loss= 0.0938212\n",
      "Training [6783/10000] ..........6783 ) Loss= 0.16889182\n",
      "Training [6784/10000] ..........6784 ) Loss= 0.07785463\n",
      "Training [6785/10000] ..........6785 ) Loss= 0.072623864\n",
      "Training [6786/10000] ..........6786 ) Loss= 0.058428816\n",
      "Training [6787/10000] ..........6787 ) Loss= 0.0727899\n",
      "Training [6788/10000] ..........6788 ) Loss= 0.16984841\n",
      "Training [6789/10000] ..........6789 ) Loss= 0.10491589\n",
      "Training [6790/10000] ..........6790 ) Loss= 0.082239695\n",
      "Training [6791/10000] ..........6791 ) Loss= 0.121918894\n",
      "Training [6792/10000] ..........6792 ) Loss= 0.23409106\n",
      "Training [6793/10000] ..........6793 ) Loss= 0.11159126\n",
      "Training [6794/10000] ..........6794 ) Loss= 0.07877578\n",
      "Training [6795/10000] ..........6795 ) Loss= 0.06810357\n",
      "Training [6796/10000] ..........6796 ) Loss= 0.104932256\n",
      "Training [6797/10000] ..........6797 ) Loss= 0.12183328\n",
      "Training [6798/10000] ..........6798 ) Loss= 0.11344186\n",
      "Training [6799/10000] ..........6799 ) Loss= 0.05350401\n",
      "Training [6800/10000] ..........6800 ) Loss= 0.070942275\n",
      "Training [6801/10000] ..........6801 ) Loss= 0.070489205\n",
      "Training [6802/10000] ..........6802 ) Loss= 0.10991289\n",
      "Training [6803/10000] ..........6803 ) Loss= 0.07506825\n",
      "Training [6804/10000] ..........6804 ) Loss= 0.05834642\n",
      "Training [6805/10000] ..........6805 ) Loss= 0.097636215\n",
      "Training [6806/10000] ..........6806 ) Loss= 0.06317147\n",
      "Training [6807/10000] ..........6807 ) Loss= 0.05920981\n",
      "Training [6808/10000] ..........6808 ) Loss= 0.08741639\n",
      "Training [6809/10000] ..........6809 ) Loss= 0.08769311\n",
      "Training [6810/10000] ..........6810 ) Loss= 0.05192157\n",
      "Training [6811/10000] ..........6811 ) Loss= 0.063259944\n",
      "Training [6812/10000] ..........6812 ) Loss= 0.18496886\n",
      "Training [6813/10000] ..........6813 ) Loss= 0.068447866\n",
      "Training [6814/10000] ..........6814 ) Loss= 0.05876041\n",
      "Training [6815/10000] ..........6815 ) Loss= 0.072173975\n",
      "Training [6816/10000] ..........6816 ) Loss= 0.11611275\n",
      "Training [6817/10000] ..........6817 ) Loss= 0.06838986\n",
      "Training [6818/10000] ..........6818 ) Loss= 0.1300318\n",
      "Training [6819/10000] ..........6819 ) Loss= 0.077184014\n",
      "Training [6820/10000] ..........6820 ) Loss= 0.10565944\n",
      "Training [6821/10000] ..........6821 ) Loss= 0.09929351\n",
      "Training [6822/10000] ..........6822 ) Loss= 0.07511076\n",
      "Training [6823/10000] ..........6823 ) Loss= 0.07638544\n",
      "Training [6824/10000] ..........6824 ) Loss= 0.11127035\n",
      "Training [6825/10000] ..........6825 ) Loss= 0.060577605\n",
      "Training [6826/10000] ..........6826 ) Loss= 0.13478762\n",
      "Training [6827/10000] ..........6827 ) Loss= 0.081382774\n",
      "Training [6828/10000] ..........6828 ) Loss= 0.074144274\n",
      "Training [6829/10000] ..........6829 ) Loss= 0.079429105\n",
      "Training [6830/10000] ..........6830 ) Loss= 0.13473085\n",
      "Training [6831/10000] ..........6831 ) Loss= 0.06840257\n",
      "Training [6832/10000] ..........6832 ) Loss= 0.1500521\n",
      "Training [6833/10000] ..........6833 ) Loss= 0.093141414\n",
      "Training [6834/10000] ..........6834 ) Loss= 0.0838798\n",
      "Training [6835/10000] ..........6835 ) Loss= 0.13199545\n",
      "Training [6836/10000] ..........6836 ) Loss= 0.05256875\n",
      "Training [6837/10000] ..........6837 ) Loss= 0.13985913\n",
      "Training [6838/10000] ..........6838 ) Loss= 0.10475882\n",
      "Training [6839/10000] ..........6839 ) Loss= 0.10958287\n",
      "Training [6840/10000] ..........6840 ) Loss= 0.060377665\n",
      "Training [6841/10000] ..........6841 ) Loss= 0.06588413\n",
      "Training [6842/10000] ..........6842 ) Loss= 0.065577105\n",
      "Training [6843/10000] ..........6843 ) Loss= 0.12368791\n",
      "Training [6844/10000] ..........6844 ) Loss= 0.063129045\n",
      "Training [6845/10000] ..........6845 ) Loss= 0.07687798\n",
      "Training [6846/10000] ..........6846 ) Loss= 0.09710227\n",
      "Training [6847/10000] ..........6847 ) Loss= 0.06131932\n",
      "Training [6848/10000] ..........6848 ) Loss= 0.09342275\n",
      "Training [6849/10000] ..........6849 ) Loss= 0.05394871\n",
      "Training [6850/10000] ..........6850 ) Loss= 0.12285273\n",
      "Training [6851/10000] ..........6851 ) Loss= 0.06293911\n",
      "Training [6852/10000] ..........6852 ) Loss= 0.07625808\n",
      "Training [6853/10000] ..........6853 ) Loss= 0.089489564\n",
      "Training [6854/10000] ..........6854 ) Loss= 0.08839318\n",
      "Training [6855/10000] ..........6855 ) Loss= 0.101640195\n",
      "Training [6856/10000] ..........6856 ) Loss= 0.06972915\n",
      "Training [6857/10000] ..........6857 ) Loss= 0.093496785\n",
      "Training [6858/10000] ..........6858 ) Loss= 0.061698135\n",
      "Training [6859/10000] ..........6859 ) Loss= 0.11728872\n",
      "Training [6860/10000] ..........6860 ) Loss= 0.10408737\n",
      "Training [6861/10000] ..........6861 ) Loss= 0.086298734\n",
      "Training [6862/10000] ..........6862 ) Loss= 0.13248222\n",
      "Training [6863/10000] ..........6863 ) Loss= 0.055680133\n",
      "Training [6864/10000] ..........6864 ) Loss= 0.07896726\n",
      "Training [6865/10000] ..........6865 ) Loss= 0.06495028\n",
      "Training [6866/10000] ..........6866 ) Loss= 0.15416715\n",
      "Training [6867/10000] ..........6867 ) Loss= 0.07375966\n",
      "Training [6868/10000] ..........6868 ) Loss= 0.069051124\n",
      "Training [6869/10000] ..........6869 ) Loss= 0.3129353\n",
      "Training [6870/10000] ..........6870 ) Loss= 0.062071554\n",
      "Training [6871/10000] ..........6871 ) Loss= 0.084700115\n",
      "Training [6872/10000] ..........6872 ) Loss= 0.077776104\n",
      "Training [6873/10000] ..........6873 ) Loss= 0.1325591\n",
      "Training [6874/10000] ..........6874 ) Loss= 0.059906185\n",
      "Training [6875/10000] ..........6875 ) Loss= 0.10370631\n",
      "Training [6876/10000] ..........6876 ) Loss= 0.13025624\n",
      "Training [6877/10000] ..........6877 ) Loss= 0.070125066\n",
      "Training [6878/10000] ..........6878 ) Loss= 0.085029095\n",
      "Training [6879/10000] ..........6879 ) Loss= 0.1291213\n",
      "Training [6880/10000] ..........6880 ) Loss= 0.04114184\n",
      "Training [6881/10000] ..........6881 ) Loss= 0.08031077\n",
      "Training [6882/10000] ..........6882 ) Loss= 0.06478102\n",
      "Training [6883/10000] ..........6883 ) Loss= 0.07275692\n",
      "Training [6884/10000] ..........6884 ) Loss= 0.087077744\n",
      "Training [6885/10000] ..........6885 ) Loss= 0.067603335\n",
      "Training [6886/10000] ..........6886 ) Loss= 0.10183492\n",
      "Training [6887/10000] ..........6887 ) Loss= 0.07539217\n",
      "Training [6888/10000] ..........6888 ) Loss= 0.062256273\n",
      "Training [6889/10000] ..........6889 ) Loss= 0.09035383\n",
      "Training [6890/10000] ..........6890 ) Loss= 0.131686\n",
      "Training [6891/10000] ..........6891 ) Loss= 0.0874374\n",
      "Training [6892/10000] ..........6892 ) Loss= 0.06642478\n",
      "Training [6893/10000] ..........6893 ) Loss= 0.077840276\n",
      "Training [6894/10000] ..........6894 ) Loss= 0.082154416\n",
      "Training [6895/10000] ..........6895 ) Loss= 0.15556273\n",
      "Training [6896/10000] ..........6896 ) Loss= 0.061399247\n",
      "Training [6897/10000] ..........6897 ) Loss= 0.07874075\n",
      "Training [6898/10000] ..........6898 ) Loss= 0.06672715\n",
      "Training [6899/10000] ..........6899 ) Loss= 0.10559536\n",
      "Training [6900/10000] ..........6900 ) Loss= 0.15642765\n",
      "Training [6901/10000] ..........6901 ) Loss= 0.050122466\n",
      "Training [6902/10000] ..........6902 ) Loss= 0.065135054\n",
      "Training [6903/10000] ..........6903 ) Loss= 0.17633352\n",
      "Training [6904/10000] ..........6904 ) Loss= 0.056686956\n",
      "Training [6905/10000] ..........6905 ) Loss= 0.072493926\n",
      "Training [6906/10000] ..........6906 ) Loss= 0.051984143\n",
      "Training [6907/10000] ..........6907 ) Loss= 0.08280212\n",
      "Training [6908/10000] ..........6908 ) Loss= 0.051269688\n",
      "Training [6909/10000] ..........6909 ) Loss= 0.08242161\n",
      "Training [6910/10000] ..........6910 ) Loss= 0.06945834\n",
      "Training [6911/10000] ..........6911 ) Loss= 0.09811888\n",
      "Training [6912/10000] ..........6912 ) Loss= 0.06340546\n",
      "Training [6913/10000] ..........6913 ) Loss= 0.057741012\n",
      "Training [6914/10000] ..........6914 ) Loss= 0.10435653\n",
      "Training [6915/10000] ..........6915 ) Loss= 0.1664517\n",
      "Training [6916/10000] ..........6916 ) Loss= 0.0774997\n",
      "Training [6917/10000] ..........6917 ) Loss= 0.06748726\n",
      "Training [6918/10000] ..........6918 ) Loss= 0.11868674\n",
      "Training [6919/10000] ..........6919 ) Loss= 0.1815169\n",
      "Training [6920/10000] ..........6920 ) Loss= 0.057134762\n",
      "Training [6921/10000] ..........6921 ) Loss= 0.26705208\n",
      "Training [6922/10000] ..........6922 ) Loss= 0.067917936\n",
      "Training [6923/10000] ..........6923 ) Loss= 0.11900464\n",
      "Training [6924/10000] ..........6924 ) Loss= 0.09554072\n",
      "Training [6925/10000] ..........6925 ) Loss= 0.08885713\n",
      "Training [6926/10000] ..........6926 ) Loss= 0.106114365\n",
      "Training [6927/10000] ..........6927 ) Loss= 0.06791391\n",
      "Training [6928/10000] ..........6928 ) Loss= 0.0942693\n",
      "Training [6929/10000] ..........6929 ) Loss= 0.12437739\n",
      "Training [6930/10000] ..........6930 ) Loss= 0.09275429\n",
      "Training [6931/10000] ..........6931 ) Loss= 0.14190313\n",
      "Training [6932/10000] ..........6932 ) Loss= 0.0875879\n",
      "Training [6933/10000] ..........6933 ) Loss= 0.10666088\n",
      "Training [6934/10000] ..........6934 ) Loss= 0.105814226\n",
      "Training [6935/10000] ..........6935 ) Loss= 0.084840596\n",
      "Training [6936/10000] ..........6936 ) Loss= 0.08480476\n",
      "Training [6937/10000] ..........6937 ) Loss= 0.11888882\n",
      "Training [6938/10000] ..........6938 ) Loss= 0.09119872\n",
      "Training [6939/10000] ..........6939 ) Loss= 0.064845115\n",
      "Training [6940/10000] ..........6940 ) Loss= 0.06753352\n",
      "Training [6941/10000] ..........6941 ) Loss= 0.10632348\n",
      "Training [6942/10000] ..........6942 ) Loss= 0.067122675\n",
      "Training [6943/10000] ..........6943 ) Loss= 0.08384399\n",
      "Training [6944/10000] ..........6944 ) Loss= 0.12939896\n",
      "Training [6945/10000] ..........6945 ) Loss= 0.15277721\n",
      "Training [6946/10000] ..........6946 ) Loss= 0.07720225\n",
      "Training [6947/10000] ..........6947 ) Loss= 0.08128062\n",
      "Training [6948/10000] ..........6948 ) Loss= 0.1106999\n",
      "Training [6949/10000] ..........6949 ) Loss= 0.06764985\n",
      "Training [6950/10000] ..........6950 ) Loss= 0.072721\n",
      "Training [6951/10000] ..........6951 ) Loss= 0.074833274\n",
      "Training [6952/10000] ..........6952 ) Loss= 0.11178976\n",
      "Training [6953/10000] ..........6953 ) Loss= 0.100527994\n",
      "Training [6954/10000] ..........6954 ) Loss= 0.09184509\n",
      "Training [6955/10000] ..........6955 ) Loss= 0.07508719\n",
      "Training [6956/10000] ..........6956 ) Loss= 0.08006918\n",
      "Training [6957/10000] ..........6957 ) Loss= 0.08480093\n",
      "Training [6958/10000] ..........6958 ) Loss= 0.076393194\n",
      "Training [6959/10000] ..........6959 ) Loss= 0.06359396\n",
      "Training [6960/10000] ..........6960 ) Loss= 0.072957925\n",
      "Training [6961/10000] ..........6961 ) Loss= 0.05854236\n",
      "Training [6962/10000] ..........6962 ) Loss= 0.11292392\n",
      "Training [6963/10000] ..........6963 ) Loss= 0.073592536\n",
      "Training [6964/10000] ..........6964 ) Loss= 0.17075181\n",
      "Training [6965/10000] ..........6965 ) Loss= 0.13557464\n",
      "Training [6966/10000] ..........6966 ) Loss= 0.07761393\n",
      "Training [6967/10000] ..........6967 ) Loss= 0.08662131\n",
      "Training [6968/10000] ..........6968 ) Loss= 0.07581525\n",
      "Training [6969/10000] ..........6969 ) Loss= 0.08286445\n",
      "Training [6970/10000] ..........6970 ) Loss= 0.07406302\n",
      "Training [6971/10000] ..........6971 ) Loss= 0.070909776\n",
      "Training [6972/10000] ..........6972 ) Loss= 0.04873715\n",
      "Training [6973/10000] ..........6973 ) Loss= 0.05070035\n",
      "Training [6974/10000] ..........6974 ) Loss= 0.1000247\n",
      "Training [6975/10000] ..........6975 ) Loss= 0.09076733\n",
      "Training [6976/10000] ..........6976 ) Loss= 0.14205438\n",
      "Training [6977/10000] ..........6977 ) Loss= 0.14428413\n",
      "Training [6978/10000] ..........6978 ) Loss= 0.060522497\n",
      "Training [6979/10000] ..........6979 ) Loss= 0.08446851\n",
      "Training [6980/10000] ..........6980 ) Loss= 0.07075154\n",
      "Training [6981/10000] ..........6981 ) Loss= 0.07392461\n",
      "Training [6982/10000] ..........6982 ) Loss= 0.04276169\n",
      "Training [6983/10000] ..........6983 ) Loss= 0.05246298\n",
      "Training [6984/10000] ..........6984 ) Loss= 0.071654506\n",
      "Training [6985/10000] ..........6985 ) Loss= 0.073126584\n",
      "Training [6986/10000] ..........6986 ) Loss= 0.066134505\n",
      "Training [6987/10000] ..........6987 ) Loss= 0.097803116\n",
      "Training [6988/10000] ..........6988 ) Loss= 0.3718878\n",
      "Training [6989/10000] ..........6989 ) Loss= 0.098361745\n",
      "Training [6990/10000] ..........6990 ) Loss= 0.043877825\n",
      "Training [6991/10000] ..........6991 ) Loss= 0.04210055\n",
      "Training [6992/10000] ..........6992 ) Loss= 0.08130265\n",
      "Training [6993/10000] ..........6993 ) Loss= 0.06688431\n",
      "Training [6994/10000] ..........6994 ) Loss= 0.15483229\n",
      "Training [6995/10000] ..........6995 ) Loss= 0.08748589\n",
      "Training [6996/10000] ..........6996 ) Loss= 0.07265317\n",
      "Training [6997/10000] ..........6997 ) Loss= 0.078968026\n",
      "Training [6998/10000] ..........6998 ) Loss= 0.05371145\n",
      "Training [6999/10000] ..........6999 ) Loss= 0.084731385\n",
      "Training [7000/10000] ..........7000 ) Loss= 0.04285844 - Saving Model7000.torch\n",
      "Training [7001/10000] ..........7001 ) Loss= 0.07779744\n",
      "Training [7002/10000] ..........7002 ) Loss= 0.09316741\n",
      "Training [7003/10000] ..........7003 ) Loss= 0.07083202\n",
      "Training [7004/10000] ..........7004 ) Loss= 0.070164114\n",
      "Training [7005/10000] ..........7005 ) Loss= 0.13434653\n",
      "Training [7006/10000] ..........7006 ) Loss= 0.13328986\n",
      "Training [7007/10000] ..........7007 ) Loss= 0.08034705\n",
      "Training [7008/10000] ..........7008 ) Loss= 0.10581391\n",
      "Training [7009/10000] ..........7009 ) Loss= 0.096101984\n",
      "Training [7010/10000] ..........7010 ) Loss= 0.077020586\n",
      "Training [7011/10000] ..........7011 ) Loss= 0.04227094\n",
      "Training [7012/10000] ..........7012 ) Loss= 0.06003825\n",
      "Training [7013/10000] ..........7013 ) Loss= 0.09584427\n",
      "Training [7014/10000] ..........7014 ) Loss= 0.31688204\n",
      "Training [7015/10000] ..........7015 ) Loss= 0.16926225\n",
      "Training [7016/10000] ..........7016 ) Loss= 0.06385136\n",
      "Training [7017/10000] ..........7017 ) Loss= 0.09430946\n",
      "Training [7018/10000] ..........7018 ) Loss= 0.0725022\n",
      "Training [7019/10000] ..........7019 ) Loss= 0.06693662\n",
      "Training [7020/10000] ..........7020 ) Loss= 0.063948564\n",
      "Training [7021/10000] ..........7021 ) Loss= 0.10970718\n",
      "Training [7022/10000] ..........7022 ) Loss= 0.09239373\n",
      "Training [7023/10000] ..........7023 ) Loss= 0.08261687\n",
      "Training [7024/10000] ..........7024 ) Loss= 0.043989953\n",
      "Training [7025/10000] ..........7025 ) Loss= 0.20105775\n",
      "Training [7026/10000] ..........7026 ) Loss= 0.104497485\n",
      "Training [7027/10000] ..........7027 ) Loss= 0.047093313\n",
      "Training [7028/10000] ..........7028 ) Loss= 0.07362443\n",
      "Training [7029/10000] ..........7029 ) Loss= 0.07470012\n",
      "Training [7030/10000] ..........7030 ) Loss= 0.07027318\n",
      "Training [7031/10000] ..........7031 ) Loss= 0.03817526\n",
      "Training [7032/10000] ..........7032 ) Loss= 0.0720454\n",
      "Training [7033/10000] ..........7033 ) Loss= 0.09187755\n",
      "Training [7034/10000] ..........7034 ) Loss= 0.044675764\n",
      "Training [7035/10000] ..........7035 ) Loss= 0.14515093\n",
      "Training [7036/10000] ..........7036 ) Loss= 0.17731382\n",
      "Training [7037/10000] ..........7037 ) Loss= 0.14548331\n",
      "Training [7038/10000] ..........7038 ) Loss= 0.14971443\n",
      "Training [7039/10000] ..........7039 ) Loss= 0.09625782\n",
      "Training [7040/10000] ..........7040 ) Loss= 0.055217113\n",
      "Training [7041/10000] ..........7041 ) Loss= 0.08349511\n",
      "Training [7042/10000] ..........7042 ) Loss= 0.14799534\n",
      "Training [7043/10000] ..........7043 ) Loss= 0.046109296\n",
      "Training [7044/10000] ..........7044 ) Loss= 0.13519445\n",
      "Training [7045/10000] ..........7045 ) Loss= 0.08011579\n",
      "Training [7046/10000] ..........7046 ) Loss= 0.06961788\n",
      "Training [7047/10000] ..........7047 ) Loss= 0.04337889\n",
      "Training [7048/10000] ..........7048 ) Loss= 0.1086097\n",
      "Training [7049/10000] ..........7049 ) Loss= 0.12692694\n",
      "Training [7050/10000] ..........7050 ) Loss= 0.15319887\n",
      "Training [7051/10000] ..........7051 ) Loss= 0.06660838\n",
      "Training [7052/10000] ..........7052 ) Loss= 0.117912084\n",
      "Training [7053/10000] ..........7053 ) Loss= 0.11746585\n",
      "Training [7054/10000] ..........7054 ) Loss= 0.07542068\n",
      "Training [7055/10000] ..........7055 ) Loss= 0.06759834\n",
      "Training [7056/10000] ..........7056 ) Loss= 0.078645565\n",
      "Training [7057/10000] ..........7057 ) Loss= 0.07630313\n",
      "Training [7058/10000] ..........7058 ) Loss= 0.06461545\n",
      "Training [7059/10000] ..........7059 ) Loss= 0.079188816\n",
      "Training [7060/10000] ..........7060 ) Loss= 0.16760989\n",
      "Training [7061/10000] ..........7061 ) Loss= 0.040853273\n",
      "Training [7062/10000] ..........7062 ) Loss= 0.10375087\n",
      "Training [7063/10000] ..........7063 ) Loss= 0.06262984\n",
      "Training [7064/10000] ..........7064 ) Loss= 0.049477834\n",
      "Training [7065/10000] ..........7065 ) Loss= 0.10162718\n",
      "Training [7066/10000] ..........7066 ) Loss= 0.10306234\n",
      "Training [7067/10000] ..........7067 ) Loss= 0.10483164\n",
      "Training [7068/10000] ..........7068 ) Loss= 0.09047957\n",
      "Training [7069/10000] ..........7069 ) Loss= 0.08780944\n",
      "Training [7070/10000] ..........7070 ) Loss= 0.1407737\n",
      "Training [7071/10000] ..........7071 ) Loss= 0.06857552\n",
      "Training [7072/10000] ..........7072 ) Loss= 0.077990025\n",
      "Training [7073/10000] ..........7073 ) Loss= 0.07012038\n",
      "Training [7074/10000] ..........7074 ) Loss= 0.048765294\n",
      "Training [7075/10000] ..........7075 ) Loss= 0.04563952\n",
      "Training [7076/10000] ..........7076 ) Loss= 0.36482465\n",
      "Training [7077/10000] ..........7077 ) Loss= 0.059984926\n",
      "Training [7078/10000] ..........7078 ) Loss= 0.073117875\n",
      "Training [7079/10000] ..........7079 ) Loss= 0.05900609\n",
      "Training [7080/10000] ..........7080 ) Loss= 0.040167235\n",
      "Training [7081/10000] ........."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 39\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     38\u001B[0m \u001B[38;5;66;03m# Get  prediction classes\u001B[39;00m\n\u001B[1;32m---> 39\u001B[0m seg \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mPred\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     42\u001B[0m \u001B[38;5;66;03m# Print loss\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for itr in range(10000):  # Training loop\n",
    "    print(\"Training [%4d/%d] \" % (itr, 10000), end=\"\")\n",
    "\n",
    "    images, ann = LoadBatch()  # Load taining batch\n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "    # Load image\n",
    "    images = torch.autograd.Variable(images, requires_grad=False).to(device)\n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "    # Load annotation\n",
    "    ann = torch.autograd.Variable(ann, requires_grad=False).to(device)\n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "    # make prediction\n",
    "    Pred = Net(images)['out']\n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "    Net.zero_grad()\n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "    # Set loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "    # Calculate cross entropy loss\n",
    "    Loss = criterion(Pred, ann.long())\n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "    # Backpropogate loss\n",
    "    Loss.backward()\n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "    # Apply gradient descent change to weight\n",
    "    optimizer.step()\n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "    # Get  prediction classes\n",
    "    seg = torch.argmax(Pred[0], 0).cpu().detach().numpy()\n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "    # Print loss\n",
    "    print(itr, \") Loss=\", Loss.data.cpu().numpy(), end=\"\")\n",
    "\n",
    "    # Save model every 1000 iterations to .torch file\n",
    "    if itr % 1000 == 0:\n",
    "        print(\" - Saving Model\" + str(itr) + \".torch\")\n",
    "        torch.save(Net.state_dict(), \"torches\\\\\" + str(itr) + \".torch\")\n",
    "    else:\n",
    "        print(\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torchvision.models.segmentation\n",
    "import torch\n",
    "import torchvision.transforms as tf\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T00:26:43.031666Z",
     "end_time": "2023-04-17T00:26:43.369744Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "modelPath = \"torches/7000.torch\"  # Path to model\n",
    "imagePath = \"test.jpg\"  # Test image\n",
    "height = width = 900"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T00:26:39.597962Z",
     "end_time": "2023-04-17T00:26:40.168528Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "transformImg = tf.Compose([tf.ToPILImage(), tf.Resize((height, width)), tf.ToTensor(),\n",
    "                           tf.Normalize((0.485, 0.456, 0.406),\n",
    "                                        (0.229, 0.224, 0.225))])  # tf.Resize((300,600)),tf.RandomRotation(145)])#"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T00:27:29.896697Z",
     "end_time": "2023-04-17T00:27:29.928000Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')  # Check if there is GPU if not set trainning to CPU (very slow)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T00:27:30.498148Z",
     "end_time": "2023-04-17T00:27:30.529411Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Video-gamer\\Projects\\Segmentation_cnn\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Video-gamer\\Projects\\Segmentation_cnn\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": "DeepLabV3(\n  (backbone): IntermediateLayerGetter(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (4): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (5): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n  )\n  (classifier): DeepLabHead(\n    (0): ASPP(\n      (convs): ModuleList(\n        (0): Sequential(\n          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU()\n        )\n        (1): ASPPConv(\n          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU()\n        )\n        (2): ASPPConv(\n          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU()\n        )\n        (3): ASPPConv(\n          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU()\n        )\n        (4): ASPPPooling(\n          (0): AdaptiveAvgPool2d(output_size=1)\n          (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (3): ReLU()\n        )\n      )\n      (project): Sequential(\n        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n        (3): Dropout(p=0.5, inplace=False)\n      )\n    )\n    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): ReLU()\n    (4): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (aux_classifier): FCNHead(\n    (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout(p=0.1, inplace=False)\n    (4): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n  )\n)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Net = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)  # Load net\n",
    "Net.classifier[4] = torch.nn.Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))  # Change final layer to 3 classes\n",
    "Net = Net.to(device)  # Set net to GPU or CPU\n",
    "Net.load_state_dict(torch.load(modelPath))  # Load trained model\n",
    "Net.eval()  # Set to evaluation mode"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T00:27:31.093879Z",
     "end_time": "2023-04-17T00:27:32.103414Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "Img = cv2.imread(imagePath)  # load test image\n",
    "height_orgin, widh_orgin, d = Img.shape  # Get image original size"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T00:27:36.327431Z",
     "end_time": "2023-04-17T00:27:36.477456Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAGiCAYAAAA2g3fNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOz9W6xu23bfBf76ZYzvMudca+219z57n2P7OBCDYweCUS72qQJVKWVioQgJkQeUh2AhniycBw5IYAmRGASWeIEH4A3BUwrIS0kQVQSx8gSOrEpKlfI1duL4+Fz22bd1mXN+3zfG6L23emit9THm2idV8WGfOs726kfr7LXmnN83xzdG7+3yb//2b0FEhNfr9Xq9Xq/X67uy4nf7Al6v1+v1er1+P6/XRvj1er1er9fru7heG+HX6/V6vV6v7+J6bYRfr9fr9Xq9vovrtRF+vV6v1+v1+i6u10b49Xq9Xq/X67u4Xhvh1+v1er1er+/iem2EX6/X6/V6vb6L67URfr1er9fr9fourtdG+PV6vV6v1+u7uH5PG+H/8r/8L/kDf+APsN/v+dEf/VF+8Rd/8bt9Sa/X6/V6vV6f6vo9a4T/+//+v+fLX/4yf+Ev/AX+1t/6W/wz/8w/w0/8xE/w/vvvf7cv7fV6vV6v1+tTW+H3qoDPj/7oj/LH//gf57/4L/4LAFprfN/3fR9//s//ef69f+/f+y5f3ev1er1er9ens/J3+wK+1Zrnmb/5N/8mP/MzP9O/FmPkx3/8x/mFX/iFb/maaZqYpqn/u7XGxx9/zJtvvkkI4Tt+za/X6/V6fbaXiHB7e8sXvvAFYvz0QITfk0b4ww8/pNbKO++88+Dr77zzDr/2a7/2LV/zcz/3c/zsz/7s/z8u7/V6vV6v38frd37nd/je7/3eT+39fk8a4W9n/czP/Axf/vKX+79fvHjBF7/4Rf7YP/WHyCFRq1BbYK4JkYE8XEMYiHlHGo7srx6Txz3j4Yr94ci427E/HhjHkWEcGYeBlBMhBERAgBgCwRxiipEQIyEGCICgEXgAQkBfJLSmUTriX2ooICS01mhN0SH9ewMCIs3eK9CaUGpFqiAitCbUIjRpiFREBGn2frXZv4UYA602aqsEEWIQUiq8/bnIzeOsH4iHyFQA6jxR60xqhcTCzZC5TPfc3t0zTRdaq7TSmJcZkUpOAzdP32F39QZp2JFTRiRwfzrTCOx3Bwjw8vYlY05c5pnnL18wBHjn7TfJCVoThiFzPB5AKh99+CFf+epX+fjj57zz7rt8/vPv8vjJUz766CP+zt/5O8zzhWWZqctMjpGr45FHj29IKZNSBhHmZaYsM0tZmOZKA2KAnBMpJALCkBIxBmpdqLUSQqDWBiEw5ERKA4RIiIkhj4w5EUNCghBTYFkWztNEWRZCiPonDaSUefzGm/zAP/GDhGHHr/3qrzLd33JzPHI+3fPxs+fM88z9+cSv/Oqvcnd3S61V90KwvRUiEgJ5GNnvbxiGPaXMXKZ7alkIQTgcbnjy5G2m6Za722fUpdj+ElKOSGtIq1ymC6VWACKBw+Foe1aotbIsC62jlMK3witjjAzDyPF4xXQ+M88LMUaOxyNDGrg+XnO6nylzJceRKJEUIykmYoikmMkp8ejmmi/+ge/le77/ezjc7NntMjFFYgwMQyKmRIj679rWPb8slbJU5mniclk4Xc7M88y0TFzOZ+bLiblcaMtMaA3QM+BnMqekZwaBELFDiIgQQiDnTMrRzru+tiyN//Gv/Tw3Nzf/O63Vw/V70gi/9dZbpJT45je/+eDr3/zmN3n33Xe/5Wt2ux273e4TX3+0Hxh3AykEQgyEmOxgFqQVlnJinp7B+SNivYb5yO2LkRZGUt4R8o79/opxv2c47jlcXXE4HNjtdoScSDES7fCG8PAP2EONkWaGVESIIaxGOLjR1Q1WayXYtpcmiLhNF5r418R+3oy2GXPs361WSm20KmpERI36shRygCBCrRPX15mrm8BulwjuOWyJqJORYaCWCerCGArHMTIMeoU5J5Z5Zg4ThEyKI4TGOAT2+x3D7kgICanCZSoEAmnIajD2e3ZDJubE+XIhIbzx9ClDCpwvZ2KM7PZ7AsLx6opx3JNyprbGMAyM48jNzQ2H/R6RSquFkAeCNMZxYDeM+pxjIgRRB9UiEgaEoEYYyCkTQiCFQE7J7r2Qc0ZEyFnAjEeIiRgTOQ+633IGiUAj5UQZijquPCAIQiSmTB5Gro4HjscDU2kMY4ai+7UsC8fjkTwMnOdJnSZACIRohsPq53Gzt1KKtJYIIRFCIcVEHvS6SrnofacAgZhCT59LUyOjz1sDhWm+gOgZurq6Yp5nlmWmiazBwMYSS7C9IY1pupijEhz1izGy2+1oBaa2kEIgEokhEoEYYb9PvPHmU774/d/HG289pcXC3d0Lbm8rtRVqLcQA03TmMk0sZaGUSin2PVaIMQQhRHdWEGJgzMJhTISw1/sVM8M42Pcz0aKn2tTJigUsCGrsg37uTfyEyNLv2ae5fk8a4XEc+aN/9I/y8z//8/zL//K/DKgR+fmf/3l++qd/+nf3XruRnKNGjLVCrZSyEKMeut0OjrtEayf2w8J+f6IukcscaWEH7QDTkfk8cnqReRlH0rjn6tFjnjx9k3G/o7amBj4EUlbvnVIC9IHFGGhIj6KjeWMRiDFsImI1AL7E/hccf2oNofXIq9VGCL5RGiKNFIWUhFRBWqDWYC+N5JxopVFLRUJlGNXjI0LMiWD3GSCliDSBEEgp0VqhiZhT0c+UYqDahkwpkYKasNaavlY8otfrm0vlcDggrRGDeRdgyJlaFjMkmbHJmlGgjlM3fmCeJlptLPPCkDIpqpGXBoHV2LTWEArZrimY0VDH1dQSELph069Dq00dJgGSXkMMAQnQpKoFQj/PUgo5ZkIM/b518xY08iMEM+yNlAIUzVg02ouINPKQkQC7/Y6YYs9K/Kx7dLaNTZvY3gAzKHoNkxlyROw61gjPX41oFkAI6tSrdMP5hS98gRQj0zwxzReWaWK6TMzzwjRrhqDXE/XeYbdSdD8MKZIThCgcjyMpCDlmi3wjOUeGMbMbR/Iw8/7HX+GD518Bmr5X0veNMRL09pEtA92PkXTck2KCEPo5ijFSayHERE5ZszMRYkgspeh9bkLDgphW9N+WIUoAqcKQBs0eQ6BJIKRM6/cxIKn9rmzPP+z6PWmEAb785S/zkz/5k/yxP/bH+BN/4k/wn//n/zn39/f86//6v/67ep9HNzdqhKXpw2lCLYWGGpRShaUVam2cLgv5fFGjFCLDkNiPO/b7A0MeycMO0si5JN7/5lc4f/iIvLth2B9Jw8hw2HGeJuZJUzNiZH88Mux2DMNASsmMi0adgUiIYplQgGiHx40P0EQspdevS2tqVIBgn8mNiB4wgzpomsom3Wh6SBqSYAkVGRrX1zuGIetGRNSAWJQUPSaMkSZqMKiBGBOBRE6JkjIxNWJUJycWTJe60KRR22o8GtBqVUPYKnlIhEC//iFnyygSMWUzHqHfi2gp6VKKQSzV0sSo1xQirTViSt0RpLgaJ41q1gwlmFEKQHRYh0YthRACh5srNWrTxbIRN9YenQZLZaUb/tWR2vvGpK8NGnUHRKM2f7VBSAF9j/1ur8YnBIJs3HFQoyeiEJM6JnvmFgXSxIwXq8E2Ix5YnYPEYNCLWrhC7XBYKTPz5czNzQ37RzcEbvR+ibDMC7d3dyzLjEDfz37vU1Inn9PAOGTNMOzyU0yMeSDQVqNqnzMP2d5H72FKiZxzf16g+1k/r+7vYo6gVjW2WGZZpQATEEghoU9UqPOsEXCIuqebnqm5FFJK3cm1UhEJGh1HYYiZkNTJ6Wu/MwX+37NG+F/9V/9VPvjgA/6D/+A/4L333uNHfuRH+Kt/9a9+olj3/2vtDpr2hqCH0SMhR7pqUQNcq1BLhQC1atpzmmZO00J78UIjWnsQISZKjUjbEeJIHg4QBkiJRqS2SG2JRqJIIOYBYiaPO4b9njxmjscrhmGEiMIMIixlXqM2hCFnRNDoOmikEd2Amy3XA2bRdcSiTNSY2+ZF9ODlKGqc80zKlcNB02+PeCWwGuEYETQ6jDEQckJED3eM2fDv2DdwiBGptUcdHo2podLrKLVSmzqObFFmba0bU2BTdXZgQA+mhkXR0lE9hCkljRztgMWYuqMVvQWWUtcVHjJ80J2chbp6T+3vEgI3N484n8/My2JwkjtIP4gaabmR12xhE3V2I6lGKphDWA2wUGqhlspSCqU2bu/ulOFjDzWIP++wHn9RMCWIILUq3mmfNyXIGeapEu2ex5A7fBFDIEjm8WHH1dWRYs8pJs3ehiGz2+0YB8VlA5oFjeNAzpk333xqTiL274tlLSklBCHbXo0EdrsRUAdblkWfWdTXNhGkOUTnkbpQSmFZijm6FZFeoQHpjq5K3WDXob9XJNCiG2l6FhqCZhAhRJbSqA1iCuRx0Gcp0QKjSAuQc2bcjX1vzGllX32a6/esEQb46Z/+6d81/PDqWpZCAlLWIk2MejA1fYmknNUKshqfyzRRSjF8VYtZfvDNGZNCocVKk3sul4+BZL8xIWQaAyGOBBKtDIQwMJ8CEwmJAx9JIO92pGEgDSMxDxpd5GiRZeNUFpZlodZihZpGThEJilkOOTMOQ48uokUjQ4pEgm54T+vQyJlWCLHy+MkVOW9wYFkPpBdo9MuWvsaExNSx1JQGYixmYCOJQGmOqyk+DUJKUMWNurAsCzGKRYkNaY15vrDf7ShlgcNOHQf0dNwPGAS7H2rs8zDa9xSzFT9ssqaqRP13tSInaLEtIISUAMPdPQUWdSwffvghMSUOxyPDkCmlcLlMag6DQR/B798G/7faQ09u/LMki/DZFL28wJsibdGo7LDXgptUhYeSQTEp5x517vcD+11mnw8M6YoYoNEY93uOVwfeeesJ8+Up2DNMKarDCpBiYDeOjENmXjTql6gZxzYi9aBDC1QDKUbykBT/9TsZIkigimYQ0gQJGuEHoJSimVEpYBBPKcUCDcvqxBxvoBcLm/2OYRygNUqr9F/amn7fImMJqIO27M1AXyuea8Q9JN17KQaa6N7NaQALzHy/7fdXlLkQQiLmTMoalc/zoo48frLm9Gms39NG+NNYrVYu0shW0BmG1JkGaic87QeCGuhxGC3KWTFSr5DV0vrDr00LF2qgdbOJCLVNNJlpcjYvH0lpoIoAA1IzIpE2DYSygykjYWARqwDb5o95IOeBccyk/X6t4CKUZWI+3TM1Nc6lVppoxLff7zjs9nZwMrsxMw4a9aYEb7/9BnmIDyJEN0LNIIQUvXBRbbNCTiMxNlrTok1p1VJHhRk0tWuaSlvkEgy7yzmBNDXCSdjtM0GSpZUaAdVWe1HKPqgWXJJCFSJCLQpzREtbcx718KMRmRtJMccRoz5HaZVlmRQaMFgBg0PUgGORPZ2BcnM8cnV1BcA4CCmqMQ4GkyhUlLqz830hXgdohgVnNdplWTif7pFaeuYTY2CMGdkJb+TH/FN/+If0Gi07yGYYU9bi6dZR+mcQc35evI0hEB9fbe6hwgWeMUWr+jtmz8aZ+DNcYRVnBAnzvGYhrVUCwTIZNdie2fQMSNxYB4aU+llqrRmY40XqRhMIUTHYmJMVyAAiVRoheAFasVoREIPyYsiG1et1NPSZ5hghJMScTwiQQkRChJTNEGPMIlhKI+XRiprCUpoGN1GIYQCpn7J10vWZN8J5P9j+iooPNU2JgqhXL7WQc+ppToyRVrcsAUuVLeXWv9rGDNI3pR66SmtQq8Id87xQ7X2bLJQiNClAxBAyoiRC1Sp3lERsFkXVCEtmCRo91yZ6IKzIkYZEjolhHOxQRTM+ZqhbpcwLrUxQM2UO5CHxxhtP9L2WRq2NnKOl0npAFHLRgyCGP8aY1oA5Ri2qWeEs5ERsmboshpkZBiyW+vUDRi9EuQGg46JtjUShGxssgk8pmqFbcdkhZ2qr5CECVmBp6qDUkYWOo6oh1ufXmScWtUaHOqTp9UY1dilnlnnhnC6Mg0ZNu92e3U4YhkFxQgIp5Y43Hq8jBy+lR0sGDIe+nO74tV/5ZaUIWmHSi3Kayei13hy/gLRGK2WFUghUqfhD8HS/Q1cGJ3ma73UFL6RikT5ikFyVnso7a6fZe4RSLZLVYqeI3rchDwhwucysIAEgbvjUDjb/i8FgyhEJ1KLRMXZ7UlRH0kSooLS/YUcwfF6djDqDkAbNKAJI1Ug4WrGuNXXSIhrJq/PP5oSzXpcIMSn2rDCI0FpgOOxZlgUwiKQ0BKWzEvTenc6XzlJaltdG+NtawzAyDINVQqM9XN1YtEKMypHNWR9QWYqlq2ZMBUKorFWOqMWVoIyHHk1J6IWGcacR2fEgvUhUSzUMLiAExZ0N5ohBECq1sXEAQkjRIvREw4xJUN5tqJkUMjInaFmxR6JF4rqBiZF5LpxPhePVNcd05KMPP+bly8wwJAgLIdrBtmguRjV4u3GnxZaYlCAljRyhDIGcGnNpVBJLaRAGami0YIWNKgytUlsloYU/j+YUN8wdz44ofCGtMU0zx6OlpK3pQbVry1kPUQhanAsxUObKbtwzjjtayz31zzmzO1yR00BMSTObq0daRW+N8/lE8FQ7qTlTKGcAYi80IY2UNZJXeKp1w1AWg6sMo1YY3j1Vo7RifmaFsPqjDWoItDDZ1kjesH43nI6lt9os4g3d0FTR2oXivlGDAeMFax3ALkXMZFpmJ1ULqdU+R8dYQzSjXAw/jh0WGPNIlcQ0TzRyhxsEM6YhWXSsRqwhBPFqBcRBGTitFoO2NLNpVmTOQ6JKozTPHoxTHILChkE/Q05RM7PYFKUNSf2nReQxRSev9HNA1LNeq1DqAgRjTgQ9ixKZLlZ8DZHWFpzRpA5dC4MpDZxO5//d9uhbrc+8EdYHlZUeBFp0tzQpRzWompZrCrjLmVbVANSywgzFKvsASyla7QdSCpSyEMS4plnTO6+6tmoV+5BIyR90UzI6eU37LGoLUSOb2mrHUWttWtCSCmGxgxUUb24CRQ8CITCmAbJzVBWHrBJ48fI9LveJm5vHXO3fIAc18mIOpdOC0Mr8cj4zayjH5XLmdH/HYb9jNw4US+v9dfoZRiArv1ICz17ccXs/AxahoIW5GBOJQrmcEGC6nBkS1DLz4tkzWrWI2iCRGAJSC1dXV3zv934vtQlX11ecp5kQIu9+/vN87t13jC4VViOG/zd0DBnU6O2P1wQR5nmyxozCUgvTPOMVvVoXM6ya1Shsoji3F5I6VAU92hSL4LzgGAyv9GwMDK4I6LMxvBSsgaC1zoDJRpGqnqXExLTUbsADAVpgabVHsiKB0NZSXgt6BmLQwipRWKR5uUuvLVi2g3RcTgCi6DloAaERYtZGH92w3XgvBsdF5+B3yEK/n/OB6XKiFHUK+91AaUKrGrgscyMPCitN08I47inF4CwStRbliBeNYCEiVaBoXUeNZOb+PHUWCkBpbWWbYFQ0O2sxBu7uz0yXi0JM9j7BjLRClmsQBRPzXP53WKJ/8PrMG2E/0D3lwooCVh0I1p3TSiHlwYogSh+LWaGMAVmpYUFx09JEGyJaJecR9/vKUWxgzRHzPHeea87ZOI2WAloUopzi1gsoIQRSC92IjMOg3jultUhhKbvCp2poNCpZ+gEILdJqoEnj5mDUrfmWlx9f9NBZBd/f28GAZNGwY+cEodWJb3z9A45XR7LhafMyGwVQ2RKCprEpoKloq1rYtMiVJlSZmWqktWKVfmE3apR2Od9zudwbfRBzCOY1DWcMIXD38jn3L1/YPWgdQm7W5eZ4aS+kWlqvKSY9SlRuakDr514QcsqgOGKC/0scd7QCj6f9wahoIoITCEUUK16BLdshQdkXxKhskWoFLjAjo4ZbKVOaNYml9Tlmc8QQU9Lot2mV32EUIwB3XD3YhhdP/5uXVtW5xKQRb0O5sY6PJjNKjUyr1XBZp4mBUW8IDVoLVFE2Tg3CMld1LCESo3A+X/QsLAs5ZS7zQjBz2UpTdkLek3KiSqQ0YRz3mnGUBeLIUoXL+QyIvVKfSk7JzlNlnmd7YAqBxJR6Ftus81HMJqRUuZwvxqQRjscrlmWmFN1DyXjcWjvSJqjwbdiff5j1mTfCRWDopyn0Awt03MfpT3oApFPRMEMZYyQYBSmg5PSBQGzrEctRC0+1VUothBDZiRjdTb+OaLQTQtUjboaimsd2ek7KegD8ZwJqKJN4QbCt3VNE41Ym6+pSjNQz4xDSJmIL3Tjl0ahkGK0nrtivY5ICLM26hGLk5vpov7NRy0RCON2+hOBpqRCDOhOaUvl6hGb5a4zQFjruqNQjw9wN125NMcuIYsgKkWgkrUUdzACGtSAm3rqApcKstLAQlUEgGzqcbQQN9i2viH609f9rURxW6U16bzVDDyRrLlA6oR7uaoU4UCOWreW2Wdcihjk3lKsuWinTYppR/qKxOLTgKZb2K54RQuzGq7aGhEQLAQwLDjGSQjIHYawD46WHqN1qdVIapAapQWEN+4LDDwRYihqvsMzagdm0dlL7z0SjdmqG2Jp0zu88z5ppZuUST9OkDjJFqgjz5cLV1Q2X84UYFO66O5149PhGG2aa4tnzrJzfPGQisNsfe4s6TRjHTBUotVHLyppJEaoITaoWhG2fKYhhhjU260BVZ3Q+nzRrdWdm9/iwP3A637OUwvFw9buwPP/w6zNvhJtFkzEmo3lhHV+KsXlhIuZhTT+N4sKDwx3MQFhXVjCP3I2hRW4pEIekRRARBjRKqGY8Ulg1HJp5WdQ+s7RlxeiAnvpaOrwsi6ayduiyUXvUUKT+mXttKCq2l7JHcG6wgIjScHrnlqVprF1nOUaIUGqx9BPLWKUbwWDRaER5mMk5tyEo60C0Yl9K6ZH32qGmqbd3r/m1a75ruKRFmzEmhkH6M5WqxRZv6SV52h8MGzbqmlG7pEMF9G4zNY7OrIibLj/pugJaGHNs1YpQblDFsVqHGAJLLQxDtvvU+vPCCkKgEXvMymmWoPBCHvfqYLK1sMdISF4jsOe/MdCL86Wh781SK83wYEUeAmUuxJTYjZnz5cJkhs0SQa1XeIdj08+rTsWbSqwYhrAspUMsaRiU31yU01urBh9KA9WscxgH7u/uNNJujZubG+ZpopZCWZ4rfJEb5aQZ4+3LF52eeTgcrHU5UkomxcjZjCHSbG9h58JwDt//hrK01sjDnsv53J19M5ZFK1XrATHSTC/Dn31K2RxV69lqLYXT+TUm/O2tsMGCQHFQS7l675loS2qMRn0RxS6VpuZRpUY8RDQ1w5s3BNeH0OhVCzra1BF7SuuGOsZIzEArJPvdwYpZtNSjqwBINwC6GapfrsiDIpQbxPUj20FsGH05rhCF81Y9Ea9qULXtOHY8O+fMfhx0k5fFWnCtiOVcUiAn/7uQI+SoeKe0SohJaWeWwRIUG27GmEBWbijbjiRxnq01hCD6QVKwaDP0iCamrFxvu88xJVottKpR5/54pYyYlBnGkaVoWryUQllm+9n6QCehsyqSFvJKWdZD3BoBbZN1ulocQoeZDvtDb32OVuApS+lUvnleLOtqVAm0pr/rUpwHnszYQa3FinorhhIDBDOGl/PZRKS0MFeNSeGc6pQ0u0rDwP3dPcs8d4gpmKEMIVBqMZGn2KmFvWPNHp7DcIIoZUvWwODq6ooQ9J6AME0zKSXu7u4oJuyTUqKWohBNfKD80O/7fr8zSAEul7OeryS0uXFaFmrT96+lQhaWRYurpRaSP5sQKHa+ayvIvdPlLJuIyfaOGOuh9b0DdINbo8JTL21/iDQul/vfrfX5h1qfeSOsxa2wGkoMu5PWMWFtW7PnEKPyXEWNrVrwjZaB0CPH5g/QltNhqnUwuRFWJ6DRULU0sLkoi1GrQtJqcMfyxKlVWslt0hiIdjlWCW4aiUVTgfJuJmmiujLQU/ZmUZPyaKXjim6OQwiQkn12xcCXUpUpMYw4pWu3Gyz6t5Q7RnIMDCmoEU6mOGbdfR5llSaaOsfBDnOk+tdbM/jHcEIJSIiEkJCgKawajqwiOkn50yIagaoxTj26qVVZJ8HEc+Z5sagnkUrl0TgSU+T29pbT6d6aYYp2sFn01ZppSFQ1bBll2lwuE6UsFE2pqNJopTFkw4kr3J/OiP18KTPLsuCNQM143cu8aDZkGHBrKuxT68WMpTpRTe8XDQGCbUDdxSrIFFW0pto99KxkGEcu84yURsyZeVksqtQicx7UeeWcWeZZ77FsWoERduNoeyDhHZAQjOLVGMZRMxw7Z8uyngctSnuBTmGgy+WiWVBnRQDFOlSXmYusbeM5ZyoQm2UMItS2aKAgjVYDJSz9vZsVmLUZKPZgI+VMDMFU/nRfN2kaAFmHaWtirckWmLRKRM9eLYsZ6gDtNUXt21rRIz4RgqwiOoClpJXgvflgafmmqAFGDdMNqJCgRdP2M97Hru/ppQNd3sfvxq9Z1T+I0ZOCMRSwgo3/3qCYqmtBeJW3SrMmhGhp/JqJKbztehSWqNrnwgtjrapxE3qBELs/yShQiu8a3U0qOQyvcInt51JkzJExBsYc9O9D4rgb2I8DwziqYWiq6jZXoUqitkhDW0fnWtToAiLRSnsRYiaNOwgZYkRCAiLDbm/iNyZ5mHK/rloV11nK0p1ubcK4zEoRtIjvPC/EEBh3exVrqZXL5Uy9nBiGnXbliTbmVHecBPbDiEwztQpNHK2NzLNim6015mXhMs0985imqRvhw/FAIPLi9oWKxVh0vRSlYcUiVsjFoJSktCrfa0G7/ZRpqcZtEVHH0xopZcNTDVKxPbnMsxp2Yu/W0zZdTO4SOjfb9nMMKPxlm9w7No/7A9M8sVwK4ziacZ2ARlkW6+qjQz2tta5I12zvdOUy0c7VIWfmad3I3mKMZaWlFgVQDOeNRiFs1nGnnZd0fDrG7M2CgBbQ/Xwm6zoUh4eE3pgE9Fbq1krPchGND1aY8NNdn3kj3Bz8kpVgrql8NJ5v6OneVuAFnNyvRlOjZ+neUoIaeI8QnbuJpYKgXWCqe6Cts178sxaIjq8J8oDfyMYwBlbcOjhGbRtPg8C4Mbhr4a3/cU6zGSoxHQkv5ASDRLR4UUxBSkASIQUSsb9WP4sdTtQxNFFnEU0/YPR26lFlFZN1d02lkWqgtMhlUTWynITQRjW+QjfALSRCHonDnjzuiWlQh5QSh8Ne22hTJA+q1ds7F0ultWqtzYp1LsvCPE/M80Qsiqkr9VB6NEsUQs7EYdQo1wqsElwnw9TgxPaTd5U1ZSfEnJGoBtJVu7bCMKNFjN7Ykx32asoP9wzI5RU9MxFpVhxUTH0YBhDVORmGoT9X/z3KqVXpytqU1+wcZzVCppHcGrEm2qxdj27gUnJjpJbXNY230axPr8mmFNi7Aq0+EPs1td4G3azwvKUJphSZ55kYAofDwXDkVRGwVCu8CsRgVE/LBFx5UN/H6gsItZV+fj1omeel24EQhCr+ejE83yAJO3/B1O+aROM1q22oZf2ZT3t95o2wt2tq85IZNNxWBhP7wLyu04MAK064x+wFrbAWfrx4ASiM0bwP3Yx89TZmjT5TSH2TaN997R7ci2YPDhagcEQ0YWrstVujWtfiYI/y14jG6U9bzmoDE6UBb1HW+2EVeqfmGE5bm5BH6zhqVaPDedK/l0QrkTKoWL42ZQipNIiV1DRVngtcFlhao0hgqU1FVOyeihheHyOEgRhVbH88qpbzuD+wO+w57A8cr3bsdy6NmCilMc+FeVbMdVkq5/NZBdwvE9M8crpPnM8nxjEgg+mBlEK5VJwJk/NonFzVSWgWQbmqlxahEsOgT6bWQJPCYK3Liyj/exzHh6wNc5LanbVywf1nfNWqDSCO88/zpMbfCluOTYvotbjzV/2Dh84y58GkNi2ttj3mjjfnrA02s1Oykn123XfuMLA96J9jKUU1GBBYFoNXKrjGhTkfWHFlaY2pLAx5YL/fa2TfqtIgYjKergdDdKgrBDr23js5DW5LY6CY0/Uz6oFAE+uqs/fQYFbPb0pafIwxmZiTGwM/H/o5d+PI+Wy84BCB+qCT9tNcn3kjXEuhmYdvsmKhPoXC6UuaDqtXFVEIwgsXgMkyapTiXU5eAIlmtLu3t0LUMGTDP9d224fRNg82rkcNrgTmxjSbuLeL3oC2XjuJP4TAMGSDF/RaVNOi9QYRcEPs0J6Kr4QAEh0u0Qhf8e/Y/xRp7FMi5Mx8mtb2Ybsvc1EjEIrAXAwrDYxV72OVyFRgbpFSBYkJCQMFoYg2AYgJ7ktIhDQy7A6MxyPD4YrjzQ3HqyOHw4HjceTRdeI4Bpf7hZCobWBahMss3J0K6S4zzQt5HOE+4UItl8ukkyha7LjyvLh63QLB22A1VV6muU9UmeeFnBLzUjvWHtIKVa2KYCun1ilbtRZqc7xZ+dOB0ClfMWrTQ62VSy3KtFhmKxAHc1TGYzaj6xhuCIEcI7M57x51t0bxAhvB9qKyMZZ5ITqt0YxdylmNOjAMacVbNwFADK4R3ch5wFvRfe9v93iHr2IgWXCwzLOylKWZ/GXuusRaY6mkNFiGoZCJByPgrBohS2brwPACusN86BQbD5hiiCbEpb9XpJHTqGe7wy6mbWFOrmcrojjyg+acT3F95o3wdgnaTunRRK3aitu5nuJRpnIk/Rl7IUrTKqtriVj6uLInEK2segpukJMxHZyDiAPPCpGIMx30vZwCd5kuiFXLQ3AebwD0vQMwGO9yTc3UsDZxacXWP7lfv1h7teKF2maadi6SrdS6/nGC07ng9u6OYRgQ09pQ42pUvxBpJJYWiBVCEYpU5mrYXYDSAkUCJURK8fdP1BAYhh0xZ4OOohbg8kDMo0a/xyOHqyOHw8j1MXFzgBEnp1nUn2BMkXEUCAOtHXDh8WVZWJaR0hpZFPops4JCS1WhlhCSZiai2UJZCrUU5qWAFbUWk2M8nc62R5rhhYEYlUVSTJmMIMxmwJeiKfyyzHTxH4/gZG07BqOvRaGI9D0aY+qpdtdB9r1jTjwElYx0QakImlVYdK5OwjDhoNDDuBs5l5MxbxrZYKrWKjGYzrR41F+trKDQRQjWKYoWhuMm4nbjGFPsOsfBNYLRiHY11FqTSUk74zwzDARkscjc9qsGT3p0lnlGmopy3dzc8OzZM3pTDmtmiGWr2uKtwvXLsqiDYtbI2oFvO5rNuP76hZUx0w3Cp7w+80Y49vQ/GPnfN29CfOKF0A+Hej/w7ir/eY9edeNFUj84sok0Wq8WeyFCDNVfdYDXIgC2kVOOHftVgn5kHEaWyQTmrZjgGytFx7hFRXaCY2DqIYJ1263p2Lp5NJqy6zHe7rLoVIvmkUAz0fag00cMtWZZZnKKvUCkziYhKUAwFTRRvHNJA7kspCQQFYIoEqlkmkQEVX1rAeZ2IQ4jxEhMg0oPak6qQi2Gse53if1eWXevSmzr7RVS1FZyN3TLotKE3nZcjQpWa2OeZ509VytB0Kaaqo5qNsnMEBPzPBFo5kjVWZayUOrSO7NUc3foqXGrmiqHiun9rlFr1wgOsBVHqrUg1M2zdmduv9csUPYirXcI2g3wiNKLS3kDDXhBLwVn5ogZcmtgQVXeHK4QEWPKbMSVZL3Zim1rEBBiYhzHboDVYNvkF/v51itl3qCjn1dn1WVOp3vd68Ej68EodxtBfrD6iGHnRs17AOPZ73AVRGdGWRJszkPHQekUDYcs13lytZh0LSYuZdnutoD3aa7PvBH2qrKnVoD10dOLUlt4IBrDQKPN0AtQsOqtup5pbSquHcyTOv3ICfBg6X0wXDgPdI8bvEW0ERvd4Afn4lpaHLyrD/qhk7bh5wbWgx0EFaAxyCJtix3SN5lS3NSwrypvm64utOMohEBogiRFwIN9Zr+XPVUN0JbWiyugNLNSIcZGSANFgkXLDYJGvWJNFK0KiUjImUQlW3XfDWdra2HIGMJ2uOkGzFvyHmYvVtwyrDUEesQFr1S77TC2Fk3jIvZ7PgwjIQQu57NKMnZNXf0du3EkhEhpCy6NKi7DaBDDdrKHFmPViZVFG2FSzCa8pEwSp4WJcaojGAMiPdivW+ol3ahuDNUmCFnrBdKvKcbYO9zAIuzWTCQpdipkKYUgq9ATQCm1P/Pz+bxmkeJFr2pdk23du7LuRzXCg9EEq0XIGG9e2QlNVmYDVVOobA7Xm2Sev3jRqW0rHl0ZBtX/XZaywlGmikYUw9+1WI6sDs21XZo1dnWn9Low9+2t2tMJ4JWb2Un5m2jCT3UwAZxVGGU1ht5v39OdoLl7jIndTm+p47vVvl8RbUmNqV+DclptM3VtXjt4bWU/AN3gKc4Y6SRz1gOmlfG1wv0J4y0rnpiiCdrXDTmfh111EY2Emkn7JYsol7JoRBYCUiuXi9KQxjHTpDEtC9M8M+RRtXRbQJzzWxtLraRB9TbokWBY09oQ+kindfq0wgZFEioG6oZ41Zfod8SLrr1L0rHz2PeAG0UvurUHPxfIaWRuemCDd4Q5nGH314BE5lmbEy6XC7VWxnFgXia21CanzzUzQjEEbQqwUVvDIRNkdTRusMuyGjo21/1gj9dqmRG9iLc6SnpW4JNHPChZleHkofKa3QP/3A7HEUKPRMUKqrWuRcKnT59yd3drRl2ZOcUaLMAjScd116EBl8uF1lTOs/kU8e05JdjgzcrhcFRHUy3qbRtqZ1jvG0BZtHsxJRsACgbZeSEvrhBNd8jBipw2sKDvs422+Ke8PvNGWJyOBbhAbTWDo11NIGjTgoP6ng76Rtse2u5tbYO4fERPLeFB1VWLEGJXIj1K0ZZn5fTGEGhBDY0qmUVqcFxa+oFIIayHMLC22aKzyny4qKCV+O1h7RsoaFNqrYU8DshSjLazaiqHoF1UpGgNA75xmx1EvZ/NmAPS9FC0poU2aR6VFGILxEELMFWqSl+aeHcQ45OG2DOKZPfdxcL92lWnAKYJxoPO4MyiEToOI6E6CM20B7Zj62tRY1GWpRsfNx5lWVhmxQm982yZtUjXRFguk6WphWqUuFqFGNXpLBfFfLfp+DTNOHNlKYthvcoAqDRNuw3OognT5dILdl3ZDttmTY1GQCPVBhujW3t9IedszTzSawLutPVHWr+fSjeTDq+oUbL947ivtzLTOqbq46v0slZpT9DI+NGjx7z//jfNaK3F5RBDp2HSgwvp0XiMtv8tU8lG7azLbGdrhYFi1KYlVTHZOONtJGvZhnb42S8VdWo6V9GHNhjDacOQWmFDyywsQHgdCX+bS7rx00fWByaKCcj43ccOtOFgFWDLwXyF67g1WA7rN6dFQH9ga4S64oAeIgUrGujPmJpbk05mD+aV19esug7RRug0Ewbya0K00DcMg36teaSr11alWCRSqMvMNF3M6VRSzPjGC0CRio5wyxYVBItw1GEVMV4qQq0zcikMeQTT/hUR6ygzCU5WnmgMQpVCdcnNnLUoUxoxNKalkJaFyzSR0gkC5EEZDbvBZUGNIbExwKXqhIRq49E7lIFCG6WWjv3WqvxWDMP12YK1VmUmpMj5fOJymTq3NBz0OYpoPaGUSi2KgSrvNSrubFh965rRvmfcMHoWhTVfmFBRiusexfZOXNX1PEoUc1reMQjeUCGbbEZ6wVYLgaG/nxexthmV07tCWBX+nG/rm3ocRz9YvYAWLOsIwQaA5mSZnjmEmMxRmnOoAbLXMfTtxt3OonhBJHYmkBf8BOlMn57VeRZqTsGdgVNRe+OIsSRapWulJFOhUyna1iVZvRvWn5N+TX/X60j421yezuqGTz3V0sjJ2xhXbBXHZcNqaH05RunY4wOc1X+m6fuyeR1AK4pJVdaxP3rY6nro2iqWLYJRwVD5ytZIeZ1gCz5CxzuBYm+tdo6x/zf1aKd1fEtq1W5BER+SoXFFo7dbe/bghZsmOrqIqj/jtB8xSchmurKtNYZhJKZAigKtakdUjJSGub5IDRHviCul4uU20WoJ425Hba6lbHTCpDjytr65uSW9W8wV17aGxuENT9NLqcylsNvvNYUOrgHhTQqVnEaG3Kit2Dilwfi2Kj/qRhGLlFtUZ5qHZA0Tq8KcbA5xMXHdapFsNMOHpf616rTobeHJV91AEtowsxpc/wz+eX316SLB907s0bVLuDbvzItB24MxFnwQi0zXomw0aAGvSUSNym9vbxmHkWrqe4hxk0V3yW5/VM0Oc87BtFgk6MzF6Xy2FuoGBt3oHnMDu9YLAiaAxUPap7cZK5S2apP41GrPhPqeEEFatGJitfOnI5Vaz8jag/v5aa7PvBGGtbDWDEfqkQVKFCfQu3AIGn26AVZjtG5esdStRyVNQBTz1QcVaFLX7iMPjkM0q+YGYCHYtF5nVIQgSomyn23VxVkK0+XC9dXVJsJuOi9LAFFczKvifhB7gdCMTt3gzK0q1zlaFckjbn1t2Ch0aWFGUuwRZzPYwCvW6+G2yA8tKhIqUgIx2eyxmDUaDubMTFS+SSOH0LsL9X1aV+lyqEeLlgF5JSBRt+jXpoe02kHTgsw6Q27LTlF9AtUVCDHqENUZWBb2+z2n0wkQUo5IyQx5oBl2X0phmQvzNOv+CdgzKORhtMYMtDsPwZWF3WB7JBugF39yHpB57in6g2LyhoNrm6U7ZCnK8d0aFq0brGJLOadupLcQl6ASlAprxO60h2FgMphFr8N1eSv9qce1CNla43w5dzqcXmLc/EyzaR3K1W3imYKogDsLDAYrtWbCTo4LW4aKsMWSgxnaZA5m/ZrRPoXe5Srm7KrJzK5YtzuatY7TNS96L4EZ3/jaCH9byzuJtt0zakweetBXo1+PSmOHLyyFVJur0UtaK9iIdUIZTqaFBr29ZbGpGJt0sKfrdSWBx6jCK625kIpGAzEEduPQKT0eKalRsw1a9XdssWtPuVZWSDAHUAz+cMEcHdOz2+0MclFVNxEoi3Y6AYaI2MYVx7mdwO+przWdGNSiCmVamAtilfyYCCnr9Qdt0vA0EzMO/jxc3McpWaVmHQNFoAg6OYLVEINj8HadMRGjqeJFF5/X9DmlVbCmipj2QezsGO/kwlpYW9IGADCFMlZsUwKmXRtWTV3bW9oAsVIG/drUMNApZ/7MdC9Y9yDeJdb693z5cx2Goe/R1rUlwmaPVdvjawOFXoXdr1o7I0SasmTGYbTzE3pWp/ioijO540lRB8hqwFIpFfb7g4n5mBEV00wOwjxNel7CymQIQeEgbTZpPfJWbDjSXD9HVmnJ7bml54/6Xxfw6XUOg2h84rZDDytck7T4HOH66or7+3utXfAQjnFWzae9PvNGeBs2Ke/Q5lTBwxTP5l9JoBce9Gfsbexhto4xe0RgB43VM/vPbze8j1AZhqFPZX4AQzhE0poJ0KxtnyGFzYEWM3D0qMA75Zwf4Cn1lhWx1RgQgyD6oMdu5AqQ0HH2safKTZpCEIZbx2ZGz4p8foA7fcrana3EocYl6xw8QlLt5miC5GZ41mGqwdS9VsMshj+WUpmmwn6XKXYwFQfWWWuloaN4BJsfl+091ghYZHUY22eleHFjCMGykNbBZhUrMmtvWKqrzeWoDAtCQ1pARK95HEemadrADOZQmuKiEes8w4pDUcGYrRNPyTK16GJEPvvwk23uazt+sKBjxXRfjai9jhCse680jYJtI6PFRBcSWnHsAA/0psVaiUMEGkTUgQ05I61acbXpoFSH9sIa8DSNaUghd/YHaBalei2rKFYpS88qrSSxZgKsXa4Erd/kmLraG6xnCR7SVe2FWh9qhcvlQoqJEhZC77hbYYvvxPrMG2E1Vs2iDCNlE7sh3BY8fJO79+/UFds8Hko7pui0Juf2LsuialqyHhLQJgCxXNkNomsSOA/WNzb42VcTpqNlrBgXQk97QUeCu3EpPgzSGj48LvTPvqVf6Wsr2XAuwSZQg+GUpV/71pmECKJDy6CzSFT5a8gq4nN7e+Lxkzfs96zzvZRbHRl2iQ5Co9evBUGDU6yrcJ24i4nwzIzjzj5TUC5tVpZJis7nXg1o2fCLfQJE5+xWHYUzXSaa1P59xOhStVIW1caQhqqF6RUyLzPzNIFUpCrkMc+V2gq1qqFw6lktK/d021ghrfVp0NEaAlrF8O+6ga1WIzrk3I2A820Bdrsd0zQphzc5Rzx9Imr2/dj3mQghrdOnH9Y2mnGADRf2qNXPySaVr9Z52azI2aQxTxMxWfFStGDZ6x0muh8MPhNWtkZAs4KGGBMjrZrabY2AXV9jGxH7ewIGNa7NGyFYRmuf2+UIepRrNSGCKvDF4BobaYN5R/J3yFz+PjDC8cHGWQH/0FP13uK7MbTNDECw4ljYGGB9Xx4azh4hSCfarymmU67kE39045jwucEV1Zo+PG1a6WVYJB5sZDiE0GNzEChd42JTULTCVkqv0NCKDrmstTIOQswDEteDtr5eLEPQ1E2aELJKWTqRf8iJ3W6nWO6ykIcdIsWaQnSbaTeaGDZfwI2vYXgNMUaKPwYtZnnW0OfeYQyQoN1zEiJ+tTFgkXQjxKVH0dv7WKvO/yvVlc100OuyLIYJLt2AeteUiJgozqwGsK2KXyCMw8jUqmGr3srrmLvBELafoqfEKu7Yo3IJqyFxRTU3FJ0e2ffdmt30VuVvwdzxz+50vHXDhjX7Q3ogEIIVwtrD9wshsNuNzMuyEdoJ/XerE0jmdDWldD6wgwUYphtsKG1viuoQlwUNUYtkRXiwF/3ze+a0LZx71uRwmp9nhRRDD6yUF2z3ULxHwOJqWaUItEbkg4C9Y/Ef1ur87tZn3ghLa+Rh6E0brUo3JJ72AWj12jBkEzdwGo/CeGsFW1ijAk9vPJVS7vFqnPtGEY9GVt0KjXI8YtWhjzqJweT37D1ScqpNNQw2gKw8UAkP8T7L8vs1vRqNVevD98YQEYVAQoAUVL/CP1sIAQmr4/HPqHP0BhV93+86fDKOI1jhI0ZvSFkIQSOXeZohDcqPSKFnHj48s49E6tHRGrE09MCmqNQ0vya/21OVrgy3XT2Ss+ehRnIVxHHscrcbOd3f9XsC+jwWm/Yg0kzhrFHqosabNa33VmUXuaFHvJrGt9ZoRXS0kTSVSmTTabl5lsVkKbOxH4pu5l548khZMzp14m7kfUy8CxF1CMLuhWY6BkOZMfIMyoMNN5jqzoVo0XWYN1CZd1ITutP0QCX0LFKfgc9H2T6HLe1LjSUQonKvl8UkAjZ7OFiwk+J6/oKzjKRfg3K4PYltVjA1+CjlNREzFpDo8SdEfxZ+rfoZtWayNvN82uuzb4R9U4Dhj75h1lJOa5WQPJU1oRSvoNr71Fp18oR5bGPBdExMKt3o+YboeFStiqFWlUJ05SznpG51VjGopLM1sM1gOLCAUbqU3yvNcLa4ShnG6PDligMqprW2vTon1g12QFuro2ghxHmryuAw6cEQkbgeVL0/ShtLFlkrjcxUjlA+cscbQzYR/WTOTIdhTtOkGGg2ofMo3bH1jreePmqTXYpr15wb4iEG5qj8VDaf3w26v5ceXmOM1EIpi6XMlWWeCPZsVMpS56bpIEmVmNSaghuuhZyGzoZQMXBWdomsHXcOLZTaeoOLU8N89JTetmY6ukYls33Y3Fr4ngirAfcINAbnq69Yrks8elDRlz33dbKEP3eFfHr0bSI8ZZoNcjD2BdYo5NEvDaIKorvQlHejevF6mz0+hMhU39mNd6UhxuvvU7Rtn3uI20SniDiIsnXeq2gRhJAsgIJmtRyxyHd/OFANnlq1Mey5efZCMKbEayP87S3x5ge6ITYYqKe5WjRxulAkyErjke1G6Vgt3Ss364qKWbmuYdMW5JvCK7qKnQ74tAs/PFsuqHM3q3FutSMsuuQKnY2BGQpR+CP0Hnsx5bW1AOEsDWmqbuYHoxTlvnoE4aklrE0pmrJGxQ/7tANR+UtLXV3cxGGEoMCoTqiuqlAWjTKVzEM0WW1zq5USFvTQRAgVog3UJHQc1yEbC2bXR2z/VShiU0yxCLdjsQ+gIP2JLQyhjmblO68tz4olL7Pq54ro2JtqcI7ik5XWFnx2GVj0hBAl9igR8WkQ7qhrj75SzP3+Ro9IN9FljCsWbJtyrT2IWGvv6qyScWNFMCPsFf6HkJpHx9v7pPdtk4mwZhP+MyCd+kU3zPoezWYL+u9xzF6hrYaPDRN7bitcUfp+dB5wp9XhOHJQPnsIPQoHPhHQODas9R5Zz71dYyBSauGwP7As85qh2rlvTYMmdx6vjjP7tNZn3gj71FzEPe/a6uj8x5DWAkQSF9deKWz+s1XWh0xw+pRnb2uxLoTw0LB5tBq0hVUPzqpbsC10aJqsguOdiL9UxT3bqywEOh9W25F1VegatDHGzkMGS809ahYt/OUhW/HDroWGSOzFHe0WrFY40whDTDAnAIirdK0HtdpUghg1Mg4evbRCiJbaept1xzZtlE1KhKYRmrcfXy4Tw3BmHBLzPFLG8CAa9ufQihYpXVZympbeAeetyrONHGqtURcdl9OCzxoLLJYqO/bqEZFHzXYjTYBnUSaAzyJromL+rHWEbuzwUUQe2Vrjj9/EZh1issomKo6uv9KNd6/0b/7eMdINpNCMXRBs0u3250JYuz2bFITGUrSnMTnWoyfIFMn0JujzdZyevm9xOAItjLoWhWeZIXqTBUSbMrN604BnTavaYOgB1KvwUmuNlJM9hkiy7FIJNhY4OYQWgqatYcWJQelrIEzT2WAQWYuA4lM6jH7pZ7m+NsLf1nJv2fEjYaWhBSNzezoX1DP2Xc/DooFjYVvKkb93a95913qkCfRUs+NoYfXm0pzu5rixEIbcI+P+pwkSpafnW1zNU0hvNZYmhEQXDtrS4KJNGZGmAyqj4W/FK8LmoLwIUZZqnXixj2rydDZai3Ozzi6wzydYRNaIOaybv1YbOx8JFAqVHAKm1qhR7xhoBPK4I9jEBZWeND2GRee7zXPjUiJ5WIXdBSgC01KZp9Kdm1iGsEaclct01gjOnEQKOtuuER88p5CSzqWrldYKy6IsCH1W2u4KIFbIamXpEEJrygbJVhj2cK1DRq4THSKyzCYHuu458Oq+vtYLRj0w8OgweLaFCiwZFFGNQF3KKkCvugyusrYyFLylPfpeaiuE4FxbhSh0f0tncGhi77a0ylpXwc5XKcUSGz2HCBSDGQiYhrXLabqTqdaItMJJLhmwbUgJITLkUbPXaLobolOysXPTwspXbrIW3loz0XZpnC+roQ9smpo8xxK9755hfdrrM2+ENZVtXhbomwrUYIkdFljTVeiwEE5B08hR8afaNrJ3oi243mLpmsXBII61pXTb02+bLbYHG00LZODqX2snHYQWfMv3TdnTah/fbdhp63DFQ6rcihk7z9NOj2jnWoyRKDYKXdZCTmiqP6DXo/fQhbEHH7Rp0RChQqFjkKVof75QNONoldImarA22AwhZpDCsgRCbaQ8ktJIoTDNE7u6oxY3ho3SwIeD+gqoEdHZZ1vCvhZDna6mrJNIRSN7N/Jl0UnLrUo/nIjrK+hvWsps71Gpy6JQDo15njRr6DoRwRyc83u9mYFuYDWLCBv9haTRo7VLx5iJUYeAguiEEDStdqdXxYy/ZyG2Z7WgZwaZ1iUfu8zkpllCUEoiAYaUjOtsAYHhRkK1iddmpGkKUWEZg8tmiqhGVlypmGq41ielgWlD5X7N7fdMUjoVrbTSo/ct1uuFVT23ocOMMSabHeiceefSr3t+LV6v4K/XCtJGrVAF3C1gMruRY37Qdv5prs+8EQYeHIDaViNr2HyPfL1vXL9l2KMArHq0buC2hryKdqApfrlq1XZjK5/kQ4psha5xsIpaGmIbdMX/Qk/ddKhhMAfxcAySv4+rd8FKz+qePqxsDk+5H+CrTXpTydYpdZihFOJgkVmrNIdRNtKT0rT5wYdRhtBUQ8IusNbqYZdF0utzCmiXXh0LQlYhnqXS9kBwrqulv7zyR+jpuEf2fg+UGeMSitpgsiyLQgeOFzY6v1fQxorW759fvka8GuXN+hkcGzXj1axq20S7WtQASt9zES0AOzd50/xtU4qxjMwMZxBq8cxIpyrrs6rrfmvQgjoYhZFcF8QcSVXtFDdCEe10q7VqBxwrtFBb3fCOQ9+CDp/sxlEbZLDuRPu82SCCZvdkyMlw+WAiR2uROKVE9f3t/jso80GKGAS2dpjC2hm4lMUwdGzmXVQn7QmHOEaub+/yrp0LbXs5xWjZmVV6egbsQYXjzXF7wj719Zk3ws7PBad8eUpnhjmucAPQYYa+hE3lWF8L3tZshlL03VdhEXupeJEmdFzRv759f408wPm+Un1jGx5rB2GltL3SAefRBeGT7w/957aEfDcIwXA3n8obQiSHtZrfCzDi1+D3qr/LenC98gGd4bBGopUBVVXTCEeMeTCY4VKVLLUHzWQyB7vmZlGIOSKPTwId31cni9LeNpGQMgO0qFjKrO3H1lhR60KZl16UqrXQpCruG9SZaX2gMg6ZIIVWJjBhGG/X1eaGBWolJyvQKuCkn9caUlqtPU1vtSgG7LzUEKBVhriBq6w+EGMkSO0FrIg1GRHJIeNtwMH2SM4rFTJGoZmRTUlHYrnBAzjsdqRsHZatESUhTaGmEGyySoq8+dZTqlRevrxVmmdcDVOKCZeG3DI2mnXNOb67LAoHaSejDupcihdcxfaOskSi4eJiiUMw6CznQR1UWA2uco/pv19Mkas7EFnPuX8up6GFsErQugPcroYHakU7C78D6zNvhGlrQU7EgC8fPNQj4NZHwzhu5d08MYR1SgYbA4YWakL/16YoZbhxjxT8dzyg56g+q0+00EA30MVtMDpRUEpQjLFHZb7B3ED5lW+LieuytlnHo+2rjvuGIIRaEYuIVfZQr9WNs/6uarCMIElnsUmPqtcGAqf+qG5uNVgmkLJAFB3HhAqapzEjrVBEiClDK11usNWJWiPSBlotlGVGWlH8tVZaSZTckSR7Pmu2IdaE0epCqzOtzogs1DohbQaZiaEAE2WZ8MwjUgkYj1aEQIG2gAjDoPPw1Ph6o482rNRFDbrUQi2qmuaHXR+PjQdyBynO4lAp0BCEFIXdbuCwV/W482lmqd3tG6dXDVKvdTiU3J+svlfHL4NR1nykldDxVuXQQqkLw2DDCHADJco4yzrdJWdhiInD2083bAmsUKvRpou2E+ht5xZdQNAGo5gSOWVevrxjmmeePXvBBx98rA5K6NGrN+4EB5ylUf36U7JRWo0qOrfO72uMWtfZNrm0ppChw1EubrQNEmB1BFvq3EP66D8icMRf/It/kZ/92Z998LUf/MEf5Nd+7dcAuFwu/Nv/9r/Nf/ff/XdM08RP/MRP8F/9V/8V77zzTv/5r3zlK/zUT/0Uf/2v/3Wur6/5yZ/8SX7u536uU61+N6vJWkGuzWdYbZNYXW7cWqs0wwT1a6ZnKqsR7pbYD5lFQ7J5gI4lb6PSbVOH0pxeEQQJad0EwSENjTq2mNir3ULbzfHw6/Y11lTfU7a1GKhRVMIOdxM98Pb7dAJHojUdFy6wEULRe9JoGwxUb44gajT7pOtGbaocV2ujEmkzJINXiBlm1Y2YphNpGMl5YDrvuZz2HA577l++z0cfXLE/7BjGwC4rhhCaNi1MS+F0WbhcFu7uzpzuT5zPJ6bzhWm6MM3KiijLTBQBUYUzr3orPANDMrZDE4Ykiq9KIcfAOGqzhhhU4Hsij5kkjRpMx9b30EZ7wCdSNGOZaGeZ75cKDaYWWKbaDcJgVXyAhGpVRLTZRrmvFWIm5twpioBJT9LvvWCwmEffUamMKSdy3jHmyJDg+niAEHjx4p6lBVrMxDzo83PNE+fgQkfKYtLCVezQhkCoZn+tMy7qOKGnT9/k+tFjTqd7dvsDt7cnlsUKnk16cdt3daf8Vf3vKpxlmeem0FhNHbDReLUesp7NV86IWGbQi51rEOPSBSuz4tNf35FI+A//4T/MX/trf239JRvj+W/9W/8Wf+Wv/BX+8l/+yzx+/Jif/umf5l/5V/4V/tf/9X8FFL/703/6T/Puu+/yv/1v/xvf+MY3+Nf+tX+NYRj4T/6T/+R3fS2lNnKypDV6lKYUFuceBkt7WgsbURtAgrIIcCqPRdQWJdO/RrfpW8aDU3mArk4l9ruktU/Qb0JYDVjr0bAdnM2GKkZfgxV6WA0rD97TI2lDTBCMP6oxzxpVx6jRmlXtXRrRNyebwo9NO1K5QSvgqSSo3gSno4UQOV8uWnQK0d4jICQaqqKmyjVJI2ir4OdhJOWBmDKH/Y7jfs8b3/suh5C4f/Yx9y8zw27P4XDkeDyy2+3Z7TJ7ItdNtQvO1/eczicu5wun84l5ujBNE9OsDIvv+wPfx6NHV+zGgZQCw5B1/Lo0fvEX/h/8/b/7m0hdzCk7PU+V1npxr9h9s081Dnsk74ghWFToTeaxK5nGpEZVhXCSaXaoQ0hJuwFD0BR/N2R2KTFEOIyRm8PAG1cDTx7vaaLdh7enC7dL5VIr01yYZ6WazaWxLILCzsJSC3NZOue6lIV5WmiXmRQiQRq7JITHC8f9kdOLFzy7PXFZoDSYN9NIIHS8OA0Du/2e/X7PfrdTBzkM5CExDlk1PKxDUwdoTrz//gfkYWC6zEyXubMzUnSuvW5dN+i9oGr3+lU1M83WthBM6K3Pnh35ARVzjn0Kjax2YI3JNpCjP98YrcPx01/fESOcc+bdd9/9xNdfvHjBf/1f/9f8pb/0l/iTf/JPAvDf/Df/DT/0Qz/E3/gbf4Mf+7Ef43/+n/9nfuVXfoW/9tf+Gu+88w4/8iM/wn/0H/1H/Lv/7r/LX/yLf3FV9v+HXmIZkrWNRusyQAtGrpHr2G2nwODQxArwO7ar0fDm64JipuK1VN880lMdxJkM9FToIWvBu3u28MaaJvUoOqwV3QcMiW5Q6QYwJjVWw7gnpsGi1cow6uDK6E7CMFuP8qfpzN3pfhW/lo3gSQwMMXX4xilenv7lPlUhmxpW0gnNScVZYlZN4UZGQiYkVVQLaSDmTBoGhnHHbtiz2x+4urrm0c0Vn3/nLR49egx5ZDweubq54uq4Z9wlQhCWpTBdZu5uz7x8UagTnOtMnU+Uyz1lOlPmCUpllxKnj77G6UPXjPV7LMzLwuV05vHNUWECgztiL9JgsE1Gm1Bcqc27zqzNN4CIwierM95kXs35x2vBjR6tavSlwJkwSWNujfNFeFkm7kR4640DYX/k6994yUenwrToRBHtwrSGEEwBLibVtQhZ5THjwDgcdNafoLztmGgEPp4b790vnOc9dcgMQySUApa1rcI+uhdTUppYiqMxOgZyGpHauNRF5+4ZpS3FxG5/4HK5Yy6V8/2ZFy/ubHJJXHF/vOj8UP0teEDRAw81llW0AKnz1BWmcfqln68th93S2E9CEdtz2ovi4iDbP1qR8G/8xm/whS98gf1+z5e+9CV+7ud+ji9+8Yv8zb/5N1mWhR//8R/vP/uH/tAf4otf/CK/8Au/wI/92I/xC7/wC/zT//Q//QCe+Imf+Al+6qd+il/+5V/mn/1n/9lv+TunaWKapv7vly9fAqwHw+hicS18Kok9rN1pfljcAHs12GlnazVLH74XAFZviv57k/r4WtP/h5CB04S8+LbFo5yS5j+3RgMP37OJEFPgjTfeIA+ZYRwVz02Z49UTxt0BYiJZynp/uuejjz6iLTNjjpR54nw+W6uuFZUM9sgpMYwD4zByfX1FHpQ6NeaBHBUHjCn1BpKclGqVYiQlFULX/2ZNoZOqYzUSpB0xD8Q8EmImJHUcKWZSzozDyDAOHA87drtArSdauzDNt3z8wczd/S0vnj9nOl84nydKEaqDqDGaiE7EZwkedgOMOqppWYxVgJDHHbtxTyBxEwNlvnB/+wKpxTD60p+pFsEg5wPf+8V/gvFww27ckwfTAQ6BZSmcz2fe+9pv8Vu/8bepVefNrZCSZkLbJhWxZ15L6c7QKVGOy2MsFKdD+l4VS9Vbf0/b03a9qjNcaZYKCdIFdxCjWcZEHgbGYcdut2PcXzHPEyIwphGi7o3eEo5s9qk2mJzPCgWBYuXajblGo0tolCJM08L5MnF7e8/lMvVItXkw8AqkFr0Pf7PvvU3b709rTdX07PN0bLnfYwwuWTVX6pb5Yhmk3hPVnxCvifCKkNanvD51I/yjP/qj/Lf/7X/LD/7gD/KNb3yDn/3Zn+Wf/+f/eX7pl36J9957j3EcefLkyYPXvPPOO7z33nsAvPfeew8MsH/fv/cPWj/3cz/3CSwaVOZQYz7VOXCGQE/7S+tYsVZxLSKFHk1sJQS9UUMl+WI3ou6Vuydl3TdbnKn74k2RzIscsMGjxN5BHhr6raKU/YUQA8fra64e3SjWGAO7PBDjSEo7bYRohRQiu8NIGAJkmC4nEgLlwNV8JGDFE7QCH2NkHAYe3Vzz5PFjro5XyjSwlNkn+3oEX5tXl009DCfoGx6KQi5aMNOZZ1WsgyoKUrai6Zn9fsc4qlTjx89eMM8T07wwL2vhZZ1bNyjiNOglVGlUEeqkXXitaReiC5irkaoKFuQdjx49Zhz3xAAvnn3Iy+cfUuYLrbnSXOnPOIZAHvY8evoWV6EiMlDvFzW+pzMffPMDPvjmB9zdfsTd7TOKd9OxadgRNhxk6dQ6h3K2+2LIA3kcGYaB4zCSUuR0vgdRjNX32cqFljUrswAkpERswfaq8mC9DT4ETIJyZp4WTqdTT9enaTJZUOw61dDnFNntdhyPex4/ecw7n3uXaZ6JMVA3FMm7uzvuT/eqvibNDO/MNM3MS+m/vw8yVfELYOWz+6RqD4KUgkjP0JrpnKzRb+jOxs/KtlDnKmtObxcxcfpg0KS9JjgEZ/DjBkX+VNenboT/xX/xX+x//yN/5I/woz/6o3z/938//8P/8D9wOBw+7V/X18/8zM/w5S9/uf/75cuXfN/3fR+lNFIyOhnbQyArXrR9YGDedCWGq4du2jCxMeIrTSx2RoNXY/Xrm2h3A3P0iAi6MVNbG3v06/Q3PZw+Y23FoztWa2l0TIk8jlZEE3LaEcPIfnek1pmyLFwutwjC1aMjx13kdJq4O93x+PoR77z1jjadtEZdChidSoVbFj764H2eW2obYlgbYDZQRbPrMmi7wxV6X4NN7tD5ZVNpLGGgxRGJAwSN1ENQreBxGFacNkb9XsqYe6CRyWkg55FhGPX7VjBqrfb5cfM8scwLyzLR2oVW1JHNi/Jt33zrXZ6+9Tke3Vwz7kbKonPlvv7Vr3M+3VHrRKkL4B2JyjcORN772l/BpRGV+ua4o7bghuAsHNhyt9f9t8IOCOZAXVqVXtgVhGWemKcLp7s7corkpEXJcy2qQ1zFGAP6nrXUvm8XK6hpF6EbrEAeBp5eXfHm9RXv3d3pnL+Y7P309c5/PxyOvPvuuxyPR/a7kf1u5Hg4EIM2sZzPE6fzPefLhXmZe+ep87FFGvM8s8yVZVZOb0RpeHUjMh8wgfp+zqQziFR/xBuv1kIaEjTKj4XOGPWb2pfYua4eK2EHv2tt1Fps8K9+r2tdeE3lO7S+4xS1J0+e8E/+k/8kv/mbv8m/8C/8C8zzzPPnzx9Ew9/85jc7hvzuu+/yi7/4iw/e45vf/Gb/3j9o7XaaRn2r5cC/8MnKqmOx+nOuwOSz27r7XZeGF/rXDik4h9c2SwhdyeoTL93AEP5aMITDMUFxDdSHoiEekfv7BH+fEDjd3XO336+R8K6QwsI8TQxDZplVs2EYR053E8Mh8PTJDad94OXLW67KFWWqfPTND5ClkM0d5KS6AUPOHA97dvudieN78c01N8Qih9ghl2hsAFChn1EEa6nSootlINU+fx4GPWjLwrAbOR6Omj4KECLDbsfV8ZrD9TVXV9fcPHrE8eqK3W4k54RIYCmV6TJzPl+4P584n89czheWy4X7u1umy4W5FJalsN8dePLGUw7HHZHKPN3r3Lh54ni8YsiBUmd8WGjOmWpz76KlvTpvT5+90qlEo0yDLdQhuJaE3ov9TjH5UhbO5zPShMv5zOVyUqcTbAJwzDx9+pRxHHnx/AXPnn1srI0G0jpvtVX9e0qJR9c3jOPAs4+fISKM48iw25GGZINKs34tZ/bHIz/w7rs8GkfKV77CZZpYqgUntg9TctW/wLPnz3j2/BnJnW6r3SnPy6x4dOfsNvKQIeqUEb0Hyg3Wgpfxii2IadQOz2y75GorvV6jtD9zhDGCuNaKRap1dXCtSJdg3fLdaStf2M/SVqlP9StihxU71BgCKxD46a7vuBG+u7vj7/7dv8uf+3N/jj/6R/8owzDw8z//8/yZP/NnAPj1X/91vvKVr/ClL30JgC996Uv8x//xf8z777/P5z73OQD+l//lf+HRo0f88A//8O/697siUkp5TTM26X0vinwLHEptqphRTF407SD/AxaCbQh9rdGH6orlfitP2rGt7nE3xToJ0Du8+gvWk2z/9s9yOZ354IMP2O/35Jw4n87sxwPjbg9yIMURCaEXVupckdTIOfLo8TUtNN56521CC3z4jfeQqvdjPOw4Hg+dR6oTl6NJgjpNKdgkgtUpeCjs99wPgtPblP2h7xlCIObM/nhgt9v3nERQHYndbs+w2/Pk8SPefPNNxsORq+trHj9+xPGYUYnfxjQVbu8m7oJSBz0yDIJSuwIcj0cIkWWZmKaZ0+mW0/1LRGy4ZFMVie/74vcoxU4qg0XlGH6LiMFcml2laBQsRJtArBlkWQrn6cLLly+7LGqMkSePH3M6nbh9+VKj/Bh59vGHlFK5Ol6Rc+L25R27ceTtz32ODz/4EJHGYbcn7HZdx6HrRzjjIib2ux0pRq6vrvt9d0aGP6fcs5nKb37t66ohLYFh3DFEZcgMedB9HJNlEot1qmmU30Qj7EudWMrCUgqD1QtigP1+x+6w6/PbSilcLhPzPOPKZGDMB1kDFjEH3sSlLDXmSRaQeLaxil6t3GmPdr10k2zc0fZcbTPVB+cQy9yS0gubrA6hd5/+o4IJ/zv/zr/Dv/Qv/Ut8//d/P1//+tf5C3/hL5BS4s/+2T/L48eP+Tf+jX+DL3/5yzx9+pRHjx7x5//8n+dLX/oSP/ZjPwbAn/pTf4of/uEf5s/9uT/Hf/qf/qe89957/Pv//r/Pv/lv/pv/wEj3/9sSS1u8oyu0aB4yWNq4Ee+w9F4pbNGKFhs8VxwmWHGjZlMnNI1yjKo+gB/8tbAW3h4A/R1Ofpg+sXltWP+xYtR60biTv9xfWKaFnLXIMueJYdxzcx14+3NPGcY9z198zOn+wrhPxGy4JJUYKku58M4X3mE6nannicc314zDQCmLGawTTRrHqwP73V6LbcE7m8Q6vNwRRWsysRZimy6SLH0XY2/k/Z7d8YZhv6eKMBeduBGtk26uAqWxPyaujgdyjCznM7dL4eMP3ud8f8f9/S2Xy8z5dGGuws2jN3jyxhvsDkeurq7Y7/dcTieGQWeXPX36lFoL3/zm+ysua0UsnRSi3XRYW3AIwcTpC4txjZXqpVoTrRaLkGeFP0zicjK1tsvlvHZuCnz88Uca5RU1+nMtvPH0qdtJAB49fkwIia9//WsADLuB3W4gGmciRpVJb6yFu2DQTwr6U2r4Lux2O51cvWhEXqLi9+M4kk2cqTRRlkrOxKzn47AfORyP1HpHnS6GY7duxNRM2hDTIRKz4q6H/Y7D9V6ZTCKczhfO5zPzNPexTn1De/3Fzp6q/in0s9I/FdYIJsvZtbZdja2XRzbiSVgnXa/b8Alj7EHNyjJSnNiZP6+exn9kZsx99atf5c/+2T/LRx99xNtvv80/98/9c/yNv/E3ePvttwH4z/6z/4wYI3/mz/yZB80avlJK/E//0//ET/3UT/GlL32Jq6srfvInf5L/8D/8D7+t61F9UNf1b6pGxprSO4amX5Re0MAKO6+yEjpeZc9Qti1brCwJEY+iHxpT+oZYo1g3wB5FbqfBihcaNq/xa9WNS4dORGCZFlqxETI0Yta5aM+efcD+eKUbTQp3t2eOVzuGcWS6vOQkL0Ea+Sbx9rvvsNxfCK0qDnk68fLlS5aykIbMabpw2B94cvOIw24EAsV0C7YUO7/mKoanJx30mXYHxnHPcPWIOB5ZSuM0zfo+tTHPJxCd7XZ1dQU5c7q/5bd/65ZlLpxPJ6Zp7vft5uaaN958k7ff/hxXjx8T88g47plL4XQySOJ04nI+sSwLX/3qV1mW2USIbPRR1YGXtRTu726Zpwt1mdTwLrPpfKjEYTM4oC5FCzmt9ZbnbbMO+FQLWSMruyeHw4Hj4QginM8uPKORdUTT4RDc6BnsYBGhZvum/ZwiVUQpc447R5Da+oSWy+XC+TIpq2PIzItStoacFb+PkYRCGkMYlZWS9bnGoNFwq8Iyz9RW7D5UHcMlLleqBa/dbsfVzRXjbiTGwDRNnE8X7m5PbPVXvGlqZYs4dm4ZoTGVVj2X0FkQ1ebU6Qy79eyGoNCHz7QTkT7RujNlNpCjZ3IPJpJHi3jtex3ecIjpO7CCPAy/PjPr5cuXPH78mD/94z/OMA64gexMhs0mdwzf2h+o0hDWUfcPeYnYTxntjY1R3Ywq8g1WTE6RvuH0IG41LTxF6hKKm8i7tfbQKNvvD5by+ybeUttAI9GbJ0/4wvd+L8+fvyDEyGG/3+BzOs5H5ELhwlJnrq6vePLobRJ7ynnh9PIl0+meF8+fc7qcNVWLPscrctwduDZMNqXEMAyKuW+oexpFZvIwsD8c2B+vOV4/Ie6PXGrj7v7M5XTuFLkhZ3a7HYfDkZ2l1sVSz2hTFsZxz/XNY956+x2+8H2f5+3PvUmteq+vbx5xf5p58fKOl7cvub29U6N9uXA+3XO5nJkuM/N04nQ6M13O+BSRpSrWW2qhLbMyI4qO2ak2bmeaZ0SUnREJSHF5y5UpknNmnmdE1rl+fpjdELTq07ptN4VIyjYZmrWtvLa1VERE8VhjnDRz2hoUKAtoGLPqHs+LNWWY6lnQkLMWnUydU+Lq6koFfmqhlcZSBdLA1c1jxv2eZSk6FWTR9u/bu5csy2xTQ6TvBU37hTxEbh5dczgcSTGwlMLt7S3f/Pr7nE8XpbIbdc3VDGtdIR6t3ahD02BhNY52m6yRypkOm/l7dl5ijPRBtUEbZvRnvDljlecMHvD4+bVfUzfncFsRarXyS7/6t3nx4gWPHj363Rulf8D6zGtHxKQjjXwiQ+/BD/TOJk0/Vpggtaij1FsjtNUb+ve3LIYYwhoxW1jqpHPxNF13z4P38KUY19o2+6ByaxSlxhqR6wqb7z98zy3s8fLlSz6PMOxG5unCeT6jo9kbt3d37HYH9vtEyLrB9IAmzqeJQKQILK3x8v5O7yWQSIg0amlcwmSjjFThKsRIEv00IWaIiXEYuLl5xLjbs7+65njzhMtU+PDZC+7u7rh98ZJm05pvrq45HvcM40gpF+7uXrC0xrg7cHXzhMPxisc3j3jn3Xd5+tbbPHnrTR6/cWQcNYW9vZ3BCnkNlX3U4o42MfgYo2VxSKH04k8iaXQp+llaQDviMLW7FAlRxy8ti04Lce0FpTz5M1GTqcZgdcbeNbodoLp9XhoQwJBj52f7DDr/fgqxO2SCtpr3bwZlkOz3e+5vbwnAuNsxDNqAEs34FZsEHYC6LByPe2IOxLFydy5cauMyTaRhx93did1+pJbCdL5nmi59D477saf6gUAeIoeDcsljtGEFpXF/d6bMxZgQypJ5AL2JDkkdx5H7u/uu0Wx1OPwoaECEqfyZ0Lx8Upiqc7lRutm2xdoj6g5BiphAz5odb4OgrQH2JpzvxPrMG2EtBhkhO1gxpbXNZNhAsihBmgpCt6DSfy6jXTejWmCj7QBs0+6eivrPeNVWsJRcHryHiEey2yaRV42r/g7dGK5V/PB1Wy2JbbRe5pm///f+Hm+89Sb7w9464KpGpfs90zQjktjttW241sZhf00KwvOPX7I73vD82XNKVUeRN84ohMDISEO64DuoDmxImZASVzdPuL6+JueBm8dPkBD5xje/ye3LO035z2f2OfHk6oqbG42glrJw+/xDlgZx2HG4esT+eM3+eM3V9TVf+J7v4Z/6wz+AxESNg2oURxh3kUd5z+lk2UnT51mK0rLmZeF8nri9veX+7l6jqlZoDWpRhoGnoNI0cgqh8gADrT62XdN9b33HnmuKqdPCYFuVZ3OP6MW4JtIpUNE0OnLOXB0PjMPA5XRimmffYRu6X1yZNoK26Kas1LLFRKVCpBYV8h9HnZIcbe/nFDgeDrRlgTLzxqMDj/Y7vvLBLaUEhsMV47hnWZ5BgLvbF0bB0yh6yBkV9degRqcfBw6HK0LICr9UvfeXu7M2O3kwAl2/YlVIUxjjdH/fz9I26/NXPij6smZ9wiZj5GGRbrvcwPbs167Fswk3zFscedVx38iZfsrrM2+EdYmdE8N9PM0wD1vMYALKSKgaPVU7X0492z6csBGB3hpAPXQPo15v/fUqsO+lbnx7VMDG/a6eV3/2VSPrGNZDrdSt6pO0xu2LF5RSePd7vsDxeGSaJ5u2ELk6Xlv7cya0hVoaV1ePeevpDS+e/SqPn7zB7bPnDOMLynzp190virUjMSeVP4wpkcY9T958h93+CMDNkydcpomvfe1rnO7uWc4nMvDm9RXXV0cOVwdSjFwu9zSJHA7X7OJAC9pZtxSBqZB3wke3F/6fv/o73Dx6zKM3HnMtgcMh4zXbNdqRzpCY54VavbIeCSETYuT66ilXh0cauUlhv98x5MjHH33Axx9/g9IuKu+Jwk+BVQdlWWZqqTrZwwXvjYdbunrcQwxxWZZ+r3zmnu8XxSgjQlPI5HxSCh+YiNLA1dVjUh4IQVvLL5dzZxeIpfN3t3d9D7VaWaqOLooI54vS63IKzOfG9S4rbNAqszTS1TVv7h+TdweWpXA8Hnn+/Bm1OgRgWivWqu0MBYIq3+nEbJto0oTbFy+Z55nVgPmeWfdOpTEvEx99PFFm7bZbefTbojk9aBJ3XnFtcPFGKr2fFgxJI8raJecwWmeVBG/eiH7IHp5x6Hs+PKh3fLrrM2+EqwhptXrdk4rQsaNo4arSA73mjHFBpctZugi7T9roM+A2WJ9/P4SNTqp+kW1CIw+4xutG2xpxvdyw2ZRoIcgMek9NWTnO/p5riiXc393y7OOP2R+V5zvPMyElUspc3Twm5cTdfaPWiVoqu0d7UkrMy8z3ffH7meeZZx++D6GplkAIzFb0CQg5B8ZdIA9CHDM3b7zF7nCNNOFwPPL8xXO+9tWvcbk/IcvC8bDj5urI8fqK3X7U1Fgg5ZFaGnenCwsTedyThr06xlLYN+E0F4alkpfCOC9wr1N0a8uURZimwjwp7OATk71JIYTIMIyMu8qTx4/5P/+f/gQ/8I9/H0OKDINp58bA3/utr/N//Uv/Nz786KQGI6WO+ba2bUAoNDFx8NasgKdZloSHz8QPf4+Om/GLo5g4kDnPanP2RDOPcRh5+sYT3v2ef5w/8X/8k+yPV3z1d/4+v/4rv8yL589Y6gIhUOrCvCxMlzPLrEXHJoFWdUR9bdXkIoWlVgZJXD8emSt844NnvJf37G7e4hCyKQ9Wxt2OPAyECGVeCCETc+Lq6oZSF0pVnrDDAuCcW2GZZu5ub+13qhtrBl00EaK03mrtsJAa1/rACPvqZ8kOkBpWVb3bLq/7+A8260T0jHTrFx3Dr0bH7LMb3XY015yAIOusv097feaNMOjj6Hq+jlmi0c26OdzzvVLRxtpBu7E0TyuRZp66WBXeG0a0WymCPJwV18QxX6fB6dVtjbV66HXzeLTRmlGBenL68P38tf7z/v7+Xs8//oirmyOfe+ddYkzM80K0GWqPnzxhGDMfP/86d/cvePed7+Hm5hEvPv6Yx5//PN//j/3jLNPM/d1zhSwO+w7bkIWrm8xb7zTu7mdiPrA/HhHRiOf2xQu+9ju/w+nunhQijx/d8PjxDdc3RwLNhm5WSoPSAneniRYScRiRUAjJaGJ14Hw6k/IdeRghRoZxUNggVkQSZalM08Llojoip9OJ29tbLpdLp49NlwvzfOFrX3vJX/2/v+BP/V/+D/zYH/8jZJsUHBDefnrD5Xzi5YsXTJcTGG2t1WrdZ9UceCUitKCaD44Bu1TlVhxGmsp0OuZf7bc5tLUsM9KEpOQGhgQxB8bYeHI18GQ88+y3f5EW9nztg+fsx8DV93yeNIwUaby8veXZ8+fMpUGFSGKIlTYJZZlNPF0zshjU+Dw+Djw/FSQkKgODaGNOEHjjyVPubu8ZhtF0M2ZCCux2e0KMLJemBhp9z1Iqp/PEuNPpF/f3J5Z53kBnPsUl9P3us9wUmhBtd7ahAT6kYDXu3lVqMEQQBJOdre7sTGCrYfffKKPVtLtbA59vZ+dMDXPtGP6r2YszOvSc/iPCE/49tww7DYZ5gtJQFBO2jq6oVJhIWGGGZsR0jMKyviH+sL3LRkR4/vw5V1dXZCeiO17XjbnJOIpP4Giu/LhGyzh8sRrg9WsaLTk25v91xwEP32cdaqjRalkKzz76mEdPnnA4HJW1YYb842cf8+Zbb3GYrnl5+4xaJz7/+c/x4tnH3F8uvPs9X6DVwu/8/b/HNN2jE5OF68dHfvhHfpAnV4HnH/4G426govzZKpHlMvH1r36Vy/2Jw/7A1dUVx8OBISdOty+MAta4LBWGA/PSWBqkrBFpyjqyfJ5nqkRtKNjtOZ9PSIBxHOiOUdTRzsbjneeZ+3tVgkOExSZolKp6wst05pvvnfnLf/l/5P/9t3+ZP/xDP8A7775DKYXf+Du/xd3dyw4XLctMK15QU0dXrZFmyIlxt+MijSAuHm5DRINSLhfD4mPTTsrWINmj6nWE6JCXVuyHmNmPkRwK59sP+WZ5wftf//ssJbLEHQxX5P0RiZmlCvenM5fpQh5GdvsDKWfOd3dcLieWUmzOncJujcalBN6/nbg9LVwW1R+DwPX1NYfjgQ8++ABEOyarizKlpPdcbGKJda35+KZpmim1MeTM6e7eOvtsr4fQgxuHJkLYct4dHlCFOhzGgwewXOg16VUiQOGO2KNknbnIZpqGluYkOKj0sK7icImL3q9abqwGmA0d7lNen3kjvJRlxaOMKwiAiEUja+10KdUeROtRcG2tGzx/eFiBQ0yfOIRg2Bc8vr6xt/chkXq4yoaLWJuS9OMGV97+d/v3LTPDNRn6TgyhC7+8+rP2Log431HxwhfPX7DbK9yw2Lj3Whu3L2/ZjXvuXn7M17/xW3z+3S/y5ltPuT9dWErh3S98gWk68f77XyNn4Y23H3Hz9IrdTeC03BHzyPX+CGlgKWem88zXv/p1Luczb73xlP3uQAhQlgu38wxt6Z1atQZqm6mknpbXWmGeqU0nB+cWCFFnzp3PF4TAab8npWGNjkSdTbHWYk8vS5mZl5lpnmitaIZiz+b+fuJv/a2/zS/90q+ws3bi+7t77u7u1HhZizNIT+d9JVM0c8xfL12F8LMpyw3DwO3dHTmvM/i0wLfSEnV8kXZOFtOjnmZhP4zEpGOJ9ld7nn10x3kWWmykvQnyxEhpKrQjIlzfXLPf7Xnx8iX3d7e61wDXoVhqIQThNFf+3jeeEdKOEEf2gzZ73N3f8sGH73O6v0da5TJdmKcLIsLxeCQPg7ZtexeeC2Q1E/kpDVphnmbDuDdDcdt20IEVlI2OppkTXW7V2UsPe0030N2mIO71FRXf8kBqc6asHuT8ZN8vW1xYf3h1hOoW1qattaz46a/PvBFutVJj6htiW7zy4kht1TRca+9AaqKsCqltFUU3j6vCLCtkgAiPHj3idH9Pa43dbqdcXPGHuT7sZl1jsu6TB5vhVYYD6IZxvunWODslrhcT5GHxrv/dCpGtVl58/Jw33niqGhKnMzkH9rsj0zTx5HDDvTzn7/3Wr9Ok8MZbb7O7H3j24gN2Q+LwaOBRPRJi5dJOnD+65dFbR2I8sH/6A7x8dqKcGq1MfO2rH0ALvP3W24w5U9uCtAJtJlCUkRLdCUELQbHWeQFmzRpCIsSBcX9kJJHGwvlygZgYhpHL+cyQBw6HnRXGPEqtneaVzRBrZ3GzNFcPfLOJHK1VTpdT/57TuJRvO3feamADLbVGtq616XKmSe1ZSc6ZlCLT5cLlcmEpdcX7RT9zKVN/5qUUbfu2DqCKOtf6cuE8BE7niQ8+vjPYICIJUk3knUBMlCZM88I0T9zevSSFyOl8YjqflOYW3Iis+3dpjRISOUIiMOx2DENmvkxcTifmyz2l6fNYysz19TXn87kXZHVi8pYtk9SQxsg8aW3B78fawOJnL/T3wYIVFWrXBiSlS5b1fCAPolClFRq8YZErYYUTt6qHAWvs0B/pw0Udm+8BmmP3IWrLflhNvgdhDwbzforrM2+EPSK1TENXMFqaezgbU6Mti8qkKEWLLjxI+Q3g32xogkYQbz59SrYK6tWVarFqp1Mk0mhBN24Ew6dsc7X2CaMLD4s6PcLd/JxPAHHvLa/Q39Y9bgUKdS+c7u65vb3l5tEjPSxVSFHTv3kq3Fw95aPnX+XXfv3/xfXVI3b7A0jjfD6xlJndOFLLwrjTzqoPvvkRMSSur56Q8xXlInz84XOuj1eMwwCtahGvVaDSlpkYGqRMAOZSIWoTSZ0L57t7RAIxD0hoDDvjJVsRVcTbigvLos0T9/f3Wlh5pUgkdi+0xdg7yEKPyFKCabpQm3bFgTZDeAbj008UFlobbhwm0mKfmOyntbDb913oRx1kRaFfEy7aPN9OL2xN2RAp0YCpNkoVlgqXAkMGiY2QMjFXssxkCZC0kDZNE/OydDhGatMqlDTFsC1iF9MUdnU0LwDPpilNgGk6UZaZYsW8aCm5NyFVY1wIsb8P2CQMiVzOZ73X0voUGDd4qqGhcqjNnucq4O7FuZW6J77Pw7a92Osm69mRthaqFabzs9polQdnLMj2tQ6HeERs+sti79ej+PJKlvnprc+8Ea61ktOgRYC4et9qQzSBB4bVgXpNsuj12T4qRVZIgr4h9AE+efLEinT3vUq8HXePaEV8WwDYFi700uTBnxACxfUNLKJVWGLlO66ykuvnduzN07/qw02nidvnLzgejwq3FMVIQ4icTifeeutNro9v8fLuG3z87H2WupCCFpWurq8IScWzW104l4XLZebm6g3u5olxGJAaeeONxzz76CPup3sG0xOopdLqTDSjtbAQgakI+ZB59GgPsfLy9l4zECvg5KwC79EEZ2itO0V1YgoXTNPcBzhO1pK8zLPda0BUK7mIKngJszqwbdFFhKWJUsn04XQcUppHdPqzKWUdGu9ZUi8arYZinY5tWU6ja98GG53sUTti/Gb/t2mStKaNIHPRB9/mhXFM5LYwSCBm6bCZ7lsBMc5sWO9TQKUj8XbdoPfEpz2XZdGhCBGbvFx7lthq4f7+TqNdE++JK4HWILGovGwpTJepQzd9IsYrHZ0eIavBdejBvhc8w9RjlkLqgLAYpOCYcGefGGwQDUaIpl/dn6H9vd8Pgy1EVODJ1dj0Z6w9PChFVV9vcpnfgfWZN8KlVlKpWs1vhj3FxhAG1d4VxU39sJRaVZIQT21scKAbPjEZ8aCFNv9Z3yQhRu7P5171RkIn8Iuo4Mm36p7bVmUftEeLFxOwlMn1JsRSMaXYrWjZasjXAqK6mWTykS+fP+fRk8e6XWulLjMxD5zPJ+b5MYfDY6bpou3G7UyMYpNyA/M8cbncc7ks7PfXfO8X/gD74ZrLWVt97+/uub99SRRh3A3shgP7w4Fpnnj+0QtiEHLQFDGnzFKE+xcveX53oYpiosREkEDKO+U0zzP77PirNQ10iMYnJoS1K62o8tm42zHNM+MwsExKT/NKeBNscGUjSGBIKlTURcEN35WgXVetrXKlMWjrcBoyrVjUFNd2dccm1TLbfDpM7zgE46z703r4rGHtqgtWzG2lEuOaCcmyUAmEnMgY9kxmmi/4kEzMOYQYlYKJ0MI2Aq69c7Tnd6YFoRx5LSX36LcUctYJ3OtI+UgMmWJt3FIr03JhWRbj5IbN59p+TlYohmJNUSbK7hlgDKRIhwKdmRBczRDRxxS1iCstmNEPJDOmfga7k9tkHioVazUVL9aJGvsYNpDlxrnSvjNilp95IyzNupXEOuTSuiE04miblEMN8CrK7tEu9v1NVAuQIrHpAEvt39d5Vx7FIf4aN7je+24RkV3HdlrGqynPiluthtupahrd6NoaYMWuV2O8Fiv0Opdp4eWzF+yvjzbivECFJoH7+1uurq6J4QB14bgbiVmIUQhBqFJoLXJ1fML3fuEfYzccqYuwH4989OHHfPThB7RSCDS4FT4ChnHUKnst5ADkTCb3FFkPcqMWgaDR45C1eh78sFjqWmux9FijnZxVdnEYBlysPG1I+Y6J+7/XaGjNcmRzb2NM5GwC37IK6McQVk2IYMwNyxCoYum5PYVuBPU9UxpsDwWDQ6OFWA+fqzcibPnEjkd69B9DojZhiCqiL01IYyLnBPc2+06gSdXMgIZYASJYtKdJ96Yo7HgqjndXthKqbkSTXXfoeWI0I6hGa15UayOGQMjZbwMgpLTCNQ8iWDN83mKOxFVvg0C287aibSpBEKLBHyi/W52DVWAM0HWxdo1iV5gqBs9EPICiQ5OR2Mck9fmC9izSw6P5qa3PvBFWYD72A1HrJiruqWK0bfXJ/nANOlsH/zXbs5ptAAmQ/WkRiSEpfzhGmxfmeqbr9TRZOZCvFuJeLa5t0zcNsmRNgVmx4C2socZLq9f6s80xF42oEO7v7tgd95qaSzO2QGCaJ3IeCATqHFguhWGE3X4gD5Fd2nF4+giRRF0Cl2UGCVwu93z00Yc8f/YcEBLaAp5SMlxUf0fOyVqEhZBN5jJHRAI16s7Xg20uSvzQG6sEMdWyapN/V1HuZDzwbWeU3aUVzvGMxf8OxoOtD1S23K3plGhtwFi7rtSJlsUmJPdJGF4UcgMbetTnEyFisEJhTnTxd9zAhgcJr13yyugJweb0pS6EE2JQHYxStG26LAY7BEPMtp9ps7s9uDDhc51u0R4MEehOAGvfteyjWkaZbZirZ1xqdFfIZv3sayDzYK9vPqcfr2CCTWJGUQMWhwm3Mxadb1wpIg9YNa4dEZNBEiJ99JFmB2qAX23kUBpbf/v1+/az22j+01yfeSO8pv2+BddJxT7qxbL7vhu2UEE1KTyPZLaRikeWKSV2+71JIzZCi1qFDRFY52jpCqSuSqFrW3DYXvdDA+zhuPj5/kTUDBj8AM7gWA2x/c9QkmmaqEsh71KPfiAyXyaujleEGBjHA+f7Sjkv1MtEzNrjP+wytRWm8wtGG+SZQuLm+orL6X5lBNRKTopTDllbZItAHOImMhHFGZvez+gpey0dr5S2FksAik2rKMtCXbSotqs7WlgLNQ+KNrKN8B5mOW6UQSEHn9Pmh15xdy2ubfFfX7U2Qla9YY8eexFVlK62BsGhPw+CirCH8HBmYG/GEf2+/zIvArNxHq3Wjl3qi6xIa887mC9QvNPueQeuwvoxDB6C7f1aI3AfdhmgaxPrCC2l4KWUKUVbjgNCENfVCD0a7qI6r2DCwSA8xMR0rEnDz5pYZKuRaTRYSEMmadIdtr53Y7Gx9D2attdtV888NjxiouPK68NdHYYHTK8x4W9rCV5M88hC2x2V+xnXNlJB6TGbAyTi875M4R9WPIq1WFdbXYcRmiHRvniPvux3i6J8W3nMVwtx/rXtf/3v29bQLd4WwtqF5J/6IQ4n1hobehS/TLNGT/sDEGz6buB42NsMuGqHPGPyVYQqJCKxBaIVO5dFBd+rDd88Xh2ZLxPV4BYR5cVqLuc6F4r/BdR4QWDIqbcyNhop5k4rg7am0Ra1+vTjZVkYlqHLYILCTx4tu1hM7JCMYbeIHeJoz0oLOsqaMHGapZETNqhz4wzNEntLqzSQFNaOTBP11yjLo8DQnX9KgaUsPavSe7Jqfwia4Wg9ao2+ongE18y4WITdQn/eWwU/fV/69ToM4u3D+i1XfVODH+zzBbr1fLAH51nbpENU1bbdToMPF/epxghB1igStsHOGhTps5B+vQ8CJjGYRKwt3KP4sL44pnWaS0qRIe+Y5klN+CvZJaxZo6z+ZnN+tF7T70u/f2G9BZv3/DTX7wsj3I2TR17mWZ0+4wWe5p43sBoQQ/vcOCvXcTVmIerY8FrWEeqr/RNjLnhUsUIGrxbk1nEtDx/0dhLHw8hXkT2NfNu38NpbKGM95MEMrog2NniRT6PPSpln8o0eotoKS5kZs8o8aoGmcjweqdJIw2Ai7OcudJOtq0rpWSC1UWWhBIceVCRcxOGFdVpwNKOFpbR49FqVAxyCYbUbCpnO0BuYLhdT91p1PbrjCupgNYIyge/grA1VP5tl7od7HFVa8XI521SMSsqZWkwvI2yr/fTD6UZ48/QQ8SGw9AyqR7LmWMLm9R7BpZTWjsdeQDOYxUWbRB3a2uLre8aCCqRH9j409Xw+k0y9T3nyWOOF7lc3QikGXFXSM7kQVc85mR5wjFoUrKUxDAOXk3V5dmO6jiNCJwmiDIt1QneMxu6xffgAnjPKGsHPVbeGCp9VPYshBnJIXF1fKTPplTOzUjzXkUdbI+x7z0+ViGY4DjO5E3kwauxTXL8PjLBtCLCH3IzK1MzjWlrKxgNvDPI2EqBH1MoWaLo7tKLsRtKihGzfq7WyFI+SPwk5bP/rf38VE36VObE1uFi0vaavmyYSVk/uWFo/UKiebGuNmEJ/3f39naaag/J4ay1cyqxqYQFEJsZx1IaU2sgpsd/vuF0W5mkmJ42OZpNg1KDIKvWbSD6g9DMPSqRVMFpRDMlgoEistvmNR9tpec1nnWknWjVRHZ/35p/du+ZyisgwcAlKVSNnSkwQK3VpHZ7qLBZpDLudYpS31njgEVl16tVma3j7LG6Tgzl2bRMm0KUs12Kpt2ZrlKb3fKN54JGbtax7McrZMq1psciLlt2Y9WuInWO72++JIXCZpk0hmP5eHjz4e7nRW/eNbjV3NDlmVYMzxxEQ5SbLq/tY70WfqtHMGLN2DPrPq7Rn22SKaxHbA4nWDPLwYbGGaS/LAgGurq94+fKlRvvyynmxv/te7JAIqwE2d0lMPv+wP+HvFCT82TfCbnhfsaWa4ghrxVa/YwGYSuDpxnNKGvbzpszfsVagQXyQfq3UGgg2ysVYkK8Y123a4+vVaPjV7h6V3hO8iuB8ZvFIJqwHuaepEh4YB6lVJyeEQIrZDpwOAj2fT1zFK7xLbJlnatDoKITEnXcGHvYgwm7cIVdwSWeGYUcgkMeBu9uXtFKIoodO/L6Yda4iNlhRmRC0VXA/x2iFFTHoQp9jDKE3D4AattinNQRC0iGm8zwRwqoJkHNmWaZOZ6tFKXsxZYI0JtOxWBs91FinMXFOFg0nM/A2bn5ZFrK34YbQO+4chvBTq1FWfBCdbzMbxXvXltlSyoNnp51fAkGdQwiB6FlCS31f2mXgHFrHu1sLDMPA6XQyRxAMqtE9kzfnRPdboBWfUrEx2A/+rFM1lqCjnUpZ7DpZcd5eOPZrWgODnCL1WwQZEVQnmmCdrM1qCL3JUgOCrkOh9+3jjz+2CN8mORs8I7bnQjBdbwu+gmW8TiENYSN1aU0bnpF8J8cbffaNMP6wVpqKe9doIFNrbiLWyLn1VMgiZQ3RlDjOmtY29BD1yGYjh9cMu22GdXoXEDyMeLcGeVu4gNVj98/i1+UHzlZw47qptuOf6MHf18ig2BDLmLwAth1xbjq2KbIEH9MUiAVg4Yym7cN+R5XG/rDncDjw+PFjdCZZ43S645tf/wYvnz/vxRONb7DMobEzbd+kYQvKf9ATE42krzZbcV3Hwtf7HRjHQfVsUyDnxH6vvGTHClOOMGkol2KkRT38MQRq8OeXqPVCzkonSymz36nsZwyRFiJdpL95RpU2nYqySUw0vg9BAJdcTKuD9oKUGyAbIeF6w2uLOhr9prhmO/4cHXMW7YQUy7T8zwPkCn3vnLNN1dCfUSOTV7H5lFbBdf/9emmrAY2hc+YVCog9MxGpiqt7+BGisXFY388McGt1ZUFszkGPvsNaJFWKmjdpeFYnfWDvejY29FKckYK9ZovtdrRQP3eISFzhHES50tHokhJMwEs2B+5TXJ95I6xTNZoNT0Q9fvAqMugXoxWt7N/BdCU8XTL3H6D3lNs2JkYn4T8UWEeAvJGW3KRTvl6t5L9qoH359zWl2xpkf4+HtLUumh3XybMg5gykOyWXXkxGIav9k20Ohh0IaZUqUK2ivCxwOp24zkpBayjG+OLFC/b7A/v9nv3hwOfefYfL5cJ0vhBCU0cmolBGjBQr4LnxgaiaEUlJ+dvoJG0PfH82+njUkZi4eAgc93uN4A33j1GLgDlnyjwbVKDvtdSiPFu0s29/OPCFz3+Bx4+f8Mu//Es9+vR0VluoFf9urdj+8MGR1ilnTsZnnjX3xJvnHnGuQnhADQNn4Ky/c322xs91YRnRPaE4N9RXsqhg++F8Puveb7pPfG8Eg9eGcVyfNyqEHqhISMiiWUAect87IUCKuUfEQbSA2SNU1jOhcJjDffY5YiA2HQ223fvrNeid6TSz0ljKggCH/R5CIFkkEl5xzCsUt+Ll+m+9I5pNrTMkvfDnN1kLmKYVYi/0CP87sT7zRtiLB36HH9LL6F/rlekmHYYIwXE/cYu3iUIFP0bBIIct7kpUnVk1IAmiY3YOG+haxU0cTxO77m3765ZvSo+4BOl4mcfwzgDw93zIkvANaRFZ9RTL8GRr3a3i8eg61LP0DEFFwRsqn9hEuL55xGWe2O/3DMOOZ8+eG30p06yoJWgEFc3wBUvtSynkrMyFvuEtnc15ICZtW/ZP2NN5nPWi1zcMOiB0HEft2EqJIWeWrDxWPVIWUYagBcKcKFV5ya01ctbZaPv9kR/6Qz/E3/27v8nd7UtU+MmygxSBRLJpGilqw0hM0dq/1Vlcpov+fNZ0dl6WvsdCjNrQkmKP9NaCojtq+2zB6GHBikrQoa0gQgo6cACLFCUGct6trddu4O1etbhaQo84hYcNQeJtySFRS8WpndI0gMnDYPdUjdhSFuZlJqLXQ0rrWbE9mdLKV9YOVdcSpp+37b5Xrr0GRtvswM9XpxbKGvU/hHgMyuigoDpKPTeh7zVnOAHaHekwhVH2FOrzDPJ1x9y3tdzQbm+2Li9E0CPMbVdOtaKAR76eSnvw7JGKuHi3rJSnHjnhXTuhG/QYdZN7KvqAteCqVPKQQcG2ZBD86u2PWCQFPVWF8IkNu76f9Hestdh4bzYbz1LhZmIxUeehpehCOpUEiEXS5/PFCnkDL1684ObRI8Yx89EHH1JK4a2339Lmj35fdDSOF388mosewRjwF9jwYmMyw7uNkrxI2h44MlWvM/wwJX2LFI2TGzViTdEaHjIhKuWqiQ74FIGr45GPnz3j13/911XtrEfCNg05KtQ0DNoJWJaZuiw94lrsGlLKHdrJw9DxyWCOYLshHRLQivwrLAH7uhft1gJs64Uqjeoq24BCDZEOxXQFMJeH9OkwPfqz6Lc2YwxhEFk01bmU2O1Gnj59i/u7s3YNxqgMmyamNlf7PpegBcwV0dtmgL4DPQqyZgtrunGx9xXWsGK3yYGqcTUjrS/vZ87hxn5ON8EWor9HNvf3IRtDD9G2VtPV1MxgfyfWZ94Ia9RqGJJ1qtldRaNh46D2qNfKXFU3UnMCvBsMf8giSKs9lfGatYEbKpgT1kqsH14XINkKmmxVzz5x+X2T9J20FjtwDx90/Aqhb8hX32OFGXogTW2FWjRiCjGozq1tXJ3h1ailMAwjZZ4oRTaGxGNvtJXXqtovnz1nt9/zubfe5uXdrbbVRh1g2UrVkTZR21jdEaq8p2YX6wFy7xl7JBRN9OZhgcdH5Ci0UkphHHUSsL9FzlmNbkxWtAmKhdpgTVpjGARJifNZ9ZOztd3GnjnRoakQtNAFqlctomwBl0pVnH1TvBXpz9kjbumbE430PSzsd2BNrTW69ijYOL097Rb6kAAsytsEAZ5Z5LCySHzIpu+rGJIVoAISTQI0Jy6XCw5NIPpsbq6vub5+xPsfPcOHIkSjrsWYbB945L3uWV8+m86hwA5f2M/2qNS6ijzL8/b/LdzgENxq3w1jd2hOMAele9Ux7z5lx95j24kntTOSuyHu/O/XFLVvb3XcVayTTI+GFhXMI+vhXSzaoG8gT+fU4ARqV+Y3k2jpUC+wsNkY/QK2G0s2X+87bCX9vxIBb6OHvrapVzDUaoMzs3ES/vndCK+GS99ImqlkCTYQ0Z2D4mzRjGdOkfu7lw+MyWLNGQEtpgWLMlptvP/8fZ6PA8fra0IsJqzvjTBCk0KQxDTNpvnQIDR0iLtJMEZ3JNYQYyCum69W2/b0aTSe9SC5qLvhGvgk45QTsWSDnSpDztT0UGPi+vqam5sbvvGNbyi8sLmX22h1HEflVS+LaTRY8mriQkQdHbWFlfx93GGARpgxxrUFu20/10ob0+cWOp3QM4fWxNJ2i9QRdBqzs36sgSOsehQhhN5Zpk4G2x/63GurLOcZF+/xLHxeZn77K7/N8eqRniXPBi24cZbRioA5lvoqW8d/32qUXZ3Qz1Vt62dXo+pwm+0Kw5qRtV6yZovmnFwewGiSqx7wBiqx7rv+nC0I8fPc4ZH2UND/01yfeSPcpCKyUrxWeAA8OlZDu/ak69+bNWSE/lqPiHsllzWV9ogpJi3KRFkfXouxV7P9vRXqCJsduxpN2EbAm78LNjNx01nVPyfmEPRrD5oJXll+MEII1rDRPDEAQhfFSSlxfX1DWSZNVVk1VV08HejRscoHCjkFTqd7Xt7eMuTc722yAoo3yTxwXj31DTRUv1ezjzWCL6WQ69oJV2yuWymVWio1aQPBPOko+2VRGpbrhLTeNGEFvtoYh9GGWCpcst/vOZ9OfO3jj1Y6olhkhrNH1PguNuyUEMlR2RNYVBhYcd7QDaC+jwgdf95CDutnXZ/9MKxHtLZCTqMajY1RFaPWOUVO7+dad9i27YYQHkx50SBBW5KaqH5EDGvBzA28dpoGahXuTyeGYW+C+EKZZoq37Jvz7kwQ8WkWbXXwYS28+SbtVDJZI17ZOCSnXvKKMQbt9PTAwhkMXUBetprd6znze4E5sbWA/hAG2q7XFLVvc60Gd402V+O2Yj/QqNX75Dta0Q1wiIEkgbKJeDdgkxkRk8kj0IwXqYpN8SEeLcFaRyMSWm9dfVih/eSG2H6/e3L7ezAvLmaMY9wWRgyGsPfqhzKsE6cJ9ALVli5UbfpvzoPBDrU7o37vxDqMcjIGQ2BIWtRRsXSDY4bMPE/dOA3D0KEPVSjUuWwhWrpsPGXPPlYK0po1aNSicVJKWYtxDkOkjHOsPHL05oIy6+/xYZQxalHr7k4nBNe6qJpW8I63jUPEWsdD6FNN3Jm67nHovy/1Z6H33zBT0et1CKU/qAf7NvQxTZ6Oy6a5xtug17rCipf7vDRXoNvi5iEo1bJu0munAJYNbu2pfbTsqaHZFxYBO1NEo07TQTYtjG1noN6ntLmHay1kC5WBGC1wotb1fvSAwu9NCg/Oxyr9qUGAR9Z07QphW1R7AGeIgL2+NwJ1g76d97y9gk93feaNcOcZAqDtjfadXgF1A9X/CASM/O6puznu6Fjohhus7yOIVE2pAzrD3fZWsuKWNBVER+hCOlvrHBws2aSu8AosYekSGyfwMM178Jb9XG+xST9g0lAtAAl4Q72nk9vWX5FIHvfsmjBPF9PdVe50LQ0xPdkmjRZT57WmFE1LIKxGgaRKYsQOISS7l9FEaryRuSuS2SfqQtvNJ4+tDTX9gwdXUTPIyf7njqY7PMsYak91Le2uQoiCFJsz2IxBsolS+30kqPC6uOHRzjxvYY8hbgo9GENgbYKIlnY7c0BfEx6kw2uXXevypYgb49Qd2tag9YJWDA++H0LQlvNWOV8uhBBsFFPqjk4/R3zg7Fx7t4N5PXtRneFaqrZ0h5XD27OflLp2S2Slq/kDc3w45ACSO+7qjufVlm8/J/SASlXtQkAjdr8HHUKKiERcYH9bX9niy0B3Oh60PTC6gQc/+2muz7wRdoEWpJli/7fC21L/eY+usCgHWaMg3FhDT+fBgf3tdGP9+d4YYEpsMYJUxf38MsKrOO3GUAKfiEy7wbFigUfr27XFgb8VjKWVdMXMpmmyKQp03M2FcbxY4/dx3O0RglLU/DN4mu4aGi436ZCJF3wwbmgIHPZ7jdDqyhHG77nfd4u47C6AcbJXzrB22nkBZ61w6/fEBIOCEfyxp+hwTYwZYemwkt9HZ1sEjG9thrzWNfNQkXePkVqfylCb4MHlyk81GUUzTCkFU9uTdYjlJnWmP95N91Y3/O5UfR/Lmnbbc689qpYHEbDvoe/5nu/hfD7z1a99DQk2lDRnYgg8ffqUly9fcj6fLYJsff9FK9ilEBmSQkwuIVql6rj6/hxCDxZErEFGfATU2pWn99RgC4NUfLL5tgDcxfo3utvqUMXOND2IUD64tb33/b82arj2S88K+/3FYJRtprvJHiWwnbn+aa7PvBGGlS3Qper8IIY1bep9+iLgXW52070qr2m1sRM26b9imFlbSWslD4qnevtpjNGiu0jdyC2uXjmsEcG3SEntwuhXtIVBNvDEw2q0fMI4+3t6Sg1KsNfIUqlMwNoFJSsmHrPqyI67kVoPXM5nRLT4VZsQ6sKYho6/ukxgSsngEcXLs3G2tTXVCl2dyeD3O2qxb5OdRMJGeS3roQt07nWrKvjuHYtru+x6HTlFil/TMFCXGfJIW1SEvlV97xhULlOpXUJIGRGnnVl3GUrNwiNVjKubXNc2dNhGN+Ga2G5Fy1MwmuPmGb+a2fjvdCPs91aw1uu4aumq7nGlayuwVv/9PogIbz59yul0UsaACFdXV3z+85/neDyaA47c3t7y/PkLbu9uezCjTAobNyWuZFdoIgwpkuM6IdmLncE+/wph+XDPVVpUf8QYRdJ6c9qaHfLAsEf7fI4fExx2EXO2anj7+Chbfh+2Rcrt/VZIA81qN7DFd3L9vjDCQDcmyoxYPR4iaJv6xjNCn17hylsPCOuGv2l6sx126A9wjWh7lNKLc7Efpl6IkHUjbUPXbUW5+SQGeTh51rHpcbdjmWdWOEur6WtUukZUPYoLaFHLKuV6l+xb6SGNLqCNB60Fht1Ik8Z80esRlK9ajNrlB2FLA4pRi1chQFkKcRgYdtk4r+qIUkpgnUwxRjBalju8HmmJH1h9LmUpPX32yM3FvbXY5GlHIgRrnrCoasiZOWqkM+60SNcWLdjVUjChgU1G4tcTUOlLo8vFBOJC+pCN8+2puBfs/H3GcTR4xiCz7hg/CUM5XuvIS7RIU7OU9XODwRnenGTPf2W9aKv5o0eP+IEf+AHO5zO1NX77t3+b6+sbvv71r/PkyRO8FvDo0SOOxyt+4zd+U4P0pkUwvf86767WYtNOgmlxBEvp1+BhS0Hb7sPtZ/TP2TbZgJnrHpx4kOBsDze2K7z28D3XLPNhdul0Us+8HgjZY52lr+C/D7HrT3d95o1wIHRyu6ecQTyVWZNdB2hfLYi9CgO5AXbRFdeN0OnNaxNGTLopQ4zUqn9CMJL+1gBvNo4eme3v0mvqRlP4xOb1lG1Zli7hF31Tbj6H8MkIy43LMm/Gi1ua56FHsgKbp3SeEsaUGMaRWhfFhdEW8UQgD3lNr7dRkRlll6ystaoanTQdnSOCK5HLK5CMz0YRtHAiDz5L4+7ujt1u/4moB3GutjM0It7GHGOkLJMyJOz5kRQmqKbjIEWj+pTSxqmY2E5wUfJguGToeGlrlhX1BoMK/Xcrlc+NyZizGZ8tfKDPdhiGTmnzrai86mwOYYud04csrfs32fukXuQTEd577z0Oh0OPDHNO3N5q0XSeZ4twhW984z1c/yTa3gWDCzoyb5miCK2K8Z3132rs7JmZcS9lZWdsldTWSHh7JnxEk6ywUvSCGz2ocEjBLsjgvvDg/PaCo0mMegOR62WsEJPYfn9YGN8GP5/m+swbYWClh7HxhiGuqaVXuHkFDti45BjWAoMbXjE4QSNnQJqpalWkRfOqbZUg7IbF2crBInSPXOn8Sz9U38robpdHhl48Us/+MBJYU/01GgEvMoY1irSoWTE5MSZBNoOyMS4W0RMjUqMde8XOV760FkWCnZYVt0S7qRDT3NjIJRp+iuG4XhSKKVp6vWrYxrQObGy1MtvI+lrrA4OZUiLlTDIDtPSILJo8ZCSKdgQ6y8JTlU7w30RKvQ09NFxBmuAFILFmnkBMK60xhEDMWYfIBsXFdZxe7JAKm+xJn02wacmAN2PETZbQoSh1XKtB7I95Q5HTf//AH/wBrq+vuX15y+VysakYJtpj2LDjqSklpuliUfuOlIYOr3lBr1bRPnTP+lICWWE1wY21GurOBQ5r44pHxe7ct6plwSL+tOkQ9M/V92/Tv9SmNR8vbCpRxH7z5kz3YrN6r/51LQCKP9F+9vt5C59sgvq01u8LIxyt0BOt0OY4GHi30QaLc1zS+syR1eNqJLmJBsBM6Yp9NRHVEtgUS0SEmnwahuLCzVo6nd+7fb81gly9+zZC/wdhVL7xYwiQtgdwJfY/WPa7l3kxI74ly6+cTv91bqw1SrAJFyYsH4l0hkVnjqixSGkzwSSqwUspQtSoGVkLUVGBepU4VO9HjKlHP9Es2zYqDCFwc33TRWicwzyZdm4tqtjVC27WBt0jyY2oTUzWZJESWbKOkq+1p659QGRwxTy6IUFW/JYmVL0dPTPJ0TUVbB6ePYIQdQ+5qMz6rE2JL0WCgaTVjPDKXLF0PWrE6BCEFySjXa+0xuVy5oP3P6C2yuVy6eL1tSrn+p13VGzJo3fv7oPIH/yDf5C7u3s+/ugZISQtfhq/+Hh14LBPTNPJoxHA5uWFYAYtbLBt/SzeYg6s6oOWEQW0DtG7/wRTTVvPgr6eTouMUYOl1eaKQTcrNCSsWZb/D3OqPstORHrHpQZUPLjuT3t95o1wDElbSoNieKC8VCyidam9FcOyF9pZW4neguggKnSDrRvdMc0tJWgL6q/Ynspmrget2fv6O1hqR+gbYHtR3QCHh7jbt/pd+vWHH2jdQ5tIG+EyTTSjSoUQNKVkjTxSSixLoRTpwufVCyruzPB0VYn+kRUqCUbR684vKG4ZLAtI0ccPrdAJBLJV4R1bV6qf6gnHbgSdi5tZ5pndbtedUYBe8Pz/sPensbZlZ3kw+rxjzLX2Pn25sKvKFQyfCRFdgIRG4JuAQLbcQBAozg8LFIHigEB2JEAChEQsSCJZMQgChICiT8RwZXKT/AhRQAEsEHYa0/mTLwERJ9zPiXFIlcHlqlPnnL3WmnOM9/542zHXOmVX+ZQ+cuxZ2nX2Xms2Y47mGc/by2OjokYtAiRN21QKaerOih2JOgKYQL2jm3XfFi93cGtAraACLBq0Icxe3dmK+RB3eY5uAvJ+BVw1mg3Z8BuHJZ+hognT9X1qrRpSXbAs6t+qoCWqHTmzUKjhWhM3sv/5x3/s83OeZ9y6dcsB96mnbmKzOcPjjz+Ol3zKp+Cx//W/tLqEVOP4ow/8ER560UN45MUP44kPPyWb89JxfnYZ3Gf0NmGed+oKaptBcnO0YrMITwV7F0uzCWi+XxBq2Xgy+2xfyZGpTfXANu9qsUx28GT2puTjtKbFoDmNZEbzmYSuPdaKbbxTeX7g8lmnBXrnO9+Jr/mar8Gjjz4KIsLP//zPD98zM970pjfhxS9+MS5duoRXvOIV+G//7b8N5zzxxBP4hm/4Bly/fh0PPPAAXv/61+PWrVvDOb/7u7+LL/uyL8P5+Tle8pKX4C1vecuzfzuIMclYqs1kY7tDoo4MYDYIFPqnDg5dlOknnQWFKJMtr0CIvRYk4L5pergbkj5Xrjdn+JIYYojPBhbZIT5PKHsXU3Gsvw+GLYt1PkS1hUlLBIkva49E5yzSAXfGvDQs8+J3M9ZLRFjaAmTxXT0aJDc3e7stV3FIAeHDaTrKWqsCdPIlzu+soa1ilLF6d3sf1+J9pKDt8yCFBhOAootVn181c9t2e4ZSJ0ybLbZn59ienSuDNeAvzn4nzW2RSyfJRjGpTjo2GatCXTS4pZbq71bUM8UEMTPY1lq974y1bzYbvVeJKhf6E4awomBXceXyFVy9cgUPP/wwrl27huvXr6t0VLA/HPCBD3wAFxd7vO99/x1PPPkkLvYHCR+GpML8n3/8x7jY7bDZTBAXuY7Ll6VPWltUIrJIu6T20ncUVZLNr+rqrs1mE+yfVVKx9adj0nqX/NWaswMAtpstNptwW7T0oa01VfFAPThC/eBkh5ByeOQ1E9jQWhfJl1U99DzphJ81CN++fRuf//mfj5/4iZ84+f1b3vIW/NiP/Rh+6qd+Cr/5m7+JK1eu4FWvepUmA5HjG77hG/D7v//7ePvb345f+IVfwDvf+U58y7d8i39/8+ZNvPKVr8Snfuqn4t3vfjd+8Ad/EN///d+Pf/pP/+mzfkHxY+waBJtYb9JPHTHJQf9KvhhMd2Wls9d65Dzxy2ohmO9rVZ1mXnSllFiQutCK/67Ak5JOW/kXD0RZGeEymNmxBuO0z2CZZ+wuLjxyqzr7EO8JKVEvOQUYUmduWRoOh9lVFGZxdqMlS0SYZa2SaK4CcIBOdbWAAp+Ck/kceyi4vReQ9IgdyzJjv9tjtz9g1nwR9q9gq4bDqhrGEvVLLuLwcrHIt9Ykk16hCXXa4vzyVZydX0HdnOH80mWcX7qEzXaLzXajm0SJNk+T+NFavg3Vr+a5YHpTd7VbbbLFWH6Nv6fJ8nfIvTfTRiIBkQDWNhsQJgU3aYP8vt1ucaa+2S980Ytw9epVnJ2d4YUvfBFKqTg7O1OpQwB/vz8ALMC73+917svceeKJJ3wtL0vzYJaufrl1SvXqyNZNj3nsRKMiMguWYS1yt1B68vBrqfoi83OaJt+Aiq4lG2/TwbuRlDEQH89g2LrXDLQSVKRs2tbQiA1/htQRr3nNa/Ca17zm5HfMjH/0j/4Rvu/7vg9f+7VfCwD42Z/9WTz88MP4+Z//ebzuda/DH/zBH+CXfumX8Nu//dv4oi/6IgDAj//4j+Orvuqr8EM/9EN49NFH8ba3vQ2HwwE//dM/je12i8/5nM/Be97zHvzwD//wANYfzTG6p0jeBnc3QuhRsfrdr0+fZDcgy+FAPoHyfkYRZUQEsrh0JcxRVj1cnyTUOSUBYsACS5ytm0qBWdWvBFrZCoKFR7Y1ha+kz5XNRfcir5rMXbKmgbX9dSMbjwIXGFi8DhtgHiJmzOuQ/BCMCPeV4Lky9LE8u6LAkq1XZ+DiPcLjwoSqaVSHLXrejlLZvQpMB1xKQStFa/uJ65rkmFjCd1hZWSHxW+ZqLoas+kjyEOy+NNy+fQvcFxQCeplRTI3EgORU6Km/Y0P3jF48GiYHn3QgGK4m/+lp3saU0o1IN2Rhi0EMmLsweo55IP+qX3YpeOELXwhmxsXFBaZpwvn5GTabDa5duy7v6LaIkPDMu0LaU2EG0u20wZUHL+Ps7Ay1MG7ffkreOevzZNWkd7Uf8rYRdZQCz0Mh58JtG6UU9c5hcLF+EdWBERDiyMWRfZGlL+T3kpIaFQ3vtlgAI1SixggXUJdmFdxXMcz37LinWYrf97734bHHHsMrXvEK/+zGjRv4ki/5ErzrXe8CALzrXe/CAw884AAMAK94xStQSsFv/uZv+jlf/uVfLr6UerzqVa/Ce9/7Xnz4wx8++ez9fo+bN28OPwBcFLXBGIxwaaHYQem7cedLbkjMHjxQNfF2HXZ6DBUKhPlGDom8w5p45Cx8YLMF427cw7AA8smzVqlETgwLwcyTysTw9Lv6+HLPRh7151RgMV2a6c2qJku3hWmL07pT2IsMQJmquK25QZSSC1F2lk+BFRtxwbLn2/uaP+iyLFjmWdjvPGO322FZFuz3e+x2O+x2O3e1knGWCMFIBC/sd7PZoEAqQ2y357h+40Fcvf4CXLp8DVQ2mv5SjF4WUpslFfk72l0owq7tnDUjDsavARDKYGuRKiVFGVlZ/Si1hGW3K1UB2fMlmyorXAFJ5/zly5fdJe3JJ5/ElStX8P73/xGICJcvn+Pq1SsibRW5plh9vpJVQGJ0vnrlMj7lU1+CRx99MV700ItwfukSrB6hvfc0hZQTP9FOIy/TNAEaBGJqiqimouuNAtzFwBwAbIfp3Ad3NQoVUzbqGRbY8g6idowrFhgjc+f5QeF7CsKPPfYYAODhhx8ePn/44Yf9u8ceewwPPfTQ8P00TXjwwQeHc07dIz9jfbz5zW/GjRs3/OclL3mJfFEEPLqG0+ZFP4AhpTHTI4uJsnAmFw9rLSL6kYqAblAjF8kdXNgMUFUt/cXF7LJaoNIW8smS1VCWO8ANRIjJDGRADvExaVechYbyAM4OuHXXqU11rPpbU/vMuJWByDaXpnla3fLPAU5DQc4an2WxXePi1EgifsdM4gpICVwIGrbLDFJ9cG/Ns6rNmuEsB5v03jyqzgtaEokBjru2r+Ls7ByXr1zG+eXLODu/LOXdVRduyWqsgKjNm7zBi0iPAcAsLNjr4+k7iEqMA+ApbAoeAm9zIfWlpP/MBsvQT2cgMv10AeGgSddv3bqFSUX4a9euovdFSMvTN0EE1ATkWXVmniCiUiHM8wH7ww6tzYiAo9C3BvBC5wL8PPMhN9bZm6n3LKDK9OGWmJ59w80Z0Wxem6oxrwNpC3u/kw4WGR6wuSPGundMgJEIct9os8c8H8fzc9f/B47v/d7vxVNPPeU/f/RHfwQAGvAUSZvzEfopWzRFQba4vm9Tqxo9rMCnXqMMmJQVA9aZ5A7gpnsG4Hq7kvSIVEaXnfW/vlw5Pi8WnUfqJsZZYTK+m7fZ3zP6gFlFL0iIru3yBt5mGMv3M8AEJebRlFmqLy+tWK6pR3oXUKjan5MZKaX0MnpTh35oCHJnwIC/2GajC4YlQQtBvBo0EgCcAMgXbFGrN8v4iGRp6gRTpUgWMLMfTFPF+dkZ6lSxu7iD/f5CVAUqk5JGhhXNwwtmB1YvF0VhBOTefBOxUGKYdsxAKjE9UYuxb2YAOSPLP6Y/d915EQPddtp4eHjo9hf8zw98ALeevonWFnzoQx8CEeHKlSu4du0qNtPkqhM7XAxHzL/tdoPNZoOdqn5k3hPQc+BKMBnzx8/AnOcUIEVV7Z1hc1UB3D103H4gG7X5jMMDp9IGpxsja9+bnhhKiDzwROfQekPNREzMkplB3/vjnvpcPPLIIwCAxx9/HC9+8Yv988cffxx/6S/9JT/ngx/84HDdsix44okn/PpHHnkEjz/++HCO/W3nrI+zszOcnZ0dfU4EVM03WkrMeu9YBTMDYUIsErITk/7UGSJM0glLuKmgfBDdZ9Ndv70NwmQYzM13f4t4O1YvhJ6NIWHXxbJTJZHMLLjia2zvH22Qv0NnbC5iystg4qyfT+wqkFIBbh2bTcXhQOg9vCMk30JBR8TphzongL/3rrkWxGe5AFKBBJpzgVl9/xmdOspUNMpMnfWZgd5RiMG9CbvtURKnm04bQNPgDHAulWMMyYJlxM2JIGqp7VZSKW63Z7h9+w52d25hPuzAfQZb3T1S7FT2L2WgtPRTP7agh6rFxl83HSvHA5JAgRU4WRaxAp1jKlLn+03ThMaLp1ktm42rTIyRu/qNWdNaQvtnxoc+9KdSsv4wR34VHb8o0aVzPUUNmkqAAex2O9y+fRsRwi82gVzd2HJXdC26Kz0BTwEAggdypJ5zQkAkKi2ZV0DvZmNJ+Yl9QzUfdnHNM5uD0XSzv/ictEamMbL72ObKGKM/7/VxT5nwS1/6UjzyyCP41V/9Vf/s5s2b+M3f/E287GUvAwC87GUvw5NPPol3v/vdfs6v/dqvofeOL/mSL/Fz3vnOd0bSbABvf/vb8Rmf8Rl4wQte8KzaZD6l0xQZwUwUXrsGAUAaH2cfFqFkYnjVEvF51zQxphBQ0+5v4pwBvrEF0e1CNwERvYEY5qw7NlceUV1UFeOaKxUYHLXwCqXFEOoJIYs5JNRekpwpeKQZBUu0UwBZ9A8//LCKw1VdsIrm2Cio0+ReINYGuV51odMUagU7T//tJAappqGrBrwlqXUkT6+vJ4mYMgMSR3IdcU9Tl7llMXoPs5ILgzcDrT6zM9p8wIc/9Ke4c+cWdhdPY3fxNJbDBbhJGai2HNCTgc/CbA1EDGQFBJa0gcpm665xFODsOl1jzojNd7AHINz7gABL1gGyopuc5q3dK/ut21iHWq5jv9/5WgsVnDxrs5mw3W6w3Ww9mm5ZFlxcXGC32+HDTzyJP/3TP0XocSNfsq0LaWtRAI0AKXEDa6kt+RoCEKoHM8xav0i+DXsXfU+bC2TkK9QphBSAhdS+tN7y52XF3CNHzL0/njUTvnXrFv7wD//Q/37f+96H97znPXjwwQfxKZ/yKfj2b/92/IN/8A/wF/7CX8BLX/pS/N2/+3fx6KOP4uu+7usAAJ/1WZ+FV7/61fjmb/5m/NRP/RTmecYb3/hGvO51r8Ojjz4KAPj6r/96/MAP/ABe//rX43u+53vwe7/3e/jRH/1R/MiP/MizfkHT5RCRRF91NTbZzgjt+CQ6m4N45p+ho9N1pEEba/ZoE1tniK472VFtEpvnhAOT3phL0bys5JKqx9XrJCNjBlC/yPS7VTFw0RYZiEPktfeww1jP2dmZG4AsEs50eUTketbLly7j4s6FTnZN0wh1T3LXL3KxUSZwMGBwAxcWVqzpCKsmHLJoplql7L0FJgABvOCO7UaLh7YF2GzEc6EvKGWCVSteloMwVM20BlZVCac2UkFBA/cF7cDobcHNdsB+fwcXd25hmXeqTxa3t7bM4M0EhhTPFN3rKUMuEOZ0di8pxghQlvKSYUEHcGnK62TIhBvuT6tnkryev5Ndt1ZxGZuutQpgQw2LILRUPYTMZU4BeVlMv15w86ZkVdvvZxz2O2ecNhcKTTrPrQcIpB4vwgmEwZZagKapPZP0R0Q6ZloRRXRn/l42jy1SrrfF1QY2sc2wZ/cz20wG+wF4mYdF4fm0LWjG1vXzcDxrEP6d3/kdfOVXfqX//Z3f+Z0AgG/8xm/EW9/6Vnz3d383bt++jW/5lm/Bk08+ib/6V/8qfumXfgnn5+d+zdve9ja88Y1vxMtf/nKUUvDa174WP/ZjP+bf37hxA7/yK7+CN7zhDfjCL/xCvPCFL8Sb3vSmZ+2eZkee9KQglcNGC00iunVbmApe3QlboG86bMe131kXG4PVeb2EK1kpqBSuNLWLyNRbA5eKYrpJyrlq02IDfJJlAxs4ROROFCIe4O5bpg4xScCNdwmU9/sDJjX4iPFE2UWz6CdxP9vtdz4ZScNwoeI8JREuaoBZW9UTxYxrluWtEGD5Isz4QfJZrZO+ZdeIPJEuDocdHnr4YVy9cg1Xb1zHh598Eh/68FO4UW5gUk8BZpEOKhGmUtAtr0EpmKYCbgSUyGzHpCHe/YA7Tz+N3cUFAGA+HDBreafNNKG3jul8gxd+0gtxOBxwtt1ing8KyBIa3NptGy3NnSB+sq1rKHIao6I2BCv9ZCoEYdY5p0nMh1NAAmV7xgyNrdt4ulif/jbWq+QclapvEmb83W42KGXC5csT5lnm9HYr6pdSKs7PLmE53+OwvwNAbRZ5/kE3clMvaJtQCNwsinRUwdm/rWmlC813PZUyzFmTEeS50d+2uVmyLF/Tef0a6HKEwB97GkXfPp8H8VqJdZ8cN2/exI0bN/Dl/68vFfco2yEpqgiYPspYrjBJMuKhEzQSiWQRxiylNpkJyrRTTl5QFfhgOLtubcHhsOCw32OeJbigq08r9HvLcZAnh7TPGBwATR+ZE7S4OGr/Z45JmVhRTDSbqITr16/jsz77s3WzJCxLUzcwSdoCIux2O+wPM3YXe68Tl6P7hKSqhwSkIvF2u00uZ8bCJnFJqha8skGdNmAAdbMBSsG0uYxps0Wpky+CK1ev4vKVqxI9tbmEazdu4IEHHsD/8WmfjA996MP47d/5/+IFDzyAq9eu+jsv84LDYcYyH3A47DEfDliWA+a9eAucX76EP/foJ2OzmfDkhz6I3/vd/wutNdy4cR2FJJBhXmaUUnDzqSdBLGkuH3jBAzg/P8fT6g556fwcD9y4gSef+jA+9KE/RW+z2gjC1at3E5mRxtFUWSkoyDYshLQFAKVqxFkC4tYim5npSIvve2swPNZXBzjJOHa9p0lEl84v4zDPuHHjAfU6ETfAGzeuoy2SJe5Df/o4Drs7ooohaUdnjtB2EMAS9dbagsM84zAfMM+zpCHVaE1TdYj+2Jhwd31xoXDx9DVhvsKm79fv/F1dDRMbkdlM8rrI9pe1AdHGsfWG/9///Yd46qmncP369ROo89yO+z53hPhuWlIPDSpgq8JqTFInOZLKoYcOcdATuz5N5rjpnATPydUathI2ZQtORoMOs/yqJ4ECvNQmY2cqwwJyPZX5BlvghbXRRDDljSlFph0Gzya8xt9yTwmACD1muKAVF1VbWySslMTtylJeyrsLwIiIGH3We9c+isneG2PaEApEVfHAAw/gBQ9+Ev7oA/8TzCwhyoDcu82S2/bGDVy/dgVP37qDxsDh0HBYZty6dRsfeuLD+LQ//2n4gr/8+fiN//if8KEKXLlyGdvtFhcXF2AA+90eh/0O87zXXAozuHVcPF3w1J/8T1zcucDFxW3M84zP/bzPwxNPPIH9bgcQ4zM/8zPxX9/7X30cGIz9bocHH3wQ/+N//A8QGLvdDne2W1y7eh3cO97/P/5v8aCwBa1MtSW9pjE4UnUSbEPXnLuZHW+mjbjLafAFoBVaEoskC2ZYecEUlcZMD2vzOM4x2wYNQNUa486dHZbW0NqHsdlsPVuc+L2zkAivbqL303cV9g9YTUXTB3cN/DEfcJdcePE5E7riIoZYHjeTTCos0AUqhVrf9t58LmbPoJzofZSSR9dVN3L6yz0/jPi+B+HMJOR3+IQzkSQbLpwJ1/BxnEr1NI/ZE8J2XgCDMcQOycvSBz1fqQW0SHBHq4TSCcwFnUIHa+oDZ6xJbJKJIjcU3bCyeLAmzzawZjfaAEnl5cSAQ48G08tZH2mKyFKAWj0Of7PZovfZVSqo0aelFBHLvVSPQEU3fVqHqmUoKlIn6eKpJ5+UumBStA3cDpiXA7bbLfZ37uDDs4TPdhRQneTOLJ4Vrc14+qkn8eALHsD/8Sl/Du/5v34HH2wHiXjTaDaChF2LN8UitdG8wKNZwYHz7Tl2t+/g9tO3UErBwjP+5PEPgltT0VhKQl29ehW3bt2CZUiblxnzMuPpp5/Cn/7Jn2B/mAFVQwggiZZf1CTFgWKaJmw3Wxwu7sC9ERQgFgcGKQW0UXACJG+CFGCdwv6g+u6cchLA8K/bPdJcdRVF+t2SzDf1PjloQdTz86uoteLSpUt44okPYzOpIa/SULJJbBEAofhaQYHnnva5qYTHC84i7CCZpUZRg1hfOcGPoe3Agl0aZFBdS4GjO57fM30mQGzGvlEVci+P+x6EwzoLF/FsAraurGsA03BgKUWnEEnyF5mYlEpsj4eBjy0w060B5lBuwM0JtNW3lBMLRpos8SJhSIBMOg5E9VBqXYdHqgdAxdUS4ZtZtOu9486dO7h8+TLqVGHz2s5jjixrpRZM2w0Oewl1Nq+EWidhylXqkBnb7txBXNRTxTLTRB6IJ598MnRwqhc2MVt8SEWVfHH7aWzOL2EzVdRCmIhEz07A/uI23vehP8H52RaPvvhRPP30k9hMFU899ZSydN2cakXrEyo18IYxbTYAay5ciJSxnw944UMvAjSH8gcffxxXr17F1atXhB0x4zDP+MAHPoAXvehF7p98OBzwoSc+jHlpmLZn6L2FSitVCamampM7Y5q2qHXC2blEnZlxVNRTcomFdLfWcH4upeaXBFpm2Nput5hVnM9z0vXPpbjrm41vqVVr/kkSnqZh3vt5Vk8Pneuqarh06ZKHPZdaMS/ivmcSHbGJ+7GxhecCEGlORxWAkw9nplD229V1kz2dp6+vPL+Z0tw36XAkRYNBlBmzJu63/s3fHa0d7kf3u1fHfQ/C+fAoIzKxJakE9Kjq/4m08ZpPvomOo7opro0/hQaWUtxQZ2oEsbrGZIpFkfJIrEQlZwjGMvJEMV6hM9BUCTZBsx7bN4E+6s5i4rFnNqtUnZ1XNZ60WUClLeJxMKmDPxD1zaqFBFsSn2mScjuFQLXg7NJlTIVw6VwWs7g6XcBcgC5fvoy63WCeF5xfuuTqjPPLlzV4hrDsL7AcDqhXGNwLLu7s0eYdzs/PMc8zbjz4IB74pBcCYNx48A7mwx69N8yHg4jOyo5mra8nfR4gWWrR6Dvxcrl0+Qpaa8IGJUcniESPfufOhfatLPyHH3mxqlwk17Krjlhd6DgSCbktQUEqF0k9OxejoSRPF2a72+3BLG3dbrbCUlvDpUuXUYpUxLAsY8xSlaOrftdmiwV0WEL57War6QEYzBu03rHb7TC3BQQpZ0UkCaamUrHf7wEAt+/cAXPH00/fRFtmVOIgFT3pVRW7jMDUUoGJffM2liubBaFWTmvBh2SlNpAZndfOaPeITUfmuCcEBgHYbDYOwhkX1vph+w4Qaal4atl7e9z3IDz4AAMQBYHohCP8N9iCGTaym5c5g5s6AIWOjB3DIKpagJk9BaYwGfXKYGCqUZetkPjJmqqje9tFDDXWJMxUS5FrshLTN8qbHe/gznRVH0GK15SYrl1joa3iFqb67Sri7jQRGAW8X8CTbgTdMrk1kGb+2m7P5N1bw9n5JVy+fBlXr1xBqQX7/Q6AWNyn7RZXa8V2u4UnhdluQbXgMC+4fuOKu0edn5/j/PwSqBa0zrhUJ0zTFqVWbKcJm+0Wly5dwfb8DJvNGf7clasAEW4+9RRu374tHgvLoik7ZzUEyWJdlqYGI2FbtVZstxvMhwOaivXbzQaHwx5L00Wb5pX4GzcHVah+OB8Gur0193sGBAQkU9rG72H3kUxwliNX9L+XL19Faw2bWkE1pLtlFlZ8fn4um2atkvdZw7QHv2ZmbM42DnySzpFxtt3isBdvD/P5rnXCnTt7HOZFbSjS9u12qzkuCg77vehslalYlWRbex7SDJl0nZtXVHZXy9RXROEddMqgbN8ZezVXMj3BjZN5/bfmNMhVPZvNxnMYGwEJcnXMeNfGunt53PcgDMB1n5Z9yVxWzHggagMNwKDutedkkqhgU4OdWlJ4IHbbGCTy+8tNdGLZZOLjAdUp4KK/2QFc0tJ8qAZ6rTU3NthEBFQvzXS0MeTfo6AkBsDnDty5fQeHw0EW50ZDX6cN0IH9YRYBUIG5lCrJ4JlR6oSrV65gmiY8cOMGbt+6hatXrwTjL8D5+Tk2mympLioKb3HjxgMAieVckrmQu7dZ8qPNZoui33V0AQg1UpVpAyoTrl6/gXlZsD07x52dhtSWCXWzxXkp2O920tZJfFhZ+3HaTpjU0LjMs6gBqKJMko+4asCC6Zat80QXKwnblyUYGhUBEUu7CADqX5fct0TvLhuQfJRDxKEisuy28tFmo3MrjWnvXRhqlfZJ1KEWIthSilQLDxlmxtIWByFm+LgYKFEpuLjYAbyI9MEVTOLtUqcJjTs2peCJDz8hmfI2ZDu6TXknI/o6kKotFJnRmlSEtlJW8eoSR2nvF5+P+m37XuZ0j6CetHCySk/Sh8rfh8PBi9JaeLvdezTO2ZqPTIHPx3H/g7DleDA1QdKj2q5ouzTMeqb7ZnGLGoZd0hhoBl8DYDCJyGo63C7MWwItYLIVqFTU2j2AA8o0uDVYTbdh4rEscBOTiYT5XLp0CTdv3sTgSK4TcW14MF1dyHnyP9ucpNS5eCx0bSOrjrdOQO2M65eu4IEbL8But8PZmaRClOQzrIl0Oh5+5OFILM6M80vnWrfs3EPLpe+qiqTqWmWMhkgqShTNrqVhp51Zgz+qmNpqRakTLl+5qucz9vOM27du48aNG7h24xqu3biGWzefFgPYdoveOva7HeblgGkjFY+LRlZtrl7DsjTsdhfY77sUiGgdu+VCGaqU9JEEO6QAIkzWFuoyR620zGwBgCzdIndIIMta7LZdl9WlIYysNm/zmHoIMTNqmo9WLWNkevDvt9iKkWxusplpngmbb9vNBleuXBa2vEjCo86Mxgsu9hfovePmzZvi+z5NoMKQIstSwsmkxOZzUvJs9EU3cR1XdytDzPO7AWJ+D/9O+8C8MwbQFUd3/V1ZDUUfeDrbBOomiVKdggCxhYEXED6hjnhOh5WjF/HLmC8CiPwwOd12W2WfTJ5ZSu6n8U0ZhOUbubyoFlk/tyUjCX9yuaTuyU4sq5rVMiu2o+fWcUQ6tdZQp4plnnHtoRfh4uIODlYnzpiSXm4pbk2vZnmQR4C2KKUQI01H1zqjdWGFD7zgBbh+7TqmacJDD78I07SJsvDQgBFlX862i4jShSKznOm9Ix1hTpcoBiDRWYouUvpnQmEGSJOcb89QSsXly5fx4Cd9Eh565BEc9gc8dfMmHnnkxWKkOhxw6/YtXL9+HaUU3G4dCy/Ynm3hvrgkqp1pu8XVq1fBDFy7dhV/8sHH0OaDGFQb0Ln55luIwj93iUTx1g/ZY8akF5+PRVzzjBEXcysDfMxZ9adkLmoW7axAEjYMFaFt3q6Aeu0FkYENTK4/bsvsgGXXbRTAt9sNzs8llwaXMNgSAduzLYg6eN5D9M2aB4NNr2vSXaQSba2hL00NnJH0h3oGRlMfxnvkd8k2E2O5+Xt7T5f6UgShnWdRjjldgaxlUU+65IP0zPb8cOH7HoTRGegCvuIJkdxonN4GwwWSLqqQWvXJF6uegAICqyoj9l+7WTyeNCS0d0sVyZrLoqCzFHGsRQpbGqCv3Yu6RnWZYYWZPfx5d+cC169f1/j9lYuO/c6M4PwxsYypGkAuy4LDfsHZmRpRpgnTpqI3xtmlc1y/fl1LzGxhwRnT+daf1S15PZv0wJAK6Ka/FBE0ADfe1Q2SukGaTto+JyJxkWOIIYkIm+0G129cx9VrV3Hn9m3cubPD9WvXcX5+js4srJuAp558CgQkqQMeRAKWjHHXrknwwX6/wzIfcPXSJbSpYn/YY3exaOhklL0hAsgi30yisDG3jZkIXGLMPAjHJCsGPKESwz1w2BT3ehQ16IlBzQzDBnTK1Djm73CwhLrrFd7KU4Adl4QawfKlXL58GYdlwaIlfywfMnrFYTYVnQn8ti5CxWb5OrjJptVUDWCVT7zSN9vmEZuZp+bMXk6U3qIQCh8b1jIhsHtaJefhOSBXMUrot62h8KbvPXTG9/q470GYioWxWvamYBGjz6RFfq12SCDAWZmNl0ABeV7SbCATsKl+L0IOtNCdmEn9YtPObIBUCGhZD1bAyTJ76dIlLG1B6wtuX9zBAzceACGc2X3ysvniynUh7Y/syN7R0lO2pWOq8p6XLl9FKQVXrl3DlStXvC9Nq7E0sdCz5lGAAooFjBDBjY7en8xYlll6UL/z5DtqV5G+g6txbDlYVixzOVvmGR98/IOYtEIEVQkVd9csBTYxgllCmi2uXbuKWsSwNB/2OKgxzXJEnG02wKbi0vkZ5vMzHOZZ3aWktBNzCkxhyVvRuuQyJps/HBWBQaqi6OLbbKwWUGOpDlJdid6y+QOg8HgxYBTJJxi9BW5kGYrsHjbXZND1PNusjXF3ByjbAG3bnqYqBkyVajaT3rtVNCowum6uavJPVrnEe2Ww9Go1SmpEYj2tnrD7dH8Oe6pSZgznDuugZ1XdSLhsHoeEwGkd6XNZqnfYhnevj/sfhEWdquyl+AC7Pm0wWozMzCeqi5ei3LeImwDXBDAYJwHZDrvaRk3krHVCayxGMBdriwRK2HmJFfhiLRWFJLeBqRPIVru22YCM1RvEQM/6xRiL3JjQlgZLusMsm8r5+RZXrl7zPA5A6DAXrefWWxtYt7QzG1VK+l1VF2Q5CwRUz87OMM+zV9slVms8YoyWRUrRL8si/bYsuH37Ds4vXUKtYmSZDwdht1WqLz918yncunkT5+fnWj7oHJcvX8Z8OOCw3+N8u8FUyH1G97s7EnoNYHt2ht0dCaLYuNHKQnubl3yShboBs/jxggjLQYI3pN0zLCc0m749S0urecE2fvat2hiiykVsMEQFtURGN5NujP3lw6LtTBqDqp67ev6YZ29BsG3PV82QPNitYNrIc7ebCTfnA3YrDm5raRz/YNy22Y8GbSM+wrTz2jQdt0lYhQ1Idbbb5pPWXVY7IK0leRUGVLcPBfDqfRPAzMxAAQorWfuEOuK5HcKyCqYqvpDS0cbkVkY1AzPY7if6IctcRllU1sN39UHOpEGE8gmkjGeYgFAxFKZrC0YdRoRxYkXYpfhJLi3yqhozk/mjARxFAZ4BA2J7/zAeCTBev34Dl69cdle1aSNGJ8kf3IFF7mleGqLfy/H72aBSBrCJQ0RkySanumQNmS4+FglQbDEZU2kdpQgT3dSKs+0G27MNuC24+eSHAUhipO32DNevXcMDN26Ipq8ATz99C9vNBn2e0eYDyvkZbj39tBi3asXVK1exqRV3bt/C7advo7UZg26yyRK+euUKpjqJW9+yYJ73qJtzd9Ha1orGWxADu/1eLPKtYeE5JUuH3zuPL2GUxk6JwUY+bTMt6mVg4FzS/CGC1qELQmFzvZQC6uqjzAy9EgGGoRqZakU5L9hMG2ymDXpruEViO2CWMZX5rUDHoUpQGh9uofrsxSMXT/mtxxrLLDfSm8Y6zucOxjztQOnynEoUmldFV75GxdnfHYzGDdRTUMmpqXwPjvsehIEYGAdZ7WxPV2cL3ZKHm+7OHPNT8ANhjEIKxhyDbrpPh/ZBtIwFLRMK2i7AJDlbQBbhZpmmbLPwupQAmAn73R6mHzNbDiMlrIYxdPstJq6nF4Qwos12g4ceeggXu52Kpuo14WodVV307t4QBo5MtgHJ/UTDsHbhC+ZHCkbcGQ0N0U0iavp1iQXpPur9uMwHPPWkVc1lXLp0Wf18JWnP7uIOXvjCF+HSZQloODs7w62nnhJWzMCtp59GrQXbaRI2rsa8eb+T5EtE6LohiRrlgN6BC2XIkpOa1cjFMA9kkOSVbssiLmBFysoXKp560Wr6WYdk8Bl15jLOtokPLFr70PJNkN9OAXQ13mt1xamMfUdwQ5KJrmhV6c20EVWLP5/db12eIWoNixKUucw+Rj4n07u666XrYzMJiTZa8InrhXnstwzi+VoBY9KNKqmDONpiBRYACXGPNaYE6nmC4Y8LEC4E17cKGh0DUWauI0OGi3eZOY6ML4dQWmkbO7er2KdLo1YsahwK8LcoM1N5SORel0YOrBVgTzxkrPzWrdv6pxSk9PdOBqF8BPOXiWXMrC0Nu90Bm+0Zpu05Dvu9VLBQ/W6d5L1ab1haC12mLSYcuxSt9evKcf17aQ+Dje2TTXZWH+2wlpuBz+47a90xIfgaPFD22Kg7mvn53rlzC4fDXnTJrQO9Yb/bYZn3AEHVGRVP33xKVCLLjLPtFpevXMYyH8SlbZ7R+oI6VRTtKwBYSKowG5sClHVCJR4LaGFRbUlhjSIbddrM15v6+jA1DiHYGpuRrozgbW3J9x4kMsA/z6wUEAIhTJbE4FUKpmnjYcpBBKS/l2UJZmkSWCEwWyWNkDJHKUyM5KDmar6CoqlYIwAms1xrs6W2XPfVsNG7VBobz9G7g7Q4guqVlb2DyH9nU/Poank+jvsehIVVySC7nyxk0uQBXtdTs+KO1VYYjsWe9BDfQYFgl6KOkgAD5pS7InERWQBqnNLFWTWSyQInuuu8MluBi+qw313sGtl6tJ2wjoGXU2wjkXLoS+u4cuUyplqxqKHK3I1ab2jmvG6Z2gbQGH2n86Lw3Bp5fKz9yqIJUDuRiMT57M4dlaohuQI0iZhcJ1QDi1Ik8c9+j93+gGWZsdlM2EyTpF+sFdvNhErnmOcDDrsL1W9LInJJJl/Rm+iYm/pBA2ZoY1fvMHfM3cTcUDnZj5ReEtAQQG5Dd1m/xzJn38zWfZT1p3kOuWEYcNUZhjkmzTPfZQkSodW95dzWmhQ0VTdFKlHSyMU1ayNYstGxtNq9IRzIemxEMFc1cf2ywrvi5yuumVKAAcASG9rQTyoNMcPLYRFFMEoitjpfguQIA09uZ3qvQsXZueXito2lTpE2FBCi8Hwc9z0IW8eTLlpz0STNbmUMOJfGkUOmVezm4xEgx4YaMP1cZhz5X+6s+ctFv7yOthMndpkcGw0dnpcFVt13CD9FYjLwJihrzKWJ1huHTigFX1YfYGPwkhtAvn/wkz4JN2/eROtNxeiG3WGPqnqyhRtYg0o86yIds2H7zNU/1oosVpsuxiWD6jX3BJNVqoBUbYYu6loEgKfNBtO08Wis3W6P/X6HvjSUIl4UBcBUCw77C9y5fZy1K4IbmiZ1tyRCtoGFxNS6lUhaPKVnd7YafqgekZWYWYYKJp2i+r33w+qITQ1QLfqRxBE63zAy25gLIZDNfaoTSi1qXGT3iNhupf822w1A4t3pJep15+tOZKS9lkjIs9W5tBgjK64FI1MVfXAb5ikzuxTB6b2HfuDYtExi8vtq/1Gai7HJp372J1AQLu6uBrRNp5aSKL5tQvf+uO9BWJcPABG1ivp7gtlj4T2RCoU+mBNjy6wkAyebOG66MDvfxBpEcIYVeAS0PtxqPI2l1BrXTNOkeQ2ieGMGbWurKsbi6SdAcA3cSBM4A+VuvxPwn2c8fetpXLl6RRz1WZLxnBdJ+E6LWOkbLYEZvuHAXY087Wd6VwbQ11hDwe24S6FPV+9oNq/1eLhEQFETrainxVQqrl27hjsXt7G/c4HeGLuLC3hhUPVuoNS+/X7vY9V7kxy13p8x1iYuL03PISTgEZBaRD+VVEE86EO9/boZG0ZYH3rh2NUxqKS860adqPWxaD6qhB5rmLIZUQ1oDocDLi4uNHubVucuqorgBGSmanDwk2e4ZDDMfqn/Bh93Y5KW5zptHj1yWjggp/dfu6jluQqkNLSpb3vK3RKbE9RgYv1bFbRNA6ESs+r3fTNp7OMLPjUiH/tx34OwZKpZRS2lr90CbTogA2cTjTnv7nGf2PkDsCXUV9yrUEw8Cr2o+EzmSRTfBSOQdllNOWvjWk/owN5Z/Csbo6wA+KgrjoA5gRlDwn73OzHAAZjnBRcXO0/gLqqSirItbqiyRc2tqw9seIKcKtPuyzLoir904um6r5gOcc3m88YYhVo3mw1qrbh86ZLmqtjgxgPXcef2HTTV7d6+dQvzfADNs9TMYwnquHL1Kh544AEUIlzcuYMPfvBxXOwvnMnm8REQThZzti9i7MwWEA33/6mort4jim7d3j7hyVrnGRvw6bE1qYhIQKxWybZWJwlFb23xNJTuCUCES5cuedKmzWYj0aDBRWNgFEht0+/mpoexjYC5voW/OGsofkgf+q/7748byJoJr/vB/uXorAGwByKO0UffViYVy7URPu5EKyN8LRoJ2Cyz6T0/7n8Q1iMDgYuHiIEc9G6ZWGI12Mip7/wMEV/kDrCUuSAKq7DrDQlIvskiJoqo6wujAKS+qqITZlDpkbzbk5AL6xBXIrhxC7rA7bn2bll3BpZE2tM0oaC4Oe9wmH0xllI89y1VAriGKL5i1u4pgbRBIcBG+s3c/HxUFDSSyDhwp2D34usZeuU6acXsKm00IK7K9i4udrhz5w6alo5qy4xF/ZA9o1yRdJGHecb+iSeUKXfcvnULzB2Lpq/0gAKLfhxAIIvMWQ1QAC3v7tgLNTyeYFXZ9JPLF+UN1VOxljo8c50HoSoAFy9RL1Fih6XhypWrHjnIYJxvtho4owbUEqoH3xABfy+L2OtdvGbyW2SJSAWB1E+amJ0x1EEMUFSy0brbHJD6eeirBLJ5TWdVggN1l02TgZAeSImSbowmBTMHWIP0c33XT0TMfaxHWigcM2MA1967l9UxFywgs7ETDFN3WdcrGRPVhepReibOHjG8EEdbY684IWHWRaLR3M9STRsd7rpWSVJgAhCL+wk9oXyVMq3BFpS1FTg732K/PwAEyTamfshrTxGrLlBUdWL9Zp0x1YK2SF5hC36whOcm4lkO194laxqrj2jOBOeeJJqDQ1il6S9i8TJLqLSoGKok6dd8F6DmaSaXFpU0mig6BWa0JI49e3+Qgp2Llj9aloPMg6RmwYqd5jmRxeRSkNwKMc4fpavmdw77Ls2ngRSk/rewb8CIAh+dl/+2o0LybLAgqFdUJiIUDdYopegmEapQ2zhE0usDoT8cZtHruprAXo9A6mdvB3ubpCxSQZHyTGY07yyqY52TtdRgqNHBiMcLBRkA3CW8vrpklL56l+x3gQE2Jj7IMrcIIA4bwCdA+Dkerg9CMMFaqpdt8UkrJwOq2DfnfRvYnO8AEJONLUhigItZy3k1cWJxGlgD8DLzRGMmLahObowC0qoWSwMKhvI4gIlvErzQ0/MHnRiOxVmbsOED29FmCT4QtoVhogtOlPiM4MwU2gYQMGlMa+8MqgW1TGha72uaJjz44CfhySc/jPPzS2iLFI68duUy9urJsCwLNpszDzAQ6UQjCdWBvi0MVO3bvqA18cnlafK+M8+EwsDc2D+3KD/WRW95g+f5gHk+aCa7yIZG+vKmnsrzYphnCPUWAeBeYNFisSkTPImUjoFMpeLsiyjdZwD54r7swvoyuDDgNRJHppnb2FKSKPN/L3VCNdVRmk/xByMby8zLYZ4l8MSCecVX2PYsdlWL6H57hH1rPmFWP3TPIcHyZFOVZfWfv4u2qftaHSUFO28t3XboOjVQ1ncCKUO2jcc7K0tncc7zcdz3IBzibURvmR7KBwvGviRyrUCSpvTU61RL1P001kIkei8JLIfrlE3kT8YTv04rHA9gLWgAdIaUAmUQMUrVycahasgsemC3Xsn3WHfm+mOG1gKTzyYVV7OurC0LdrsdHjC2pp/LOzscH+ns9IkoVQx3FladdeIGdrudpIY8HA5yj2LJ4At2O9EBP/LII7j59NPCzHsT1QSzsramHmMN3BjLLGy5AGgVQJ/gK6ZLxWVeZtEFM6NruHWpRdPMMXqTatdTIQ0tVoa6mk2DGL2SrgyzwOZmVtXIZ3XVTEUDeEY5hCrC9lU6un9K83my36FPzoCdIDO12TZqU3l0HiPzCJpn2sE33T69KwDNX9LiOWB/T1bxnZvYEiTUe1Hw5CTlqQqLTd9sOtp4s5CSIjtf3hS9viLgCbOyobTUqmuXY2wQoH9syJORNPafRvZ5Oe57ECaQmOKL/kUu1Wi/mj7IJia8v93dJW2RLu6smIMtCrdq+3WcHmh4m3ReNvOd2MaCrEVKDDFYqx03L+GTF1be9bN+0Nrqf6uob65U5oJzcXGhbZcw6DsaDUarhZ96dPzbGbOCi0FL0SAVLSlEzrgXzUMc1u47d24DEK+EaZrw1FNPCmio3repaNrVLa61BTQbiFdl7gU39fwIbU2Ly9zL1LuhaSS26ni83wGpXzca/4431FN97qwO4kkgwGBFIs1bZNzcDHRtUmYRmpMqLY/12paB1aiwTkIBOQNoDha8eo9h8z4x6uuDO2O3E28TaJpPezB5CHU21GpbOfqhq3qOkQDc2fQYdNSVSVshXrmvPZJ9nZldIiQ+3bzJVIQAma7eNyjdMLxCOacNKLX/eQLi+x6ELQGJGQPEwCRgIQMSk969ELQAwpGbEMvuf7Hb4dr1awI4EzT71JixzA0RgBsELOELXDSN+woDKarWkIUak45BpYo7WGJfR6oPHIdUZ5bDLCoCy0JlQGCqEUCi4S7u3IlNKUkJvnQJwehgH5FvIvaddYbodruLmZLgvbrLlOUPyJGDVvJcwC+xwiwYs/5AjJbMwLwAc2qXjaO5f4W43bHeTpRa+WIbIv1SX65ZaAZHIkLdbLQ/oofCu5Wwfqzxu1ArnI6ay8/Izz11jOCsftWcrlldO6gtesiAzkkSOWEV5Zd5Htg/23fKjm3tWeJ16kEwqBAqChos2x6Bid1/mUr1jdEkKb8YHX1IpsPehjUpsettw5f1rQCeN54E3mDVAduGqNIo7tLXH+tx34NwLEYTuTUqR6iKAqIATjMrKhuDA2xRWDVjlIr9POOa7+px9PSs0EWTqyqMGTPglnnbiW2WW80tgz2rP9dJKiUclhkDnU8TDzhmNv4vOLUNzv6hz4HNMzBu3bolGwPDq4QQQpzWB7kXiXeDSxa20QVog1MBUGSPieKVKYT5avkhDcwwRmMA5aXHvesN5JKelZMhFHkx5tEaIySlL7o2PwNYbC75/PG67A9ejnST8fBjAJb2Dn/FJuPdd+KiVVvWaqhoY7z3sQrjxPn5IgpPkDUJtPJQST+QGIhFWEr2QgdByHNa8jYQI57cphYvJuZBJKNOnBSQa8pRvfLSObGxjJ/FfTh9t7ajwBkyDzr85+O470G4pAXjuyRHiKWpHFyPBI6EPaQ6wULDgJ2fX8IyN0xTUXaZHMqtmi8LypOJM0M7RlbjouXAl8RoZyxO8L+CVCwzWMz5B/KxNsjZYgmRbBRjxXAh1uqnn346DCLR9Hy6fxHbR2rDalEPLJk4nS3nCGiFxd/6RyLGR5HbAYPi7rlppASUyffY+P6E6J7VAFix+7zZ8dAPx9dnY90aOMbOc0RU1k3DhmhSRvTyWu1wfN81AK319OtjrdbIYEUm/Wn7PK8KCOCWJCp1UVvF8oYu1lsf6jdvD2subV0zRF4bMfJ+J4uMqxbGN/J5nRn+qn/Wa4PIovWWYX3k63OeDc/ZUcgDUO71cd+DMCjUDDasvbGDc0eTSa8srWC1o64Gk0hCintvaC0WibFpELkTuLBenSUs1lln20cLdSzGSDoxoaK4vwMF8JhKRR6mBhIe4DDexWXKzAb0EtilIkrevn07+V2ary8dqWecs1HACxlqO8CsgE3DPwOitb6eiyd8pAZw9caq39YA7GBmQsKpfvAhXbMbW83yO2lT8/XHd80bzDMfRCFtJI2s3DE3xDqL7EnHTPDUcepjUf+cet/TwEypLfZvNUByd7Fg/ouWRRpBNp7QdC4vmh3N7hsFZlO4sxnUOnttusE4CU6Z2PogPaw3JPv3pLqFkHS/cZ6r59YbUto8PpE74jkeRUMVBdRkkXk1DI1us90/M2I7bHeWRdRFrNZYcytzw6tJnVUARFDVg4lkds4qy1WVpOqC35oOUsVqmwyWAMhQLwAY8JwRgLsDOX0oFKWZVoeFsGqrQJA0ja0tALYnlqu/pQNKiLCJ5Wdkso1BGSy5r5BKCyeANe4T7XJA8g2ER3DU8Y12xQYilx2zpWNxnQftg6s5BhYccypecXyOfZb/FgZoYJfe+IiJWqDAOA9PGdGsrwJYpW3xTpFQXa7VJFKUnsfpUalDlBciZ+aTbu7iDmhpTGEMVuwZJm2BoOXdJKDI5rB5Rchz5eGNrbgtuyshc9M5Je57/l7cQVRTe3jon3zk7zLTtg031JTj2EU/Zx38JyprPKfDHcwNdEPCdPZ2avdci4A+EOqvGsaOYF4tWcmlhPriAG0VYTPbOFrM2jirsGtGAjgYpXZZjgu/flycVvyQlYH74gTShBxZj7Xjzp3bOBwOuHTpUmK0d+kTvbeBJUD+bHsqVgsDA4CuAdhGBV7bT9oYjA6u1rD3ts0w2uRcc7WoTi3S/HtWWZCJzgA8YkGbx9aoeMthsQ/PN0DSce08vvXQn6k31208BuLYHQxMxe98veHYu2OcK8SoVTKFzfMhberJCFkIPGdDloLmsrgBjjWgRdzVYib0zppDpGjQi/mVM1CK6ou1L3uArAFzZt7MEcbfe5fAphz6vNrU8pwefY21OrMOoteg02hUe27+cXBfz+N7dNz3IAyI4l+T0w4LxaJzcmfb4V4FnuzEVBrwwomx+GKQbfCXpZ2cHLVW+c4nrO7OHJ4Ai04I8W2WmmKtRxFNbysHR7Rnm07OXKoi5FZDk1sb2jP8q/eY5wP2+71OdmEcriw4yTiEAUWP4AjshhEZqukONHP4m4Ejo9XR7xxrw9ivbDZ8BGZ3b1A8dmC35rdg/4s9J3zGh3aM7Sv2EkCAzYp1G+ic6q9hw7M3UylOzZv6aAOcvFmtRXG5vtSqRrGGWiecn52BiLzkkMzRyf2bPWDGdh69rQTVNPXpjSK2VtCgYwROUyf4xsDs1zJB7S/Gyi2XQ3R0gLO8Sa7GYTYCm5cZdE/2LQMxo2NMU+dZxw0zqD9PySPuexAu0N0cXWPXpaOdCVLogvLuOf4OwGrJdAxWW9GVaiXeFtV013J8TJBw/QKAxdxzgJikPO7IUoGXVDQjWPFERtPnpZBNFusxJ71atiDbsZ6coquT92htUR9Q1QtnkErnA6fnLpBJ4ikoHDexdT+ZGChnjlJDasGgNjCBxJk3r9ng+v7RZhc7V+0JtYm2JG00WU8twGgvbUakcXMJOcT6rPv3a5DIapTYcKOTKbVJouNym6OFJSejIcZms8X2TCIRdxe3sd1scTjM4lddKlrrOD8/AxDZ0QSUJRzdU/Uwa/IjNUIrOyYFyNH9Mli0uAUGOJMy7tastJJaGbx6cldG3QHSLZHIg63iTc32As84+EySXu5psXsAYMn7DYamLBBPnLxZrtf0vTruexA23DUXGSAAMfepDdQ6Y5nMHys+aWuBVtepLy0pA2OAkcqw6KSLnAjGBjkt5mNROBgn6/fGCMIBP7MeK/oIBppv7/J9v4tp1xY4IRIbtbbg4s4de0HAJrnhiuGB33IEE+u+teriWNXDR9ePmwOfkAAtptHMgpzAV4HuiJWObYiN9bidw4JdvZ+NF6zlBIlytIGxM43spfl2fIx9c/ztqr9gG42yQ46+yVuDaeoFcM/BnXFxIWM5TZOnraQyAdC0pGmOMfMAwKUUld5mfwpDqmWb/Zl6uNUVJRpGKqz9ZofhHqxZgodUrdA5xg4xN2x9hO567BPvbjZ9Lw1SpqwRWycnvEFSO8HQAgyyDnpv6vuvLpUfpWD1bI+PAxC2LP7jYbpSZxyq6j1Vc8sBhE6zK9l1yQFYRrzLPdmWBRy8eHhGTLDuu20ssGCoZphrCsY2GW3iZfce+MLKdfPgn8eRxX1bBCBoJrGkF9Mv8v/zlXDQuLtYHYu9DNedHJvMcJ3dHQOojw8lLmuIZc8/cX+khX6qrXHl6DaoJ8W4plwRODE34nm5LckX+BkOb4vNm3Qv1Vw5MNv8A0mWte3ZOXa7A87Otu67vLSGtizom40TArtfTrGaf8QL4jC8iwP1al0Nm6gnnoqB5J7IgDLkjq6qD4vuLEE2WmRH4wZvj/VLJjaSo9sYQmwo3o/DpjqMzrG0SOaNFNkAReJ9flD4vgdhGQ9hD6MuU62uqo91fVPI2563Aat1aRE+clpizarL7dxFx6Vg0BHpCdGjQqxZiH12KJNjhmg/0iTyYpfITBPpevuRDcDOyxOxZX1w5hsczxcmoiDcIwE4yBinTOgjGD5isAFyI5BmNkIDFAb4Gmjn/rWFRcPnd9P72kZk4q+P+sDWTwPx+jAb4LAh67N5APxnNvzJLh4NfCa9+WDkS3OSXEfKMW1MArD7lurJ0iVCsWPabDAfDjFuFH0s/Tlm4BMXzOYuZKMqC5gPs5+X22zGtPBNUTB3TwryAqcWipyj2djAVMmJqyusBxUwPRF+t4rJytE52pLXu+fv6Gk+OGGKTceGSPp3XJfrpPz36jiVHOAZj3e+8534mq/5Gjz66KMgIvz8z//88P03fdM3+cvbz6tf/erhnCeeeALf8A3fgOvXr+OBBx7A61//eonSSsfv/u7v4su+7Mtwfn6Ol7zkJXjLW97y7N8O8MVjR54oURs3dMIGXMY6GHx0ne205m5j35sIS9BQ1B73cdccZDatEKI6Ew8TLgwUAfK8qx/7MsoLeruAY2DPE8y6hIDwmlZmpTHXpt976qkn5Tu71p/B4wQ9OjILGa8RZtEVSHrqy3Q1r++R70sn3oVCHbF6V/vcz6E408An9+dpUGQHznzewFIRv6+PWAdpg/fP7w7Ew+faX6R6UZcjjvpH+IbknyZPXSm11kQ/XA2UNCzcSjpJOtCGed5rRenw7UV6ngGYhZWfYs+5WS6hKRHt3DQDIVwKtZqMzKw17pon+Gma/MdWrOmHe++iOiD5V9of457ba5uJgH9L7ZG/h6T9ULVH13qKuhm1lFnvXh/PGoRv376Nz//8z8dP/MRP3PWcV7/61fhf/+t/+c8//+f/fPj+G77hG/D7v//7ePvb345f+IVfwDvf+U58y7d8i39/8+ZNvPKVr8Snfuqn4t3vfjd+8Ad/EN///d+Pf/pP/+mzbS6M8DAwxo/73913ZDXTAhziGTAuurW6IuuQhZwI81U1FxIH07bcZdFBBr4AVgwEROHlACASrzgg+xsiwGXtuRDsJsC7u/+0xYxaKUZ1osKtp59GWyzl4LiJgU8L0+MCtA3N2pg3jny+AbJdEwwPGV6H6hrpWjt/1SDjuGGdt/c43pCznvH4iDYMDRgb84ysdmTIsQFlScWeldVQIQ5HK4IWjEDtb8khXluF6VqqhuIX9Y6QYp7MDZvNhHk+aHskj0fui1qTH/fATiPiTOwb0RaZ+9KvhcTTGNy9nBQQiaWMoFgfFavrBiTiIuMm7Zfw91I1yb6dq+3n1K+Z/GR1XMz9aKd9Z/1oDH2ZpdjrciI68F4dz1od8ZrXvAavec1rnvGcs7MzPPLIIye/+4M/+AP80i/9En77t38bX/RFXwQA+PEf/3F81Vd9FX7oh34Ijz76KN72trfhcDjgp3/6p7HdbvE5n/M5eM973oMf/uEfHsD6ozncCAD1HAAQSzSJ1SylTGQCA0WdxkEB3gZwrY1MME9SB1sE4yWK0AH3PnWQLTFJTBwyIE3r03x1CRG51vLkGSbIihFSFuFH3Z9fob+aO+zF7g52ux22Z2f6fQCp3NM0pesjzhtxif27NTBntYAthHxnSy1oqohg/Qr6pwDQNlCMG9Cp45QBMW9Y0cbcppV/94ljUCV4H9i9TA0wfp83Md/CVxvX0HbA/Y5FG0bYbjYCHsuMzbQBwFjajO12i94X7Hc7MDfsdhfIEgeld6nrxP0EoMvnvTcHa1KyYfIfs0SgenUMvadE3jU9NzaK9Ty0O8UaKprLl10okSEfGfgoORijjfsauJsR1dlvOtf+BoDNZoNlYbRZNxvq6CezCn7sx/Ny11//9V/HQw89hM/4jM/At33bt+FDH/qQf/eud70LDzzwgAMwALziFa9AKQW/+Zu/6ed8+Zd/ObbbrZ/zqle9Cu9973vx4Q9/+OQz9/s9bt68OfwAmQ2NopuJVaa0J4SOCghWmyepiTanxLRB5CVoVWVlLomxBTDCWfgAlqpn9Ooresi9uibUkUkRInYwlDWgGPOQJlikUjwPSOKbvTgY8+GA3cWdFVu0TQJGLU+ORe6vYMdj/1sfxN95jIItSoDryJR9bad3SLfNDRneM/dL/veI6ae2P9OxZtD5fnFt9nCx59u784kfrH5fHat34fR/23CgyZ7Oz87R2gzuCw6HHfb7O1KBui1yTZoLVhrKavUdqWjS5rA0YYcAoSPaYxs1M7uWiVk8ImAsmO25xwC87m9hvcXHUUL++zAncwWY9VjYjxeB1ef72geFNmy1biYtDkBk6VTrkUB0r457DsKvfvWr8bM/+7P41V/9VfzDf/gP8Y53vAOvec1rXLn+2GOP4aGHHhqukWoLD+Kxxx7zcx5++OHhHPvbzlkfb37zm3Hjxg3/eclLXgLAJmXKxK9atWCkOin0PoP4YjumZnwSNZkiJI4njX+WxKFai5dNit37mLXatTaBmROIklwTCzeYlE+m1SSspYhe2jegWGzro2eROukM79y+Hfo0WzBpI3kmoDqlrzz+fX0o3UKA/mq7u8v97/L8lSL2mdUOJ1rjOuTjPnsmPXI2/srz8s9aRx9jE5tVYuK4e48N0g7gCY/mww5AQ+8z9vsLz+2wLAvQmxqNY86UUrDdbj1t6qnCskYmWutaJsrCn0+vBTEmp/5IzNckM6jx2oxsMd9Tes/0Y+5i63k3SHY8toU5qTWYteSVlWmyLUzvCWHeU52w3+8BEKZpi1qlMGqh54cJ33PviNe97nX+++d+7ufi8z7v8/Dn//yfx6//+q/j5S9/+b1+nB/f+73fi+/8zu/0v2/evImXvOQlvuOWWiX9ngEOJIAjR6pZYhBfHgyYLrJQAQrHwOlkhYJ77x3ZyT4S1RCI1kl7CMFIR/ZkjLYCYtijCCH1dIkEqb+RsqnBnm16Rb217Paj14G12yceMIjvRPKsi4s7nm7TGOgac/JkXrPwU0dW65jXilxOILob3PiT/Hfiuz8jP3/NjPL3Hy3TXasi5LvxfXjVx/J7H661wIp8zfBO6T52VcwQa4l8ObYtARIYbZlx0RYHfhjl0D25UJESVBAJkkgqqrSl+W7BaV47ASCAtVJJW5rP4ew2ZhsJFQK6eaeITGPt8Yoa3ja4NKoPEWDsK8AlguSM6O62uQZjD3Ry3TAcPNnUKpwkTu5DpZntdiv1ESFExvK1aLjXXefKx3I87y5qn/Zpn4YXvvCF+MM//EO8/OUvxyOPPIIPfvCDwznLsuCJJ55wPfIjjzyCxx9/fDjH/r6brvns7Axnqr8cDtL8EUswQYYMSKklJpEy49a7Z3QCAmwHTqbz3uvEAYFOaW34xAfAjcXbYdD70aCOsOfZI4otRUsnyKzucYzCBFBBJwNCC+CQyW15W6gTUv4TO1kWvLL9QYyGTDlnwr2hcgez+E1KLgBpXTC3sJKHju0YsPPhGxAn397EfmwRpY/H600qkN3t6JSsJzwF1qf0xKd1xxnk7Ds++ncAUJsKjNW1H3mjyQCcUs2kpx8zwGGDItl0TT/s5/XIwytWfsvrLOeYCmLRunEEaDWT7uBrBt35sFdvhJBaRjUbw4wLVu4eiFwgHfo5NM+F7zRhTLN/Pfx+IE8kqSWb2QSShMOsFaETMdDu6ZqqtVBsymKPjw3ncDg4ox4iY7US9PNxPD/8Oh0f+MAH8KEPfQgvfvGLAQAve9nL8OSTT+Ld7363n/Nrv/Zr6L3jS77kS/ycd77znV6ZFwDe/va34zM+4zPwghe84Fk933wSgbQYdeAYkV/XDXiUnPLtHibmdAOOnJc3QkMHcVfxwQsWog9ikelnj5glx0RMHwKwkucCzoXEVYl7N8WKZoSDs8uRVZl+LD0wscMQ+cjSbEjZo7SI4v3s9xAbdHtJG8yJPsljkK5a/8SGl38fD8Ng3wRPHB+t2sHveUJ1cQzA+XfpNckQZtcfGzxHMI7PT7YhPdXfjSNiTn4C/DL7lt8YVCq2mzNspg1KKTg7Pwdp5FvruchmjMk8z1Ipg4pXOQlvg3inWgsO+z2aunr1Lpn/JMeJkgCMc0VUE6YETEeHVDM28G1GUuKnZaO1be66eUsgUpZSRinBVX8kZbGsQWbrMQKSV/zgSYHYAORez8AqPobjWYPwrVu38J73vAfvec97AADve9/78J73vAfvf//7cevWLXzXd30XfuM3fgP//b//d/zqr/4qvvZrvxaf/umfjle96lUAgM/6rM/Cq1/9anzzN38zfuu3fgv/8T/+R7zxjW/E6173Ojz66KMAgK//+q/HdrvF61//evz+7/8+/sW/+Bf40R/90UHd8FG/YKmu67JduRTbpUPMIiKQKfnzpDYdafIHZrZInbCo2oT1hQgTt9gXlN5RPyN4scfBiJAisGATWPw7C4mV2diGP1sZlIla1k4BVchkkx1Hd4ZRhDTWXVR3bQB7+/YtLc7YVxP7hEGF2Sfzsd777ofpCx2sGFiD5ykd9BrEnglwP5Lu+tT3Wby2r07dYwTuU+caMBnAAMeqjVXf5htCNvrxXkh/x7/27bTZoG42zm57b9iokdv8ezebrT/PPBUAcfuSclPLyc1tv9/jcBDvCirmJgYXtSLIo3n6VgOx1nr48hr4WcVohhrZqn8flbxlrdh6yoEkOcmQrG9h/fZZ7l/xvEl9xX2lbjO2No4tYUxDe6+PZ62O+J3f+R185Vd+pf9twPiN3/iN+Mmf/En87u/+Ln7mZ34GTz75JB599FG88pWvxN//+39/UBW87W1vwxvf+Ea8/OUvRykFr33ta/FjP/Zj/v2NGzfwK7/yK3jDG96AL/zCL8QLX/hCvOlNb3rW7mnAuMhMbcAMFGgRTVbxUdkrdKCGgVsNpn1u5xatCQeEX2oRbUEw0hCyvR6d7+w0qiUSTDtbJWYgMQIpuSL+ksyW4Ykl9NrqbxElF70QB4lG/0h/N20T6aYSeYUDtKM/TNS9OziF7nN9Hh2d6+04MdWt/daHpyT6j6SD1j8GHcndwN76wN4vs9vxFsKO5N3yZ+s2kbOoUBHIueNn0U5Tj9k97fv8nNxmZtYq3tJT82Gv5YKAZV5w6dIlXFzs/I5ROUVIyebsDIfDXv1/u8/j3KsCeAsO8wGVwu2xWby9z5/or+Jqg/AgsvBlJpMmLeQaGPXLOcG/Sq7JqObzq5u/Mafxkn+jOMGxsbEzD14c+chqHvOu+jNTWeMrvuIrnpF1/PIv//JHvMeDDz6In/u5n3vGcz7v8z4P//7f//tn27yThyfSMdncJozqowxUTgFKNjxY6staq0+E3jvYDWfGkEPXlDcANc+5CBmMVtrkA08pRwJzOt8mpzmeA9RkchYqIO6R05gtb0YYDPUuEB8R1k0EA63Mgu1uv8M8zzjvDNRoawaQ3F3MapBxthdMzRab/R2fx0ZkVxwZ1TIEB/4fn3eXv+9+5DbmDXcUPUVikQf7eFnpK+8Da5huFIPkM/ZRvi/QT7a3g7Fu/ZH+N7+Jvbv+bpFw7XAAWBgw98XbtiyzJOYpYpRbEqPUGw7AaSPAvWM+iJqQTSdskWQMkJaelzkJeQst8spEQ5CH2ANYiw7EWlu9tUoH0f+mWgv2KuvZDdgsG8F6c8sbsr8bIaXYjPk9qttiLT4fx32fOwJYdX4Pxb0xQvMoiNptJ1gvElAk9pHLwhs7hgJgScuoJwrFDPVHPnbbEeas9wN7m4zJNm23MWwAQ14Jyj+csqauAckww1jmanIygN1+LykLB6f6JDGkDWK4r98nvcvq+adcm5yM2znW7tXcD4Z9vGGObH11vr+ybYan2E/eKMj/XjNg8s9Osft122L+ZDXFGlSHawiJ8fPq3/FwqQO6MZMkdJrnPcBSsZu4A2z2A3UzUw+KeZl9oyuFQLVILcNmIrhZHaTN8+GgRir51OZ555THwTYrWyvm3+tt1pfkzO7zvOgx/iSGYlkDuf6bnRvGXZtDlqP9bgBsaplKI0OW6ZZdBMMm8nzpI+57EB71iNaZ0vFmJfWqx4MfYGJ6uh5dDKLwp3TRUXfUnCgoAyXn/xmorYDJLMa5BQyo8S0WWtiIAxh6D9UK6XNcoE1AaeBIhiL2kKJFbExlwUBbFhyUSfnuwAyUyF+8BuKY7Jk1298ZjI7HKbJUBRAeq0woSS9xnAb12JiOj2BW+fpgxzEXRnZ2vBLjHutv7F7js6I/AoSz+sYeI9t0AP76/dbPB+BJn5b5APQmEg8E1FjHr81SG471/JJsEyCJduucS23GPVrvGuYMf4e8uThzzvOfbR2O3kRR5ogcoLP60F3QbN6nAA/WDjK1XWbwBLVvWFeuCBUzS/AFggXbxhWG3pDQ5FRCp/9NXdT+nz4sBjxH1ZgOdkkJ1kemo2wVMYGcBRM8bNgmTSU3SQ1rNO/YWS1BpiBY7c55zdpEUK1BMAhjGX181rDjywcJ6LKOW5lDEr1FLaHAoGBJkOogu4udi4J+f2WMxhiGBUangM10pxmMnJf6ud4F6qxtLcpMz5IiOUtKfT0+0/orvqdMzZH7x37P7Yl5gOEeIyO2hZqfkTeidPXQprXue00UbPOyjf/4GO8/ALyyXp+OzOoPHNeVQiA1KJj4zcxayn5x1ZtcpJIYIARCda89ec3Y8zs3VQcYXZBnmuqFEC2Xc4r3q7hMAsyaWMtIixMNVXuQeFpIhfCYtwRLcAwUc6GkMYXrerO2FKhAhGoL7psu2TwwTm+A9+K470G4YEyyYwBs1lkG+yTPTNQ5lIt3Ir6UBAikyB0+jLHTMmueCkrACFKieWxcyoYvguWFYM21KgUP/Z1I9GvDOnQ8NUAZ1QFerdkTE5WY7NCFlPGLZCJe3LmDbtZnO9f+7+RjnKTHrHTdUByBVDCZ+Jh9eRyzWYfwzB7jZo6U62XDw2/Hdz8FpOP97+5pMRgY/fqRDRMADNGPcW1ukQFL7ML53LsBglwt9eKiiEH+vpjO3iS2IqK+R0ZCDMdVQ30lMk5K3RshkbJEbbVRSb905pQXGwK+3ACGp6XMlV+CqMR7m11B1g25Ycz61+ZwUWd4gV9SNQR7zhePCeDRU6Ikv3sHZx0DwwdZq5TW0ydA+DkfMmnC4irYkTrfFp19p+KZ51clA7YSfxub0+tKIdGROcD7o2IysupnOycf4yjHEjrrrs+0yW1Gje7i2Vr0Y9aJ6240auDIDUjgYaxvrO7BFjAMw8feGRda5kj0iSWY8AA4ayniBDA6xKyQ/ogRxrlD7tyQDYbn+J20v0LldMyMx00Bq9/XrDV7dxgrxAowjtuxfmbcN7FsZdC+oa9A3ySCuIJP3Dv6JlievHZvLSCfksGqd9Q6wXKWEETllD1zSqnYTBOmOoEh+uK2zCgAGoC2iC7ZGwkSLV4vAHVQKSDPRSVqOipF7RaxHlsqAmpHqVLZubWWyJACMMLAbq5rXvEmCv5F/nAD91JAJ9QIrn7R++SxNJIiHxDCEL6e0/fmuO9B2MDHxeTEIgRQRgYngMwOuJb1qZTkXuMishrXlqZSoEOgAmQ5crmJwIco+T2I8PIlXO+lfxPU57fQ4CozqDvsGQ4KI/tHandmMe45YUhjv5OAsHMybYeJqs80KY91hWsAOwHA6X2R2jJ8l9hxvKFVMD4O3MhjO6oJ4kwX/ZVxyXnFm2LPp/Qeds9T7x3PA0xgcaYIK850ou/SHPDnHZ91BPp5c6m1hsrMgTUiQ624pvjstiQpVUzT5J4/poutmjOh6WdLk9pyVKtrzTsAKuzGaNZ6jJ3II7dZyQyVgqpoafhrzLcrky6lyEZC0U/W/tgQw5/f571ScDfteBkoWde5/lyt1dcZ8vrk2BhkvLoPApW7z/eP5fg4AOFxELPIYaw3T1gbFNIVR0ldEcAxLlhgVEPUScFc9bZD+COOgWGtrxzASK9y/sPNM72JAQJetcDOcnELsYiN9crv9pm+n01CGhc9Qarqms6XuWjBUbtzBth0XZrUo5HuFPM7NWQmtq8otvdBBsFs8Dt9u7Et/om/ZWwO2cg4AvVwlX596l3yM5ydg3H8ZB4AJr18+v9HFn/Xqp+6maJ8llaOsYTm61zYpo6Y6iRJamBtdUUIQOSBSQBr1W+JvmN1+5yIPN+CMOMCZnWTBINKFS+ERcKlOxpK72H4Y/LNwRi05T02iaFxlhgjnasPR4nySCAoEPPROw99Vwq4NwfevPn6xjUQt0+A8HM6HFwBGJVhlnDIoqzXMqwxjIFphydWacU7fXoOYyL3XJYFm83Gw4utaqu1Y1iISQxyIEZyVwO5WsRaZDoyIkhFATVUUAFKL6pSYHAzRgJnsaJbTju/LXzXe7EbO+RjacN+f5BNi9nd3RLMjH+tAOEjge3Rdxy/uOdIAigRb2WMfDNLTegGpv5qp9sWv8NFVQNyW2xxqW229r727ykVQbzzqfPsN6+NtmK08vx+1/46dYwAnvIuaDIq1+fn8QahThPqtIFU6gYyoRilqfFY5lkDhYobxggVVHT+dstdQpLApyQXL8D1rbZm3AAIm/OyPU21ovdFKmtY4qqSVQT2OjYBgsHWEjUZC5lOeQ3CtgnJWiXSrINOsnrcC0Cnu/fJx3p83IAw0i43iB8w0a3797bAzafYMBdgNTqHl4NNYBNTp2lCKVYyPHRJmTVZTmLXSxnw6UbgyXb0U/YdPVj5KXG/K/MmIrTedMM5BXbsKpehbaRWZcDkR1xcXAx+wvG8zG5Ps9BnZLur8TkyTOkfGcYY8X6M3G/5e3Kd9XozGJ8Z3hoCQBn8Qmdv/x6/Rzzf5sDxu8fnvolan6/vdBIo1vdbvYdcEPOVGYvWkYsWkrLZAqpF52fVlxvbbgzYrh51odKu3kR67IgIzFIqJmYsUDsDqYGuANQJIPE5JlRUK6ZLACjcEk21B7sv4JXShQ0XVVVUl2RNP5w3j1yZQzYCoRYABjB3+wbJJiLna68xAFRvExGwmaZPgPBzPWwQBessIGJMiA7EQnUQJAJIgRjmJA4EM2KwucRAdlKJUupYmtTGyozK9IFw8UgnWvbUYAZ3db1ZrUVpt3pXcHh8mNO5sXjStrAxmxMMUIJVFOf0WjYxXF7bkW2/36MtyygVuOh9zHrH74+ffepv+wzK3s1gcgqI4/poY16EpNLO+t7Hzw8AeqZ22litGaoxyJGR2bnZ7SuaswZRB9Dh+pHdjnvnCAJDvxio+mdF1ABqaBPgjc3AIJdNzNdwZ3mh+IlEP3LX3huoVPFGsPcpBdAMbUQEah2dIXXiwKggdGJ0CldRppSFkExnaxtSqAqlzcbwox+PIutI8pIPel1moAB9VlZbayI+xautF0S6ylgzPbwroNWlPwHCz/FIiyNEDSTKJUdmlAy414RFthmotd5UJaAuMc4o5RnzvCB0SOOEiZ3XnpnKd2s2tJF9yGK2kuDiKZHCQYGBCZTCGkcPcbc7weCICrgTiikeCCAuCeQoOgHi7rTMc7BRTjrsdRevjruB7wgy+WLlt2zDY54JlJu0Gi9D4hH01/deL6BRLcEnP19fe7zh2Oc2VjbHQn2RN6rxWnvRVX9YPxgp+CjWfYOMaykFtVTR75YKolQWdGgH66aas4UxbL7lZ4Yrl801pChRkTpqqcCmAIsEeVAtGgRCKK2joctnZFInwIu1JKQb89PtfcwhUeuEUhhD1WVNBG/2jyzFDn3HnEo1GTlQJixKaxApu26Ls34fp9XYPx/H/Q/CEgaGWBjqambGHGOgdhp0aSXRkaD63a4R/WQTl7yUva8rn+CjfjKLpc4qoO5eCUDIxLU+glOeRKd0XP69BjqUUiKZkP5uujkRCsgBL7NBub89ShjAbr/DVRX/MnAFYMWLrvXAdwWgE8fA3CFbX6ho9BkkBh8zIpquXTZOwA2QJ3aGuzHdU6C7ZvaDKiFJRPJZgJsl0BnBOH2yfn8ebQU2fh9dPxFAFZtpi2naqGcEObi7uiGDOSevG4f8NC98Y2F/T2PLAHDQoqC1TJKelTtYM1VZ3YOmz6BCqCCvTGEg2npHnTYAiY8yKPq4c0cpFa1JJrdpmoaxkHEomCZSY6MUbLDvcsIeogqw5f2QqhqSTzhUGnWq4aJnGbcA+R1NQLpIvME6wvVeHfc/CCvSsIlq3AZ26kwUiMkLPVcnjaV47Aq+pw0xBq8hYoUhIQxuxqhNFOqQiQtthYHKAIp9BDXTj/Vs2OsmxtlrECoRWo98rU5zSMXPQuq/WWNhsgtmAMSt586dC9HHKfCuIxDt/U+BbIh/8HPvxobXaoHB0HbCZ3g48iZoY/kM7TjVllMbx/rvU+A4bOS5MeuW+hw48Z21Pt3+lC472giUOmF7fgW1TrDIM9gGYUCe2G/Igic2w1TYNp4fIdsyl2U+CNMWiYkbxXyrVZ7SWdYOyabYeldVgIr386xudBVUVfesyeYLqob/S+5iU7cREaZp40UXigO4EYMYQ2uLfSog25zxGkhn9YQTDJt7XRg8iuqoC4GxHI3YvTjufxAGQuc2LDoTWbrXuOITIAJmyYbm3Ex1TuslZBOBx1wDMqAGIOY6pGkB1V+XQZ6c3fShw5K+i0jk4AFzSK8ACcDH7h4AZ6DvbJ5sAUV/GLvXpQciwq3bt1dJfBKAIRhpfudTYPtMov7RmLloYZtbBmggh/c5qH2URCWz2mj03cH2I7Fn1v8GfclKurDNVR81bPjjW4ztXP/OzGAqoDLh/NI1lDphTDa+6m/YXFoBfGo7vD2cbmFuY/Bx7z1qy0nGQDH2WYZCkzCpAlTFsNV7BzVCIwK0Ph2VIkoS3bjE1lFVwjTfXGGxpVT3by5FKi+XWtGWJgxYJToUQlsaqLCXMyoWhowObkVzQDByXpDMnm1cY5OsKDXIE/WPcoI9y+O+B2GbQL54HXxFrOk9QhoHMGE4wLjxrJCUKWIGnxyPsNbafSJJid5U7+kqDV8YCdwKDdUsToNv0humRN+UAVZOHu6/Vidk0bSQ6Mm4i5eEKQTu3BmrLrOpCYx5rZhIBts16K6PNWCPOt7hpeP9OE4YTwkVySl1wnEfRhtOYODR8ZEY8anzj97exV0k4D7tRRK6zcyxRQVxfukaUCffUJMgcywFwIAlGVfz+8MFRviWoiqIvPl2zaxGFBSEqrqTmSeBqmNEkikohQE0dF4gtkECoaFpOyUntrmRiWFbHIGaE55wa0t/6/oxxkukubVToIX1IaFgszHG2zzfdpaGay1OoJhZXPgAMKuLHAqm8vzA5X0PwpbZH7CNvulCNcOEZPwnIlfDmqEtxLtxUEvSHQ3qgxMs79RiddHV72FGj64TcwRgY/HeDkVxQjLMUb6/ttN5kOiD3fVHMacn8DPndzHSRcURANilgp8AxOhXWNsd7bf2ldVCsOOU4WzN8sdrEjg61tjWGP136jgFmM+0IRgx/Uhs3TcXQFlcEvNp9KJYP802RjZen5r/TM+NzVbm2PbsEqhuomvYRllu6htavu+qPwTgerwzAEvGLq7Yek3KXGaqgQhs0DlTCJUmKDeQ+ocdwkCZUKtMpmVZ0EsA36AWBKMUISu1Wp7lqO1oa4ZUOst16GwTNRVZ712KdK42ZVNTzBBPh1qLGwoFxMPjCNqHtU7oraEz0O4+3T6m474HYYak36tEbswJHdLYq+JR2OGViSjuYgPlrEG/MRIlEzmz7iyKpvaswMZEHQEcTUjCC6KkjR3h7kYWWABOi2FtDLPPFGC0ojTSc6HSAEG/Zx7Otb7aX+zArQ9VCOId7w6sQ+tXzDRfczfQHpmkMXvA6gNaG42Z53s/U1uG8aE8JkenHt3L75cZfLrv+j0CeNPnOgyWfiY2AG+hq4r0hWF8oNQN6ubsCOCz3/TYTvjnJw9DzrhRqC9sful77Pf7cDEbSAhJRjYqYr9Q3TFxQS/s6jhA/Oet71trQ7+YpGoZ4OS+IdV03TzyRgBmnwWtdQ0ksbJmkKxwrtcWgJ6moj71wrAtreV6jA0vSq069z+hjnhORwejUEcTx0SNgJFJZgmiQwQWILWJ6AvbAM0mJoDsizuwohUI2MEGcCA3qNnnhCihIrOv+PUDYyBy17PseO5qgh6bRWbM0DuDcQSGBqSiXuiuWQj2xFLYUWvNkRpZ5DnHG5l2411Z5ylWeUqF4OfbG7B5ShhYcRDlgRyPjM8X9qo5wRBPNvOonUMSKByP9d0+T0Q+9kBtdF9vHLrBxPOVpRksEjBttihU0838wfnyu0oZvoGCffzcPzbx6ejoeAM3ksEAqrivL0OreTDQWse0mdCauHS2Fqy11oplWbyNOaOZePCYT2/R30nz8Sg4a8g0oAFRvqbVRVOJhbWbSsFUYv1C13wpAPfiumbrn3D5VIJCcD34M23SH8tx34MwEbkbWBym12VnQs6OO9JEZJBZUDXKDTDRsKnbyggi/oQTE39ogbEoo0GGE0wJ/DkKISbmlT0TQq+FgdWByGP3QeItakHXoUmVdz6lvw2GR9gf9rJwhncRfZ6BRe7buxnm8r3v2hep704DOTvoZrI3gqb/huje03rhI1306l75vGc6jvvBsDa1S7cRC0SPUdb/26aQJDBXiwknQ60b/SbsEmw5lo+Y+Lgh+T2Dm69b649maCFM/WlLU+OxfDeVAlBBYQn9hXo+1KmiTpPMv9KARUDZ7mnzZ1EWXIhQpknAUN9D9SHyTH1/K9MFcPJa0jmdvI+IVEWnHhayOdkGSqrnVe+kwgDMXQ2D109kVlNV3fNUaRn4OABh7l1FJf17xUJ9sBCVYG1HtAqvdo3lDYYtXmc2VrxTJ6TTs/GwhVyUQdhnBops56xAKDOG8eWg7YMDedFdH+4TrGqGFeDGPUaQ9zalZ/fecTjsXfwLEB6ZNxljcSQ5BtOP1gAFYMU+jc3Eq499nINgoJsax984De5rIP5IDPeZ1S4npAJ/R6T28/EcIVOsYHyP1SFRXJz62anB0FaY+mq1cTrzBWt4PMPr5XGyVShL5p6BeAH3puHKkvinaQSdJc5BatV6TAsRUCu4NdRaV+PLvrFYsqFSU74TvZ6I4EuWY22YVFrVm4I066GorsTIbLlibN0Xra5u6WfFbU0A2fIxc++oZUI7mZ/53hwfByAciyZbQ0P8sg3T3MeChwKAla4XAO5R5BMrxrQSB/NzbKJAGXlHjwnlO20wEZ9AKzY9xsnbQ4oyoB4UkAFADGyWiu+UyC85AEwBPqoH8t+dO/b7/RHbDC1OMI3Ar5EZrsFrDWLPyI6TNiFGjGDk0Wnx+h52b4SWc/SnGNv2kRjvWtVxqt0n26+tEADheJmkDhnZ62mAP2UnsDfL98nqDU5j61GZem3sTomEU+iCuVtCHpn7rc0inbGwbu6Ls3uYm1ohCW5Q97Aw5hEAAeCz7RatNczzDMByX3dvnxRQkDaKUU+TR1FSiZCQmVK08jjURZORJDSoD3X41EOrfkybWEsR7iyhzVJ7T6PsNBdF4Xo0v+7Vcf+DcNqRbYLKQMpgZaZl/2ddBOsIMSBy6ebPyBCJTK86VvLIhgUfRk0MVECh9DdWrKwSBOTdNwMYqXGqEIFrAbdQN4iqRd8PFVgltTYWYs+xhZzFeH9vjRTc73YwS7m9+xpEQ2wOQFkDxxpcTuqBV2K1/GNVo9OiTCO2vjeN/5N3xTiWz2QsXPf3qeOj1XuL+spsAhAgONEXp66P9x+ZfZ4XebsjMuJh/bLqS/uGgxWLJ4Js28aqSYFX2i3BDtwWFJKpW2vMdfQulWBIvCR6I9EDw7CZtEwQo+o6mAyklwZo2HGtCADWjZdbx6RstXEXXTFbfm/SNK62TjuYgDqpNMryHoywBUlJJ+kJUv4hKmQLeyZQ3Xjf5bHDJ/yEn9vBDI8aE49BO9Y6XEB25cwmSEkG+8CDOcrKZzEbgGUeM/Ems6FsfJDqt4YR4krm4o4TJ2ktEaNzc51mKYTWONoflhhn9KYmcQDoCN0yjPGQ90k4PavbWyoXzprqb7/fiYjIETE3ghn0zn0A3rz5rQ1cdxP7879uPCODIFMNrS9EnG8EM9BL3y1YaX7G+tlrQD7F4k9dn0XrIzar88hYnDEvYPSEOK2bVujUOemfst/4JHM+3jCFGvtGDYDQfU1kwgJm1wlD02I2BeFaSYxdurZMFdGZvYJzB2FpYyBE0HUFbYgBTsKULTK0oJAw0zxjSyFQt0hRaPXl7u5kplqIDRsehAWyjG9FGHpnsGYZhI6HjQEpGLsJ3oajAyED39vjvgdhqxp7LGoHOGS9k5wDPScmJljrs2G1SLm4G40b90y/t2JEg0ok3cdy+Fay8GHVHjo2SnrAqOJCcGOETrRJGT5DypVPhZWJiK+mIjSgXNJUFIVIc6UKw5FlXlftLq6O6MYujgArs237/XTF2+jnjzypx8Wo24RtMnIGoqPy+NpXprdO+9sJRty5D59/NBuEXevnJ7YZgBp6c7v2buAPrPXg6W4sUpMDsYGqnSMUOMbhBCgboLF1J9t8hoNzXBfqCJn+XXS5YJzVCiqEpbXYEozda+cLyVDQZ/bc17VKLom2NPUZJjXcsXpERE5r7k3UZTDDnM7RAiy9o/VFIugKIMUOtDAnS0RroZjr2+3keTWYIPku7N21OwpVDzbpBKDYXChAN2+Se3/c9yDM6nMo/oOxQC0fQ+9W7PJYZ3yk97MByYfq0CTs8hh4AQxGNUv4bkxkfS4pw013kYngZS9IKnd09clUVTP1rqn/GDD3uQSA48I30Bdn9VqRVDTCZoEi6jDIJnFxcUeZsDnRSzKjqNyMBLj2O/uaPqXztX/XhruT4Mzxj/NNXn2JaIMxRjJ8SkDJaXFmN8TU5amdcf9nAmZ9W32uSk6r8NgMxMO90qZyN5Y9bABkvD6BPptqwWE2+oxHZu6qCI7tkgDJDZLexkBapCkB3O1mg2ma0NqCSuJXz1p/sLUOuLuXzX3tc8hmTiSMmgjYTBXLvKCAUYsQC8nHrjUea0Gn8FoArE4eYcOE3kWlYQVJw/At/VMhhjYqBVTJ/ZJFj1y8baYmrEVKJrEy5GbqFiYNTgpyci+PjwsQpgJFXtmRq1dz1XIqJdjMWg+cRUwT4cFR9t6+tmrMWcd8pLJIoqj9rQ9aPSMtJDnRGa+/Fxil2oRXn2EtlChuODKJ7DmsyKiaNK+dF+3SCezVPEKHWogw7w+6YTFExxzlxbPxSfpMfo93GtuZmeEa2HyzUurKuSec3to/+ncaCGe9qU3RaSOQ+W1WTHVFrtPlIzAeqQ5IINiBi3KOEfLINn+u/cWjufCk1EDQTXUttY2/nwLvYR67NAXbYaM1HAANxFiYOg0w74wOk6kKFcytOdlBB5gsX4qw3sIFNUlmtvYKCFMliLKtoC3yjGkSF7PWO2ohFKrDexQtHFp1PXYNxrKcFKWQMmHSJPDwSjrCU+we0tbqtR6KJyqSjHA2KgrOH4Xk9lyO+x6ES6lq8eyeJMeqvNp8S0v7CDQBOLAMLAwxaU2nJPe1545Gn1qrhyZz754syFQC3opSQuGpzG0qaqlNwGLGtTWgi6VYbkE9XNvyu8gEHVNies5VhgYDZHAmLPMsWatyO9L7rxnvKBFkoLDeOwFi6TBJxfHSReYMkuzqCb/nEXqyKWBGlocRlO6+wOLcuxro/H8Cwh0ydJS/N55MWjFCmagBa9KVxJPZROAC2z6j3bFVu6sZEHP5hMpk6AFO7eLcQ6d0ySIpde4oKFjm8PttvYefMpU0dc1vgj1bWWtNcvaCMRGh9wVA1wK2cPuD6Iw7qr5qMy8NmNZFVEcWAeeVQvS5lSiS+CRbSCnFN3dChCmvbTtEkIKkuqYZ0ECZtXfKvTnuexCuVXex3nUSJYDUVdFXC/FYVLbM+3bJ2r2qOMt9Jku6fF+wpKQo7lkB8k0BBK9c0NkqEOSCpCMo5Kg5IsJCTXRYqh92Y5UCMDgMZadSUkInsYCLnNeWBfM8Y9u7uOsAAjcaXHLcJ6N0se6DZ2KVGRhDWMjAvQKT9aXpXAGp1Un5ftEARcRghcHo98SkNwABAABJREFU4zNg/Cz/6YZOGufI2gBsbRxZeLDiGB+oWN/BSKHdFlU3/i/2Rt0oyQE+gSvHBhGsfPTc8L73+6qBrTN66dqfqXpLOkiVtuwBGh29sa7BBm4NbV7AsMKeuhZ6h5c7ImG/vS2aqJ6wLBKWLNIcK9iGzSH6HR5RZ+22WnMErVieKjXnNWRrwlQc8sNOVjQN2z0/Pg5AuGKeZxeDALGuFnXIsXBgIIOuHLY4jvWZOAm4nj8YwSKzyoFVDDLDRKkAEoAFGOo9YDt59upwgoxpmpwNhx4boM5gRHXaHBqa25cnnpBwM0Do96Sfachom2c1YkKT+CBpJqNzbKPKfZr/RRLZ04W6wEZSKG52sfGI/j2P1ah3zgl0jOGd4rh+jt6bMrNM4wwXlcgZvX0e9xj/b1VZ2Nj78O7hXhjnj/0X/aUtNAnA2OWA/wGy6zdNAoujP9nz9J7kQBq6bLu4c9eTxQuh9SaeNkSwOnKszxf1i3ZeIVTV07KWOhL1awehAdwg+SFyrUVLMMkaEKIMGgTm5jl9dfBCyqAweBcqmvzHNqvoTwBwEQWyWdSqNhSQdpIFfmhgCGz9RvTc83Hc9yDcloaQYNVFrBOYIoP1kd4MickBsSoRi5dWf9t9Tonj7irTu84D1fESDwvKduAwIEW4sVJZBdQR2PJzJOl1QeEu2dL0ARbNF5FCBFMnBBsTA4Tpy4hy+SSts6XpDEvR5NtlBE8DscwC15JF6GQzaFKwQ9h3ADTYRG4Q7PQ0yxwlmVNjc8TcIICSwTBHqBqZTHBtH8g9i58l52tfirGHBtbrsM+ZdCdWP/RduLDFd8lgyzyEKXNqU55UAxD734kd6+JwdyySDbZzkBaQld8SlQKK+T7ru+vze2+Q3L0dHaQubpA5A7unqlgs+tM2A7KgJXXv1DYUklSZOeeEb+QuQYpnRUhdjDqJurFuqtaJNMnPGDBQat7QyrCmhEgwuLEmkS+gTyR1f25HbwtqmWyfhy18LcOp/x9tzc4ofW0VT49nNLQDqAbIK0a8NsiFbk0WvDHcnF3KgNd3brKyLOK6VgqBWy66eKxzNjeeWgBClcoAHIY+Ail7BVyGNvHLmbv0ATE89WUpFZ0Z+90F4CKkJN4OlqWbxolNKPfR8edjH0XfmZ7SANR+H8d3YNreMwhxATR4iZw8OPqH0t+g8DegxL6Gtg+AoHxS507O2esfOtJieD+9GDGuwZUdhn1zXt073T/0uxnMlYUo4Jk0YVLP0A/Gat1HOKSW3kUvDAuig7lAWn9YYUyZp14T7sS1gBjdOhjzsqBMYkBDM9Yv7eksgF1qwWaz0fsuLpG51DakdA1XUtMdD+k319IGF0/rmu0gxvo7M3qqDH2vj/sfhPsYfADEZJEdXnZhMnGJwoPBdZe+O4pu2QdKQSdbbfMRi57R2PTTkPVURqAGEAUU7XPVeWW9VeSPyKAVjLPWKmLb0kRlUEkYK5NhJfTVASrg3jwLlakgSEOebXsqJOLlMh9Er8fdxb8AEGmjufvczehWiJJB0PD7tARxsi9XksPIboOlUnckBDEN7HZ9X5NyzYQnRlM478zMPjov/0pjUQocbxZDOxlHgG6fj4oKe4aAgnnkHAOtkdbsr7wG4Ogf1/fqqbktOq39ehn/gsN+j8VKFjkAhyQnwN3U2YI8ly9MXaZP7wrwogGQ3wuJZzoB6ERauPREhkDdPHLeYLAwVQtoij6AG94ErEnTAQRx8jWO0COXIlXTqaS82uqGOR+Fjd+b474HYZslNpim87HPAivi98xknQHp5Lb6VgZO6/OR7m3fmVHAgwEAZZpWjJPveh/zrbQAiby2LYQzvwuRuOdQrZoIhbRSh24y2gZLZmJ/V99AWCerVizgqC922O+EIWku1qgYMk7OgMLjsQgl0Mjk10Cc73EKoEMHnpPId0/Sbc8bWrNWT9gfXSUA6xMVw23xDq+g34ujXzBkB/Dh3PBnyADMiTkfH2OIebA7fYX8RmmTzwCLNKfiewyfiRviOrDDgDu8EVhz7opdpXmEmqgtpEVZN25GPmuCN32QGuI8keJ0A89ucBwRezoczlDn3lQLRqnyc/NusHVUSDbf8NeWtWCqFaszJ54UMYdNpSECqTDptkhwyPNx3PcgLIPJeR7Drb06+YomzCEFG7ZRB5zx6h9+D8q629Uzj4x7JubIJ9oGZWkqgmZQOTLqwRK/6OSSGzgTl2tbhG4u7JNRI+Zj4RPQm6kOCtQbLRgGLMSTvFYdQRh2X5q+r4CviIqsJWyO3/2ob4ylEQ0eC8d6Wl1PGZTThre+ZrieR6AKpjhC5LgpqqrGwT+LC+le8Yj4y96RzLPQ2KPeh42EGkON547tH1UD63eKp4pUA2Oa62P4nIfrBfjaAOI+78AosM01VBLWltt3bktlDO7OKCVRThiGOzPIpTUDVs67hqXKHjamYT0ShtJeS28ukdYSUlQnwHx0WImCAXJSLAqzrRPAjAapzFynCT3lOEZRzw0Nua9FXVrRQZ2w2UypzuS9P54Vv37zm9+ML/7iL8a1a9fw0EMP4eu+7uvw3ve+dzhnt9vhDW94Az7pkz4JV69exWtf+1o8/vjjwznvf//78dVf/dW4fPkyHnroIXzXd32XK93t+PVf/3V8wRd8Ac7OzvDpn/7peOtb3/qcXtDy4Fq6O0nLeNAMTiKG1DpJ9vwEhHIQRGFfYEr9QhFtkwH6FK8xUDRLuZotlEHJYpR7j0ncTxuy4lneBiL3g7b7hMN5PD//EBUxdChjnkrFVOWn6AQsVNT9R4wiFuU3LwdwUzVHKsXUM9j6Auajv4cfYPgbgMb088nFmQMH1j95Q833Hw+hZ0nSFlA00ZoZjS1I1gxWrAai0+23ORJh3Cp2M0NvlB4Vi7jjRF/Yc9f9hwzIMhd6tDDx5mOQ8L4BhrOZO7rXkLOrVpuntYEEaA2AmcVTYlkWv3/v8pn8u2h7NV8LqZpBazlmtzS7NsZP+nuQQinAVZ6hXj9F0sF28wQi9tVFkZgFzMC8NLCqVbizv0spkycWKhWYpopaVL1XxHBX1Ci3mTZH6sZ7dTyru77jHe/AG97wBvzGb/wG3v72t2OeZ7zyla/E7du3/Zzv+I7vwL/9t/8W/+pf/Su84x3vwB//8R/jr//1v+7ft9bw1V/91TgcDvhP/+k/4Wd+5mfw1re+FW9605v8nPe973346q/+anzlV34l3vOe9+Dbv/3b8bf/9t/GL//yLz/rF9yenWGz2SjLIxwOBzAzzs/PcenSJf0uWJKIKJJUxL0IjDeRWU0T00WwVtNjEdFQxmX4DlA8MH3aeP98PhALsCholqPnEaKSQFo8zO7iP7SBYvMgkvtWBd1aTActLaqUNh2NjzYxVsAgcqwegcYJMDwCzeGaIHzc49x87zUQro8AqONzJf2i/QQvNrHX8xX49YnxniSc9n7sybVsTJki4ZbBpbWrr8bJwMcYncPtihA4t2XrX29JzKUhq1HqWw0tNh2qbUgOfNA+8Dk5zjsjLbHZsvdbPtfaLGCsQUl2fwXqo43XNliCEwgvRw940MXQJiMgZD7uYuQOf3RJ1FNquGCKi13XwA+AWIt+DpuU6JHrpkpy+jIJQSsV07TBdnN2PBHuwUF8Uqb56I4/+ZM/wUMPPYR3vOMd+PIv/3I89dRTeNGLXoSf+7mfw9/4G38DAPBf/st/wWd91mfhXe96F770S78U/+7f/Tv8tb/21/DHf/zHePjhhwEAP/VTP4Xv+Z7vwZ/8yZ9gu93ie77ne/CLv/iL+L3f+z1/1ute9zo8+eST+KVf+qWPqm03b97EjRs38EVf8JexqdV33MPhgMuXL7uPrVtwgQHcTPEfgMvOCEnPtZ0xg2cpxZnClStXjsDCDQogmMLfE7Hj2EDl4ClN8LaY6FynyVkIEIYvbrIQrNCps6wejLCovrmkthGpjpUZOQillIK6OcNL/8JfwObyFdTNFlQnFKq+YYWHQNzP/6XkOWHPyr9bqamsOadjnWy+7zP9bTrDu12bJQ7bXFmlE9dfpvfhE/fwv1XdY2qWvFHna71feHw+9H0Zo9osH0wVV649CCobdMj7OYd28DQEXUsGkgnNdjjSTUdeMyWUAoO4g3vDbncbh8MOpRQ89dRT+K//9b+i8zLOTw6Vg6koLFR+3U9RMKF739r6YmaP9MwdLWtFVBLN9bRSRokAUYWRMmILtS+EaSPVR3prQi5qdfVhzjNlzxVXNVbAF5VF0Xp5Ro5qrdgfZvzM/+df4qmnnsL169fvMlLP/viY+PVTTz0FAHjwwQcBAO9+97sxzzNe8YpX+Dmf+ZmfiU/5lE/Bu971LgDAu971Lnzu536uAzAAvOpVr8LNmzfx+7//+35OvoedY/c4dez3e9y8eXP4AYC2zFFKpYibizFVA+C1QWwddGE7M1SkKVpJILOWPDntOcPO39PkVE5bqmRgMy6EpI7wJCOk+reOBHSmEqnuBy0PV6OC+kXa+bUUN7xVu0eheLYzeLgnxFQn1FpERFN1R1FGAbCLl2tW571ojE37xAourlmyvC88ebhXNHBp4e4/62N4HpI4ns61DWmtwsjs0Bm8Ms71o/x+iQ0DOHYb0zGV76BxD8qIrY0I9QIQjCw/K4hvArXE+K3DrZ/tfHczc8AViSBeN72vqoK6MtymYv88zzjMeyztIGxSCYb9WP04Y8Z3GydLvo70bPj4i3okqymMWDB3D6Sxz5dlQVPf44UbFjXetS6BWJIwfpE+qxPEqhEuoUZIbHgMZG09LIs8u1RLcCVrv/5Z847ovePbv/3b8Vf+yl/BX/yLfxEA8Nhjj2G73eKBBx4Yzn344Yfx2GOP+TkZgO17++6Zzrl58yYuLi5w6dKlo/a8+c1vxg/8wA8cN1RlS0uA4wnNMbIV+1+29K6Zru/WytaMFTOi9IodRywssWtjEAXhd2w0gOPu2g6ShCNu16FxAsG8H9jDM0GiM0NvAKpP7KJAmg8r+12pwLaDtfqkaL5YIqB5rTlYBRzRx2kmLde3GKs0cZ7KkMBmzQQ9wT4HIzR25+zUzz0deJG/9/wNZF07cNl0bjyDYAtUTmE4htor+fXk75WAkPJ9V2Cazk+3jPvaPDBWPVwLRDhauoal30LRbdPdkC5esut4Mcc722xLTwEg+ttuBEKj5bJ06L1gC8Bmjt7cjGc2ThJIkXI5mO+6+vWJMW9J7FrGNpOkSgVM4+ZpVZTtOdAN3XLDEBUnQpNW2KiTqBeYWfW/ZpCNtcnMmuqSQKqaNMLwfBzPGYTf8IY34Pd+7/fwH/7Df7iX7XnOx/d+7/fiO7/zO/3vmzdv4iUveUmcsJocllQECHGykxe8CJ9CDlAaFlbI3CFAM58cqLWu10A7dHLQjSKA1w0UbD7IYkAwsQwGGyUSVtcp/Cezntieba4/hUiZOWOjWaYywOTNx1UJqnIQECZn3eTllYAx/pYCeGD5NlPHpb408IuoCgSIsy7W03h71MehQ2V/pFfh0HvmcSRvzhiQYc77vuHpyQPTp3jtnvo5t8MXuZFnL5CWbmSIvzqccAPqVcDwOLy0oZ3cjNLFrjNOxlR58Xw9K7iz1JKz81g23kIFfPSOSGHulqI1+rtMln+E/XGdYc77wZJJXCpt12Nm9KY1H7X9BEm9ihKpLU8FYJhkJay3+Tq37aYzoy8S7NEKqd5YyJDnkBEWg06ETa3Kvtvae/CeHc8JhN/4xjfiF37hF/DOd74Tn/zJn+yfP/LIIzgcDnjyyScHNvz444/jkUce8XN+67d+a7ifeU/kc9YeFY8//jiuX79+kgUDwNnZGc7OjhXna6PZoGuF9ivBEz4DpBFJyqTcB3XcCS0jmkHw2qvB/ZF9wttkCmMfc/Bey8MAJkm1aforJUBWCy7vyK13TFVBVHcPe08zKi7Lgq7n9dbU5UcmXzHA5e76YVDcwyUATXjSulTJ7RlU3FdYXJZSCeAUIJEs3qk/SPvHlBie5xc+FHb50eaWNzxXhaT72sLTy4M5ngDLtX6YcOp5+ktKEm8P8G2Yg8OOoMBpQ1qDprmmhV7XgMRbb/1rz+CQwEBIqhxH+uPfrY+8fZGVLA7RHS/zDFNjdGbM86KqpJx92Ki1qs9gKqQeIdudQZWUeaqxrqmaollfdU3BGmPICr6jUVxTrDJcLXLkrbDqfyEUjFonmL8xqZ8zmUub9nnrlhxA1n/vUhJptsyBhbD0hufjeFZKDmbGG9/4Rvzrf/2v8Wu/9mt46UtfOnz/hV/4hdhsNvjVX/1V/+y9730v3v/+9+NlL3sZAOBlL3sZ/vN//s/44Ac/6Oe8/e1vx/Xr1/HZn/3Zfk6+h51j93g2R2azwMgIjfHazDTxinVHdgTEMZjnPgEVMVCR7Lpm0Z3nWbwvVCYOMVBcaVA0B7b7C8PFWzOImRrDMv7baaWIThklwMZ1wOpWJp+JkaFILDOquqNNpTrTLkWswUVj9E2UI6ooZQJR9Uk7zw15s8n/ErPWIMsbT/ST9flpl68AChOZs/vSKV3wkcgfD0s/8AV+6vz151aJxa71c2EAo38pebTn0eo9121yPfPRYZGMBrz2dzBRf2SaQ7CAigTA8pM8T3KWMtUim4ukzWuXxvSaZZnR+oKxz4HtZqMZ+NJ6AEBFfX9dopBnlgLNBSyAbQboWGsAkeZ8YHbXxKakoaOjLxIYJGoJuafMV00utVKvGUGRZ4mqzewavXfXY/cmv7vXh4I+UUGdJkzTRiLmmLHMs+unl/b8UOFnxYTf8IY34Od+7ufwb/7Nv8G1a9dch3vjxg1cunQJN27cwOtf/3p853d+Jx588EFcv34df+fv/B287GUvw5d+6ZcCAF75ylfisz/7s/E3/+bfxFve8hY89thj+L7v+z684Q1vcCb7rd/6rfjH//gf47u/+7vxt/7W38Kv/dqv4V/+y3+JX/zFX3zWL2jg0VobVBAWTdOVtRYX/dUjgJM4vlI3OECCQAhjWQZNRso1QeRO6ib5ueirTMbEXSDWNqketZq+oHdVWYzx7+JGNilbbaqfLpimoq5y6jWh/SB6ZEmX2TW8l9SKLCGl5vaW2mHW7y5iYklAEayxw1zoocwjEd8414DJmWsYn5J8clINlEVPA9ijczg9C5y2rmPme/IzZ97sWOvM0xlzgCKp5KTfJAZ/GnYz685/r7+HtwCqCtE26ftBNy8DUkIAcez6DHMp1NZ531mLXZfbG/b7CwfurkC/LLNLVFQs6X+0n5SpSg3NiokIVYHPHlNUupu7gfToLYP0nmDJaU0by4pG/k7TVLXPSevUkXhBeL7uvAla8YGOTZ2kOgbgAEuUxplZpcrsZtfcUwjAoA+/l8ezAuGf/MmfBAB8xVd8xfD5P/tn/wzf9E3fBAD4kR/5EZRS8NrXvhb7/R6vetWr8E/+yT/xc2ut+IVf+AV827d9G172spfhypUr+MZv/Eb8vb/39/ycl770pfjFX/xFfMd3fAd+9Ed/FJ/8yZ+M//P//D/xqle96jm8IrvbWE79aGBZM0DoTm8TtWjWsSiSWTzPAGAqCRXtTO3JojogIvGQWBafWA7UCPZqPqt2ODDr50xAY/bJ6ITZNwjdaHTxV5pcjLNJY+/tVmCSxNcEQrFKzERobG5ro4Gy1ur/EhXNzcxSDqezG0x0v1E3IBOvx9wO7pak727P1kEY1AWndJ0uUnOcY/pbANoe6MKJUlIGo1nUzyqHU8Aom3GwRgKrOjwitwLa/Eb+LE73iveDi/ED1c6AmD7JZ2XgNV+rqFk4smFrleuDjUisQNtnIotb2rJIGXq7V++ijiAi1GnyKjScx86fGVnKfJzNeNazFApsNhM2m40z1JCSIrSedI5Km23dFkwbMbJZSP2U8geTEgkLppI13PRf+PpGtcCmcQ70ZUGWRmRTlXX7PDlHfGx+wn+WD/MT/uK//PmYpgkAaYUN0eu4ztMAQXGyDsxW2bEbqswIBgAR9WbfiXeAdKd5FMgR/onFn8l671h4HsyBAAHT1bov6sDQ4ZsJmDQpkBowlGG3RXTAFsnk9+8cUX9KWc1ZXuLlq7PMqUzK5ivKZosbn/QgtmfnYmkGiV9lkZp0Z2dnsMCA5os/JIXwoAjx0aQQV4FTLMJ8eK4O63UdOxdxwSDzsgCncVzPjhizvDEOZxxfBDOLIZ3PYFc/5YKydg/fTNjSsmdYPc2EI8WpAT2hbi7h0uXrYFcJJBc7N9bC1QHyXgLWPWVLc39ynb/MHW3ZY95doLWDJNyBAXDD4TDj/e//H7hz5xaazpnUUOQ83cMGBvXaYQNkAbRllvyF06Zgu93i4uICnVk3+pjzDAb3KBsGLmhd6iAWlTDZ+1aeb6y1K/ufpsmlIgvWWubmhALKhGtRYKcCaFSdhC1ruH5rmDYbzPOM//fz4Cd83+eOMMwkZ0cjiLGBrJ5n+7ClkTRwyaAM7s6O/X7OtApMj0e+YNIEhbaDoOBuCWiU+eaoJ9KF7z7JGi49vJ8BkYEdwChqrBHH9jJVtItFwYuUaXeZcJC5V0gc4Iv5BBdzKxL2UUuVelzqM+lFTYl80YJLREklBgltleXLyOoYY2tIZ5/iBUTCPg2wox9puNae5WNLPuwJIKN99n0G4qyeOHW9u975ICl4wIxwp5m8tTk/c2Tga7VEmreJfRqTtYaZaXNgwbmPXCpJY4GO3hrmeYdlv4MkWg/1RjDhjv1+L363DDQXWpyfJ0kxWDCgAT5FAnoseTqrPzBhwv5wCCmGhLi01lySDJYvNo06VUybSdrVGhozyjTBVtvhcECBFBCVzUpaWFR6o1JdopwmJRakFZoBdekUd7ZpmpxMTGfbgcDc6+PjAIQFJIoOilv+FYyczzCUTQpAFYT+mNlKIqnbNyXw0JGJ7Gym6gAMuE20ycY9+Ll6vTLeju5JT0zfiwwIxvpY34ET4ysRqMwApkkih87OtljmBb0ZqwB4ZZAUBlyV8aphTsuHV/PAgOnnVA2hiXtyRWNz8u+qpiBSPTMiiT0lFBpAKIEqgPSdfaAAnCQQ7pYWUReyXHFiJnSIL/L4aQDuCIgRzTWCcroyTTIFQEr+sqv720QihDokA5kD6+rd5flp40xjkG4Uc09lJmO5o45Un9EblrbgsL+D3mfNfdGdNZo+uKtR7M6di5CkSD1rNE8JE0f7SYOENDvgsjRx+9SkPh6OrGtyniVfjIn5vQf4Zymic9c9gl2taDsZs1ZqhhgCJ62skeskApA8EVDbRxFD83yYZTNShk1E2EzqkjaLWmaaJpxtJmHwnyhv9NwOgnkz6ACYOIPwNxT2ybGIPY8vIMu7DL6mkTqRU8HNYQkZ7Ev2Jd1F14aYzISEJCqbAqsuOgoT2sQMS64USxTAD6sfVWHiU5lwfukyiAjzMoPqpGtQxMRKaeETAaUK49VYeVCUYrLSNGDxtvD2dICLgJvV72stp5a0voADRahWRrByHu99E0zamaiNgUka5GtRx2nNJZH62RZ1PxqHPFtOf74CU9h+kdwM84aS3jsz6g6oDWCUEmIenPAzz0BrbIziXdfs2P926cTuL+5nbZkxz3u05QCR++C9ttZzSzUMcUu0BFtEhAlA4wYVfvwZpkqwsHwqBYdl8WhNY7Z10vJepYoUVqTSsru3UfRbbDyy6VjwhfdTZ/Ak0ttUJ9Vpm7QLcGdsNgbAWtC2A9wX8eyAZWir2EwbSRDEhgOqAhNbdUr3em+P+x+EFaQiD8L405P4PEaTJYaZxLlhp/bgBzPQJZHViCZD3eCEARHgPr28WnT2d1VDoqestEXLltNB2+4s2v5VcVDbu9/vpY1E2GzPAZo1Eiq1Dwh/YWW/VMXdTtL+iUsbupQ18tA9Nl2fRB8REzqRZxSTvSKDHsOjoZAXe1JDUDD7jINrHamKK4HE/s+oTrDf4eOXJJA0PwwkrbuP9cEmSmfAS98a20wMPQxK683DxNpI93jEnOOPFGAkIBpi9lrFYO9n79uUeQPgjr5o+PFygPkOE4lY3pcFS1+C0SPcAwEJ452X5v3WW9emkybCiY0tfPEJaE08kBr7xlRrATfxtGilo6Jq7ToLROoxZ5hdHdghYe8WUWFGwsIEnlnUFaUKqNo2oqwdUFUWoJJTwdJaCuXXLIKFwJ1Qi2VXrNhsztw/uHyCCT/Hg0IEHVUBOSwZ4pDeGkoJELDlZKW4ZfLFQgzjmPxtA26TVfTNTqkVgZPeUD/0JhkgAFGCvgNUgWKi9Eql4ZOfimMTaX4L4xDb7QZt6aDSIlkPGytg0ZfVqnpfAKWi1I3npyi1AqWDsWjFgUzQEotiRunFNwFD+mBwCibGe4lhGY+jv0dJIdid7hyqVyywvu76meHCsSrCGbkX68yM9pgNj+oHGc8A+czQY7Hn57g+UzcfVuiVDIsBw5kB233y3MiSVdc0jqRhv860feqYG5f5CWvgxbJgmfdSEkjdvJhFVVVr8U3Z3ocZqQ0F8zyL73BrnoI1z33qMSbWjm6VPV3lZ9n7ZLhY69Shk+R1UbC1djsIk7ifoTVY4mvyZ4i6pFJUIadJ+6UnKktQdQaDawFYQqTNwMiMCJ9GwVRJVX4RJVqY0alLPu3n4bjvQdhEGxMZ7YiQRwEtUcQHy3UfR5t4djOMC8/vR+ZOk9iUXleMNeh9PUuZ3RIxuVyYTDK4A7zfNwOxTXbdUEpE5NUi71XKhIZZ9GRTdTwrRdIFSm6AApqqJvCZ/P5inGHVL4tVmlAHnbD5jxITOjoKi/EuANdYrnWhTXrWgqvKIBVJjakH+x39gtWs5Koa4dzJ7c8uS4m4jY2vdY52BBiuRU5OTNx021hdlw5KzJC7D7CoIQjhtZwNfwHqMe4UyXYAtDajtxmVMhsLNix3FhBubUFbDliWg4BY8qQolguhr5l4PMzaUaeKwywJsDwVZ2d0aqhU0/w1FzJ45QrrR3+uzsXudediPHIuFzf8IqkJARCpB4gmYRNvig4mxqTpXMWdrqnxmpRUVX835gq2Pzvcz7jqxiYJBrtft91uMS8dzQJJ/iz4Cf/veDCHoQm6y/fetWigBBcQ1IIKWn3PDoykDNPYwujE7dM5LVBBEmG01pYUGJJYlakhZJFKuK9VPA7WQRFm6aoUdb0pEunjLM1BpqJzpHXcbrfubpeT6FeN6hODpagloAy+1gmbsw24MbZn52ItnioaR1Jv7wuWynTCfFfgZKwyMaQAY+kvWZfOh/2yYIx6uHoj2GLCY6eHAXI6ZmT8MX8/ShTOrF19smbKRzqAE2w6PYcNduPJWQ3FbNLBuq98qFUiYhx2Fzi/XFHKxoFTulGykLVlxjIfRC+qrNgOk6zEKEVoHeht0axr5iue1RAytrdv3/aE97l5TC3WjY43g8ROwBC1gREMfVfT59pnTna4w8KS18mCZK2ptFckNJ5Te2phiGK6CavuUtJrsjrlrNIWMdoyA0vzSLuqJcDQCdxnLPOi3hOqp+YOasLWW2tpbt7b474HYWOQrgroEUiRY+SLl04R5mMMOTuLSzpHAareDDgh16vF2FhEFmmDVQbbNWZnLlusv5suzM51GGBhdR6Fh0h4LQmuVSxUZjVNW1iF6FIKejEmLW46vSeGqfcnIkx1QqkT6iSMRPSGG6DKApa3ZWAlmQVoGYTJ32YJz2oH0UQE3ZWWWP1rk1CElWYG7Iua4bBm7fdwYt/Yul/rmgRDbdsAXOUwHiHwxEaQ4F4BI4BEzllvOvY/cg8Csy3EphJsMN+HgKOSXETChne3n8Zme6b6STGaSejtEv7CScWxVpkAqlYDMB8OOlZmFOt+rRmdrTiBg23qM6lu4coWl9qGeWXdwWMGtLw2zH/f1BDRN9Y/qlLg4m1zNVjvEFPcAWZPqZolrRQAvYHZEhApByjwsbfMg5K4fYJXGlmasCvEOH+CCT/Hw6aCR4IhrLTcuyeNyQYRYat6Hkmimw5Wpb7EpdNRVqkUuTVY9k8tchPNky7CWK9zSDZdilOiQgHAZhg0o6OFH0MZe+9dDQsbB317w9bUkT2pSbizxtpPWspFANdDvdWLxKoV8BLvxsxQt2PhH52Tl4dFyAUrzQA4LFbm2IggUGpgZsBI9nkGLdddpH5HGFttfENMId/Y4tEnPBPSYVKMv2+SUgaAM8lgvNgG3lqGtL0OwAb4KwOr5hAYrc9oF3N8ZxuUnwPv2yyBZeBaZlFTyFZl6SDh+RsMnFo74M5u56wzPBZ4/B2iGoj+pKFvsVp3ROSMl5lT7mAF5ak6w8/PXRBMOlcIt2eabz9MFUHx7qUUMOl8ZvP/Nw8mcum222YCFSZ0M2KGVJt+Ho77HoSBwDIA7qt4OBxgO7YBsO34tqNTyuplGfwNHCktArlAgaPofZx5BofyvLzeLmkYsRUjDR9HM7SQRrV5pJYCsk2M3js2UxWQdGBIAFGMoVN6X2iO13ApkmxXRRMOFb/OIosAgCqht46pppLjZKGqphNmVO4S9DKwxNFYhwQQp8CPdOPLAOxjCUK5C4ASzFNhDKkYztUzvT/Sd+O8oeH38ft4t/U7xHmhT84qlZzOMgkE6ZrMKtcvYGw39xQjZ9ERaUmKW87zwZ/dm6gsvLqF+V0ye/h8Tq40LwvuXNwZVFd5I7J3rpUADfaJWoAAk+qJm5EeeW9Tr1mSdYBRqEq5LJZoNReZ9J9p2qC1cD8TQ7BcKy/YPfgJWqqeHfSNsQNtnmVdNPLaiq01UBHdr603W+M1eXu05RNM+DkdQuIsOCNYpH3pYIW00DiYT3ExRHbaWqqWh7FblExW0Vvot0QCHkVqAwDKQE4FROHmkw1iBJv4tPo+kg+RuZf5nCV1sdlAyTym7cZTFhCEbRQF0spSoqhWq6hRsdluvaBnU1YtLQdmXZTGamVtWfiq6BtJRf7MFN04Fjah1Vgl0FaWPgKRqRrCvLU+jPWbmsPVHmmzPSUyZ8A+Ug+sWPspEX9on3+f2ejqTKe7iA1SLjo+ObXr9OfC8LbbrRewtY271op5PgyqCu8HHQfdIgfGbJvsulZizuVrh9WUy+y4cYc4CxVPEC+qB/EJbi0RGgDiXSH9113/HOvFWKirn3qAtBEfsAV8LMOGUOrImotGyHbuWEQLpu6gHWgshjhIQh9T55VScVg+UfL+OR0ujqKjkHlA9IGZ5KU0DJR+t7HEP5qRycRZ1wkzQCUAAlAlPonAh27pLeMcdgCVZ0h+C7hoBgJqnWIT8YAMyMStBRUk1ZFL1fwARXduSHBGkYKHZHoysKSwVF9fKpK0WsSz6h4iVrZ8u5VwTcyzqyWaqhZak8TYnRuqmZydHVpfSg8e6RIxgpodWb1hft3xOQXDR2wm+brhGXatNigLLsN9FazZ20K+gZxqV4C3AS4lsDWwWgMmpb7I88z6wTYHACsRfH3Ec6wvGeZzMc8zNtOkFcaBw3xA77phUlcM1jwObN4rBrwjC16WxTdbM+qZhOdGM2WOloAHYPEzLwTqpvIxvwqtpciseUvK0K8G7lky8U0bDGYN7nAmLUTH+xPJvc8jBdNaVeVLqQWsG0bTdwCRl/2yCjw+cladY1l8g7vXx30Pwj1PZAVfIBjOKT4V6gRlBZ09NLdpxQtJL5kyjpHtyqSeScUnXM5Z7O5LJDs+9fChtOfa5BS9bbijqeToLDEYSUQD1TqpQKYLq4vrUi0VZ9szEFmid8ZG8woDwGazSdF/lsx7xrTZ4PKVK+IvOs/Ybrc4O9uCuWNe9iAWfSDB3tu7Qvs5F0uFkp8I2Mh9buNiJwo7MrWBie6RwexU/BJhDYB5ezi14WbPXWuzMedR/Fy3MTaSDPEj+3UgcR9lXn03tjezyfGI+2avB9brKzpaYzStitx7VzWFPrNzGhPnvqo34NiQ0rO5jXX4zHNiGM+W+sYKA3Ak0zGxHgCWuXmSqa417fL81rdMfWRtIXDBoL4w/W+UTjLpSDc6nTudxS5h66ovVhTX5pV6Ymhxz1IgOmndSA+HvffBPH9CJ/ycDqv9Zsy2mThlYg1HpNyaUQ1GH2VepUipbEKRGlUQVlaIPNdv46bMu4CrqiISkwhGpUBPnMQ8uUdmgvJMYxNVF4/t/sY47R6s7bFsaKHDs5SetVbP3dxbG/Ko2veXrlxRd5/u/WjfX7p07swdedF6Dd/wILAjG7H6YAUfv4+//TdfxME/FRSQ4C/d/xSo+TtYX9gY24JHMGtLTuSczCUeHpi0PXdse1Zs4Ogzy1+xZtf5vazd8YwUdZjA2O/DwLLMYGW3Zhcg87PVdsucsV5NYKfPyCDNzFjagouLC/GiSH3ndYCM0EDtHcauVRVCIFUPQBPoqMqA2T0VO3dJ8qP6XDPmhvQQG7AFhDAsc529r3pU6rouhUAq1UXQTLxbrhoTgAxdv6wFTxsO6jlRLUDpaFzvzXH/g7ACjOnIlq4AmUSs9ZHZAAEeFywT2Sqzht+x6KK6g2yOxDMGIoNvbWENeZYHEDOoGihYUc9QWdhE1sY52Mu9wq/RjlqK1uySsNTz83MXISV4o3jCarcSK5PebDbYnp1hu92ilIJ5CX/gs7MznJ+fo7XFs0xx0oFK4IWuLmd6p8Vq6Z/Y/I7BczhzGI8AzBGI832P7qkit9XFHEVeBOork8vqDPL3G9u7Pgz88u8WiJF/jyapOK2Jb2JDXbN5U2ME6BgwGboGS1QBnAU0EvkWOxwHe7bn3O33+TBjOcw6hwXBTVhnfaZoNgiNJNqUu/RuqUCzHMC2PqBrAMVHzjblKDHPqe+sXxjctSwRGZnKOmJxzawk2TD6Mm7uMZmM/Qp7NuN0Jl1LW8B8cPLW0nj/mUjq/r/jQcj6QVG2g8bFtwYDILFhqPu+/l1rVSAsaCmixxhhLkAIiFGAWeLnbT6YT7EbFxC3AauhSxObkOnJzAVNZX1xIYtJlCMAJYiDsdlsHHTNF9P0XhWm/qgwy7K1fdK4+flwcLe87XYr7m6tYyoF82GnCdS7gJMBn72GZy2LzSGDYmauxyOWO2R1MLv4mVnpqft6eyi5/Slo5LbaIpU908A+Pd+uH5p0un0juw8pJf++Ps8AM9821BjyYWbaZEw9qXXWGx25pLRiw6u+WrfXNlwHpN5VOiqDDl6ildU+0JuvDSmlVcHcNBDEJECNtuMkWaHrGgK4xxgNKiUS6dUlkTRdYq3qpqGeD6Z+sHMYKcl8mh+CBUkfrEEcTfvFfLU9QVc/PeYf63Hfg7DtXmK9hSv37cjiOhjhEnaC6RQit7YzJKzTM/7DdF090vXpoBUCqIppQOLyijMLs+oCUICUqS6pMUlJZWK/cqa32SZXqFxYAy5MryX5YI0R+0ZTLL0godatRw+VacLcGni/xzzPsriKJOCe9we0+SAqE82aBgZYXZA60cDIdQT03cqw4NdGtPgd+m6nRtPe/pkFwwGQlDJmg0+wXpdzhenQ8XNHhh7jzPxMm4otdkAMU5bGc/SxPTXH8jvkrx1M7/ae6d3GT0e9rr3GGrwNgPO5h73knLCxs1STnlQ9ne+AWAQEDZhz38jfST+r4Np7x5LHJd1P/1AWnDcmA9axH/w6z0VMIcnqec5o9e9Fr8vjkos+ALL253Vk4z067nsQDlE7OhNJ3M/sFQXDDpqNEAGW3XWtMqiywEJnW5LxTQFUzxEvBdMv6UJOwrSpTET3qgPeofedjN6Iro27V5HIz7SkI5vNRjJNkejlpHgiq8dDB3dgu9167bCqsfLTRnIQb6r4fbZlQa0TKhX0NmNbK0rp4Ca+ur03UI28G5w2KjeY3AWAn0l/mz+THkhqgfT53c4/BVB0AmRBKZiDg107Xq2kowCBUGowr+0K5vulwO/PXQNfendtS6hdEnu/C/jmc+M+1kZ26WwN5oHFpxlx1+g5K4K5dp8EZONlPReA13gzT4kju0p6fm/NJS+XPBgOtFmF4OWRODwqBiFiNT75uQ6o6ibq7dJ7eipbFm+Qol5EFpjFzO4xVEv4y9/r474H4bEU7OiYnwE3/6wZ2vCv30utsS7+mzGKhwlTSvFk0lSs8gR08ZOfMxoKTFUQVaHF0Ff9lUpK5FJcnKoOwu4bDVsgFW1pmHGQyd9MdWE+wIz5sMd+t8OlS5dQNxt1cZvURW8B9QW1ABMK3ETCYsQgDuMewwx5YYW2Pl8vyGdig3lBWT5Yu+4YHDFsmkNQAfKmh6M2YNWGNbsbAT0MRubfC7v/sGlruSOI1LB+zfW8so3A26oqBLv/yQ0qnR0biUgKAup6Jz6lggiwjh9jiCJ6L63FuwJuMDMvGlAEPwHhXrleQ/a3MF/ydzPbhK+9kiSLJOG1bu51o4TDXYynndltPGZ07wm8LRlXnjO2abYUBdeZQZqewNpuhrvWR737vTzuexC2UMxQ6hEImkmKR1HeBgw4BunMOkTg1MBcZvejXLM78yDwXT6L0RzGGCgQrBlETBrZmTvgkwyuJ4z2540EHWjoEnyRgDn6pONwaGK0Q3gCbGvBvL/Azd0FzrZb9NZxcesmKgHbqYiRhgs2xKjEUgZcY/UBoPau0Xt5wtrE/sjM99R3xQDjWY17WMVl1HVxE4nXAAV/PLUZZABeszrmYMV3e49c0DTm3chpQz3AYfSLb4e/8uZs1xQ32KnENTDEiFzLz4t2hmfIWq1iP/vdzucYSNRNAlRpbYC88OsRQ0/SpH+nzzSg9L6nSPAjkmsU75x0fXXfPKApV/ReFn6s7H+0oyhR6KNKam3QNdUDqbhkElFv7F4Tnwhbfo5H7wtak8lqZXpKpUFMd0YC+MLNeuEANp2gCA8HsTjbQKvRYa0XtUnGWiiRZRI2TQJk5WDsMPWCi1NUUlkjhgWbEE3JWBLXATZBxTvCRMXOUvhzv9/j7OwMrTUc9ntMpYhbnbqrbbcbLMuMQ5txfnaGaSOGuGXXsHBDqRLBVBAMijo81zD1MNbZYQmQBsCKNXW00Qn1SSqAu4zv3XTL68MeRQTjpwP4PhP7Xf8+bBC6aHHyOlOBBBTndue2YXh+ALAz5bwRgByEmEdjldwniMQReyaLSOPV9+PfttmYDYWZwSVSsPr9AICKBEjgOAjDgDiHLFupsUxgTC2Wr7H3NrWBfRapQo/73e4DAL1HgIe+VFzHvgV5EAk48EBSbpoB3Sqmf4IJP6eja55QY4ubzcajfNbqBxB5YpD1BGbm0CdDGbHutib6B/gei8p+v6LhxV2yPDHDM1ENRsLVQfZj9+sdzA0oOdkJa2ScACIV0x0XoHfMywzujPOzMwBiJDzsLkDbLc42RQM4CH05gOcDGjMWXnD50hmmDWE+LOC+oHIFUUeVB8MMPSWxkzXrkzksE9s2DAPa/I4hKcTn6944BZRHAJwWTRapJe0hFOBtXHA8F46eaS0cD0/atAJxe8gakPybNejrD7EZ9OLNo022IUUfA4kVZmYNgWK2zUzvI8Ddh3vL/eNv947w8PR4T5PqAFG3yJoR+bCiYl1+KDPdUjVHsM7LgQmv+s7axByBHQxoSmtySUdUVePmKCoGuba1WQlLAa2yosmGIV48EnUaNfnk+eIWVyAqkfkTYcvP7cg+u6VKtdZ5PsCdypMob2qLzLvyBGGWcvI6PaBZoNGa6lV5PN/EKhvcUtJiBdAgn3Fv6ByGgqxWyJ+ZzzOItPy3Bm9ogojeGWUC6iaMCLvdBaZJkvJM0wRUUT1YMcNaCJupoC8LwA2H3QGsGagKgH6YsecLTIVw5XyD3kRMRWcUlrI00EoGna1Wtfa9sbdhRCTht+slDcj18N9o/GDQy38ERnLEjhHt8ty1gEb4ndL7Hh+ycXx050rzQ/n00T4jXx3PPd5oZE9Z+YgoaLlbFbIBK/8+AnC+3EDa5rokuVIXLWP0hKiu3OM+WaLMUiGgKrRJXSg1ICnrggdd7SBFhh7Wth5OagXrpZyRTdZM828t+MnulcfCrutdVqNhwdj3LkOtNth7d9z3IAwSH8WLiwsFxGnUr1GyjgPDwjk1SXrwYDkzsef1jm5Miwiaw1evVlZFUOsy2V9JzF2pNEbGAEybCeAIWShU0dosk079d5dFLNxgwmajQ90Zh2UBgXG23YK4oR32mCrQlj2gte3qVIDegA5MmMQTYmFJBN4WzPsD+tzAvYC7eG90Lzskbe+JrUjJpSIJhSw7nWkdlEla9Q1R+CS1hA/lsU7Z+ub4e4KVpohxMBHWFldJ9x/F21hwsdxlER+3JS9Oy08bdPaZF+4RyAIwQ94xAKu7md07sWysBAufzWnT5xzunBjfwP44nnWYZzT1lDDDqOfU1grHrm7I91itm0IVnZs3zlwfc1sy6WBADd3Csv3+J/osdN/hN51JlQGp3F/6xIDdR0jHLycrKqZaI8krzBBPoefjuO9B2BI2X758GfO8YLOxiKPIDwGcWNCmAGCgkLiHSc5UcpDwXE+d3ZAAH3QL22UXe1y3rOuoQBzDY/HJ8z2HL2Kxm6uMufUQLHev+HHWql7IDMzLjD531LqR1JTqu7lo8EUkUmkoxKhEoN5wVknBXYwvrXUUalJbqwB3bu9xfn6G3md0FubcGgG1AF3fzZ02CBGsQS5W2jtJzmEGlwS1xo5hYammvbWuTcxvpe/IYr88AwrsDHTy88WQpBspseZjUh9vVgNlUi9kSj58nJth7BQBpJELd6V6sBv4hpB8HOw+DvCqfhCa6uekt0Z46HQPwe85BBm2F6mXhMlxq/4a1SYSDdpbkxpuyXfe1k1HeDeEyoGAbrrYlIAJAJUqdd44GHBrTQBXrzVSA1aPB5MmiFz3HpuQHFGqKW1iCJ9mIzx5DHQJ6lLlo/YCKb4gzQFO7P5eHh8HIBysctKS1gaKXjWgRIUKs6xajmpmBmrkN7VCkQbk5p42LI40T2Ty9+G8rIfMVubMwjIzsEQ+dj/X7zkrU1VAIXUls+TVkXTlbCt64DpJej5uC4gbNrWAeEZhdT8jwn63A/eG+bDDdiqoZxMO+4bbt29jv9MyOdNGytX1BeiTBmpYNjfbcErKnRuGDmZzVQIoFa20zcveU7sSEnBQHA/JSK4DvTEvzZrlQNedjHqyGJh/i/c6srguXgYeVOsDavfP7TPANIAEiY90Zld3VUG4WsFAH/p+1paRtWbgcUBRUJVnI+7nMkR4TGRy4Qa91DYrM2Tz9dad2zEPS1TWiDFQmkKevh/cO6Za0XuRYgkGeF37qxC4S4kE6R8lNDonxu4J3beBsvTOKFsQiZqx9QYzRvu69e61tRvX+WbFgLmWZg8ik+K8j3ueD/f2uO9BmAioWpvKjAsGguPuJ2US0SW5iPjhsrMSFzF9zcggVytrpOIhHblnmRgb3hiS7Sx00NAJufZDXAeOgAikrmoy+SZ/h94aoOksNy6qSkz+dnOmO35Hn2c06qikSVRKEa9fFsZ7OHSgLZj3O0wTYZoKLna3cdjPaK2rsYbRdzuUzZmwZU0SbkYOa7OJzCNYdZSgy8JUUzcYe7H1zu531UNUzp3k4rMwvOELcPrOFjZc1UEmoBuTc22qIc3qdohnxILOG7CCAI6Pu4JyZscMRDaMxNysjbpRm7+tPbFrB7K3MQFtGgM3duEUANtnAry7i534CUMS+TOJOoEKoZPmDCHJU9Jt07DrK2mfpug02DoqLt8UAspk7p3dgfyor9LmZpFwXo0caex8DsYYHUdwaldzsGLrM1ni2UsnhmLdZ/fyuO9BWPrNalN1yCvz0OEeFNF18G0gggiJSFQMLC3+Xf2N0VFQEmDYs3mYBOLYbvoq+cwXB48Lta8m43pyTAmAAdZk8gwURmsEYtETFwD73QUASV1Z0EG9YbOZMFWgYgEvM8AL5lnCksGM820FU8fhcMA8L9jv99jtD2AGttOEpXXQzMKCW0chE/07OPkNm+jrqhjNA+vip/nskkkXkSO2KxMxx3oY25U0cb5IxshGZVZSqCZEZVh/y0YqVR+0ikryGs166HxtuLVF7ToNFfAFLfc1/2/2BbzWZQ9tYud7/lwaHX51Q5L+jXgKmTPd1R3BEU8x3ZA1jvXCth56D+Pw4XCQisoqPTAzqArr7cyarEkDoTwRUdG9Ksa7VvO/F8G+W75TbXGWDKHzI7IGRpBLMQN3khJt07BainmdhCoiHUTetnztwH51FgApZ4V9ehrPP+bjvgdhAODewNAUd4gBN/E1FnEsxg5G4byYyEVrE2EARGy9urMAGBaADazpdE1UyqoI7t22iQAXZTNWG49ZcjgQxDPCdHGVSNNzSmUMBrDMEoKJSd5drupGInC2qUDX8oiHGTzvUbCgQPW03FCnDW7dvgMiwuGwx2FeYNncZs3Jyv0gxjQVZV1fzh29W1pOaZuPhTIeqYCrLk9gQCUP00muAYQRZjQyAEJyG/TNFgHACd4CTBXomKBFmGIzRhmYj+lGTU0AWOkr9fDQdhRTJwgKqygO976wBZ9zKw8k20BZn3WKLTMjgjE4+gRsKga71ygPWMu9J1f3NiIeajPxIz9Y+krdbGhKAUnFclhrOyjaQFQAMp1sdkEjLybgRjaKtZLHJ+cNlueMfvPZGGj9Z/jYnCDRoCbM6qO8KbstBxjnKYApuYyKzv0TOuHneGiki4omHeNuaQsjp3TMg2Fp/Ex06QaGyjKNCWWx1AeZgg1nlx1XgBRZ5FOtgKoqxLNBmU2voEl3fl0ojbu3iZkxbTaqh7NsVvA2cGtovWNTJVk1lAFX6kDruLjYgfoBmypqle1mwnyYgVJx52IHALhzcQetLZiXjt3FAYC53S1A2aDXCm4NNEmpcdLq1pKwxja55puWNm4QJeV1czRShM+mfwJMsooB6UtVbchii6xyofaxrS1UJiHCK0B3o69wlmrqC1OJyMX2OwVIU7iQBSiynmL+yat25/Og7VvPo95gBjhy9Mo9FV4oa5B1o54yS/ssz39notYCZlzsdu7MZkl5GJY3WMbOwLdQhVUrZ5CzXiL5zBQ8Od1r9H9mpsFe82Ys8wcnGGuoa8gCSQYJbCREa+Mis0iQtjGbjYiRvCPkQs918XwcHwcgrL6CDq4x2Hm+2jqVelgU+XQPs/oGywST3d+WmeqiSJgdacaqnEoPpQ5JTsiYCrPXfGtL81pWeRELyVSfR7P2WmkiAJYdonNHazN6E/WILZQCsSr3NoPnhsIN59MlEDOW5QD0GVMBCjHaMmPPHW2ZARDuXOz0vsKAD4cFrcmG09qCabNBaw1zO2A6t+TZEhRSC+BBEQCMXlqxxVJCfIcxqK5QF/tXWpCnxzWzTIBdb6iCJgYUT9QzeDX8HIFYUzokfTEibHrUT6U34/E7dnYqz6GVT+9aPRGfr1iwbxAUumvQ4I+djVdj26wdapPQxno6RrZzDKxCL7y0hovdXsTxQn6f8PyQNhUvTGtkQwOFfEPoKJXcDVOkhuKbSefY1AIoC4JB2xygAZzzT9V0lIl9uFxg9pe1Om/4m+SZEsE6hoYzJw29Menn4bjvQdi8HWjtJobYGbPu1g1rWgKezQCgE9UYknk7+GRXWTEn7glGnAMw9KMOr7IMyC682WwGUam7vjEFNygYmzh2UPG+aWYqZkbZbCVCru/R5wOYGzYEnF3aYjnssG8zCotxzghSbx18EJ3vfn/A3JpHCBGk+GHdbNCWRTYhKlh6x8ILqoalckkszxcVvI/ixbqEwKYjdPNyqbivHeciHv7t8E0JgKsB5Hm2avV6k1QomGvn7I2hZ+X2koBoZstOTq1Omn1luyuFR4INtNQmHIEks/BR95/6ZFAkxLXDdxy/h244ruu9O6NnH4fROJcBmJmx2+8kabqxWxa1WDGwRgSuWN/1zkBvWlox9XUiFb139auUfjdwy0yVWV0ji2Xgi80p+/dnNjxuXAyE56mvO0tla+dLVB35XG3JNiEeRbKJeIFber58Iz4OQBjQQUsglYHYvreBlHR8M2bEYrdzgDLooiz5OlT/ZwvKrgmD3xjrLvlWOyqZ+qM467LJVWsFdXYXV6jut7UG5/OtoXNK6M4MRsN+dyEmE5JqA5UIm01FXxbsDzsQOoqGHZezLVqTSbfb76WsN0HVG+rV0Tp6a+h9Fh1qLbh9cYFDY8xcMS0LME1A16rRJjqrBJLBxioWZPDQDk5+xNqnuvHYIgyOE2zTUn2KNBJg78xPWgz3nPX7cDBDIm+fJ35zrUSoJOB4zwEqtrH6q8jZ4oaXwtgBsTFQAM/xxjLaDPxFVCyyLGb+fv5IBZD0J4Z2m1+47nCpDcGEFZy449bt265CElCSu1mYsm9gvg5YXQCL90u3NhVpLSF7NCS1TeqLNcgaMWq9xUa7kiByoiTZLMLrqHXGvNuJysSi/NJ6XidFK8VUaZb/Ofz1bSN7Po77HoRt0YdhTEGxFJ+4Zmyw8d1Mmku3RSijD34SgwhQXRn7gsuLSgwQyRND28SIkF7J8SCsYGkLbMIySxWCpgyTiNAWYarNWZnE67Maxtqc0gLWCqg7G9AxM2G3SJiyxFZI+y5wUEOMAKyERstmc1ga2iKfgyWXwFQlmGN/WLCbG5ZOqBe3ceXsDNnP1BlkStwyLqBjEColfDxPTXjr5xDEMwOST0pZif6m7pA/FIOSXjizXg6Qc7BSEG8AirVfsdVcoA3uwgc5SQTWWmZQYmj2mf17yk/WPDnk7+5tyZKFvaslOIfOm2JAzHE/6Hy8W4UIA7Lbt28j1wi0Z9g6kHVRYkO1vsWY7tF1uJqrggqJX3iBqGiSoXL97vZOWXq1tJNZ0sx9KM/i4bPDYQY4gqlGCZUG8N9ut0AyCpvQRErHc9rLe3nc9yDMKo55EulV0hAALmOKDtYs5qOYyGkB6tSSawAFgLGkSrq56MpWzMdAgRQ0OrMb/UBQdtol0g0LNpstNptJvBk4VCHdWEImlfZDALcFjI7d0jABEl7cxQpOhXDr6TsAi2W4Kw7N8x7LMotU0BhAl8QypWDuDeCCuTG6ViXe73a43BskXE68MMxQI/0T3iADwKZxMAZlf3eYKLvOJWyh32OCSwMnB8b4n4Bz4o4OkUzIxFeea3ph+H3FFtA9gGMgvg6ycBB0HEeMCwED84opYqoT3yr0fsZY9dr1EwkRwMCMqEyd3jEBsPyQq02yOiJfdLHb4ebTTyuYAV5KiqE2geRBpGxfNn2d+8xYLJNfuj+z2j6oopK69qkUUop5KLmPEKzqBiASoBMbD2uODUH6k4YfW2dnZ2eY5xmH/QEM0yFXrG1sZgMSoapIj5vkuu6ne3w8K3Pfm9/8ZnzxF38xrl27hoceeghf93Vfh/e+973DOV/xFV9x1CHf+q3fOpzz/ve/H1/91V+Ny5cv46GHHsJ3fdd3ecYmO379138dX/AFX4CzszN8+qd/Ot761rc+pxd0cUtF+bVy3kVk+dBdWyzLU1Y/ALE72sotpXgdN0twHbvyWMF2WRZPfN2YJUtZU3czsKeC5M6S1Mc2DISeilQctBL0y2H+/7P3rzGXZVd5MPqMOedae+/3Wpe+VJtuEwdHJB3sQ0ARtHLEIcG4BU2EEpCCULARN7XVRMJI0HJkIgUpgOBHsIQCkZACUmxFgEBR6BjLsWNLES1BUPzZhhOfD38cbNxd93rv7957rTnH92Nc5ly7yiTdrpL4CpbVrqr33Xtd5ppzzDGe8YxnyHn1s2a487BGJPP0ijb/JGE6DANWqzVOz84xjAUhdsgMnC9XODk9FwoaA+uxIDMwjkLxG3LBmAvOliuMKnKdszSEHFZr98gt/G6TY9yoZlk0wajel0cVG5i3bU6b+gQWKdgGJOcj/w7LiZvrtU0nuRrEGuA0HrRmzFmDcB71XbJvmNXDLPX+9Zks6teHm8xF+H1NK9T4izzjxFjqZ3MzL+vcpMn3LOc//Qw3xt3GsP0OcOv2HaUyyiYqfebqOa39V7u7MMkcyXqd1NC+zIN1I6uwk0R4BFCQ5pwgMAksp3u7exJMMpOK8ZD9WWtHD6fTuRdbI+DFYoHdvX1sLbYnLcnMo+60fyJgcGAdE3t/03G8v8dr8oQ//vGP44UXXsDf/bt/F+M44p//83+Ot7/97fijP/ojbG9v++d+8Ad/ED/5kz/p/97a2vK/55zx3HPP4cqVK/jd3/1dvPrqq3jHO96BruvwUz/1UwCAP/mTP8Fzzz2H559/Hu9///vxkY98BD/wAz+AJ554As8+++xrekAzstA/25YmLQ7cvrxWr1RgA6F+USBE5bdyKeBAyCyFGgiKO6pxiGS8yqnCU9DJUpowq7AR/NXXohpSm28zqKKVHNXDsvCSITSmlCKMVV7yKPM4BIzDUA0js/vxs77D+WqF9TDArCKFhNT1OD9fOt1oXA9gKK4HwjAKL7owsF4ucXJygm4+RwgMjlpswgQES361IIAckqNhF0DSJ5vG7PBH9fcjH2kx4gLrWFyPUlcyy7N6cg4tjFCvIteugLC0app6tDBnkgFQxX3J3VP4Bs3m3LaGWP9N7Qn1+puQTTXuGwNB5slOYZ7N7+pffEzNoLPvEPXnBYzVeoXDO4fiYRrMFgiknRFlkwWMDSRwhKyHTLoZNYkswIxa9XYzqudqzr9o/iplzALOmOR6BARuob6ullBDHR229lLV27e54c5WIHR9j4TOx9o3iOZzaDYmM8YGC3nR0H0+iL8E837jxg089thj+PjHP45v+IZvACCe8Fd/9Vfj53/+5+/5nQ9+8IP4tm/7Nrzyyit4/PHHAQC/9Eu/hBdffBE3btxA3/d48cUX8dJLL+HTn/60f++7vuu7cHBwgN/5nd+553lXqxVWq5X/++joCE899RTe/v/5f6NLna6tRieiMcSbjAlmbvqkkffTCqrHYFnxqt1Ak3NudkGwITa5S8BwMqu6o5o0wZSsXkwusxUPKc0EoUpcD1rjHwPQBTEPgYTmNo5ZF3RRiphOvBBwdnbmhRilSDcOAsQwg52CBIiNGUZZSLkwKERkJiAmXPmyL8N8sUBIScq+fbzF40Ezxu14R+V4socYaH4vVVoC4TQwgbaTcpEXaMbeo3qxhAYeSHspaD6gOY/CGvZ/bIvN9AYUmtIBa2AQi4pawX3Z7MnPTf69+qcOhz9pNbLUPL8ZCthX9KKOhLvxdUuPVlHMNxv3dPVjpYiimW1fpWDkAs4F167fwJ/92SsYxrFeB4C1BQshYBgGkYTd2DgNQghEHinIWjBxHFtvxZ0cy4XYn4UZ4OwOinvoChFaJEHN+EgEmH18bHANSghEWvAUxJnSDbXV7jYpWstJ+GfNNuipx3HEJz79/8Xh4SH29vZwv44viX18eHgIALh06dLk5+9///vxyCOP4Ku+6qvwnve8B2dnZ/67l19+GW95y1vcAAPAs88+i6OjI/zhH/6hf+Ztb3vb5JzPPvssXn755S96Lz/90z+N/f19/++pp56SX+hCICJXJ7OEAhFNXoZBCs5WYDWSKnqDxmjHELxVdjtZ5JLk/97M+ApjABUCgeBcHs6yTYyMPGYUhSUANQnstkwmt/go+pgMoKAoPDGOGavVgFyAzPpfkZZEy9UawzhqJRwhFziPdyxZOi7rJpAtvCVgzFlhiOLjwZBCkZOTY4dFmJvfMwNNCCi2wBWSmvY4DRGfDdLg+jk3NLKQ2q6+Pt5cXDBGFpL2F4OG8ID+XuJo+zsXZUZwY8TYoAL9GeCwBAAtRKgQDFsrHZ6G+RVa0EcpEi1VWKNuqtWkwf9tXu/ktzz93J/rDRuEVsTntR8aNEHMWK1WOD48VMzUlg75dQExlqnr6vqwSKPZeGR+5PqeCE73ss3Qu72Yk6NQYTbjb4bXNmIY1CD+R4FBGu0YBnhBkIq0BwogBJ2vWTDtUjefaSRs41icm19zN+rF/0VLzJVS8CM/8iP4e3/v7+Grvuqr/Off/d3fjS//8i/HG97wBnzyk5/Eiy++iM985jP4zd/8TQDA1atXJwYYgP/76tWrf+5njo6OcH5+jsVicdf9vOc978GP/uiP+r/NEzaaDSBzq2Zs1VPYMMTGiLAvlKyqaySepWC5CkEw3HiiMcY1EpwmAG0jKEXaDBHIJ4UYvGbiBfLCMT8XKowRY0QeRxAxUpKOBu4lkBgo6UhQMGiZs55IEDj1mlerNUz2ryY9MMHPQ7HrBv8skTApGNAmiBkHdw6wtb0jkEVU76yBf2x1F1M70/vxBc1sagTVUyzVJ5t6jhV+YAYoaCjKgLb4gDFSqPEEqblu4z7JTx0Gcg5CM/b1PYq2SPU66ztmgEODglDduHGPQ2EF0q96dNLMGWa5/0IN6wYT/3cyJlCD3aANMua6WcOTgPq9IlVjR2qAZ30vZfGWjwgqnWrVpOrBQkfJoLOcM7quBwCM4+BwRtSGtEWjN6dT2rMT1T6QkOSd/zzrPE0BhAhTIyylAEkjklLAhaQbMuqGRE1RiDhAtq7rum/HWta+JbwrlGKbhG2uD+J43Ub4hRdewKc//Wn8t//23yY//6Ef+iH/+1ve8hY88cQT+KZv+iZ89rOfxVd8xVe8/jv9Xxyz2Qyz2eyun0tIaOFtxYnckNwDH57gbI3XmccRTHCPtnq8rNF2DaWNzF/pVzWBh2KFAjSZBIUZnM2za7jFXJ+DGV4T7zqv+mTy+aazb9CkGvNkFydiRCYsV2uMg/TPciZDMz5+X0SiB1uK9rsTA5Sz6WUQ+q4DU8TBwQH62QxdQ7avk13kLOVaBgOQe73yT4k4fHxcr6EuHg+lWYwKEYPYKFWkcLCFlmqAHENAYxDrufSM/rN2vdkW3hrlTWPZThY3gk3Vxl0bsm4YNYK2+7VoqAoF+V2yorf2M91x66bfXAt3/z2bIW6eoZSCk+NjHB8dSZFEDL65llKjgJKn4XmMCdYsN+esHqd4kYYlexADaKfxjHEcJk6ODz7VZzZKpLDZyGEOkFW/Bo1iRGyeFZZIMSLEqONUBXiYedLhpo1ezZP3wVPHy26sinURuvhgyGSvC4744R/+Yfz2b/82/ut//a948skn/9zPft3XfR0A4I//+I8BAFeuXMG1a9cmn7F/X7ly5c/9zN7e3j294P/tQ729TSrLpjgIUHdKgJAhnp5VA1EICFGNg00MPdpzW0hjFLWcM8ZxxJBHFJJFISHuNEXUJjXGnD3ktkkooayEweM4QOhxaoABlDE7nCFJlsq0EOwrYhilDJl0wcnwsCdbjOnR8iuBimVbmGwbXNHnOz87w8HBQdPnq2abLR6nsgEzuKcqyU1nRDBPRFNaw9FGHNDwtE2mTNgD9p+dA9XQyTjr5xrRmUlYj5oAamEn/4xfgpp7qve7+V8dvfrsdp81dK/Z/mzwi21EzY21z9w8po91/SA0ydgK3xSs10vcuXNbN28GWCiPpFhqPZXMI0AMMIUofdeG0Q0t0PDeqXrJhgObUc9ZDLcUAQmmC53DpPdVdK2KfKqsmzzK37lwc02LW8QpGNZrDMOgieG6Wcp/NXJt54udSDY9w4+xwY6qHvL9Pl6TEWZm/PAP/zB+67d+Cx/96Efxpje96X/5nU984hMAgCeeeAIA8Mwzz+BTn/oUrl+/7p/58Ic/jL29PTz99NP+mY985COT83z4wx/GM88881puF4BiuqqGJZxh/XnjAd8Lv62LUD0ShQCsSKOMMokCV+/Zul6IlxrFi4Pt7NVQtMwLQBMaQZMIzjUOk02hgDVMlP+GcY31MMjnUIn1wzhgyKPguuMoC4ckMzybz9HPZogxoktdDSsV8yosHE+j0lnn2qI/qzxXCaKJ7HmLekry2du3b+P05EQpddNxts2kXURm5OwebFNyc7VhVD1q4Iy7jFtrPNvvmaVUo6D1X/r5AnDRvmzTc7VG94sdXiLd3O89Da+fb5PmxpONePN7NuJmV6cG3M7TGBy2N2Q/h31b5qFu4sO4xq2bNzEMK5Q8AmpoSSOtsWjnC3U6QojoZjNQjMh5xKjGrobp8mKNkjYxgJA1EQJ5BOdrzd5D87zejIFMMCtMnBOdgQgU0fKLrQ+jOU2sDlSx3IFu9ETQ+V/XmXny5s1LlCLzPJf/9Tx4vcdr8q9feOEFfOADH8B//I//Ebu7u47h7u/vY7FY4LOf/Sw+8IEP4Fu/9Vtx+fJlfPKTn8S73/1ufMM3fAPe+ta3AgDe/va34+mnn8b3fM/34Gd/9mdx9epVvPe978ULL7zgcMLzzz+PX/iFX8CP//iP4/u+7/vw0Y9+FL/2a7+Gl1566XU8Yo0/Q4heStuKe6SU7tl4cNM7tgjTvDoPeWvpVBNWT4sTrBa+lKx0MarsfWasV2v/Xg1dya9hiQFjWJTMqAUlZmzqgqihoQhvxxSxXq8RRvGYbaG0fbUINYKuIVvlsBrVDKQi8kRACOIB2mInmb43b95E6npsbW25Kp08VzWUzAxuS8l1LNqwXWrv6uHfLdaws7HmG5+xMbQxtoVbn680zg01YfH0hC1k1G7Q/q4AVKy1/RwcGmrPadAIb9y4IeXtGAm+v4E7T+Om6tFBNECsh56Pt5kUdtOCPI44vHMHZ2enOpayEaaYMORBzx38vCkmhNSpJztK66MYdd2Y/VQMvhHhsbVi9FDbNCokwU59m8BhAAg2n22jVMW2aIOqd0hR4KhiKm0RTOwQm81NdYP1PZO+Bzs/+ZwQmhsQYvDkn9H0HsTxmihqd2Fgevy7f/fv8L3f+734/Oc/j3/6T/8pPv3pT+P09BRPPfUU/tE/+kd473vfO6F0/Omf/ine9a534WMf+xi2t7fxzne+Ez/zMz8j3YD1+NjHPoZ3v/vd+KM/+iM8+eST+Imf+Al87/d+7//2gx0dHWF/fx/f8g/+PvpZwy1UHM3KE21nBjDxtuoO3nhs7U7tE8lqsUQXQDRqg4diRmmy3Rml0mRaY+S0HVvkjXdeGkPY9z2YGXkcMZt14FwLIsywgll1BtSL0ErBop/lUhC1wmkYB78XjwTcW5XPGo/S7QJpMsW8GIr6/OIxFcU4Z/M5Hr9yBbPZHJa5dsOo4jitUvYEDrJ/2/XQ8jR1AVlytPmenMc+I29vqqJp5xcDbhxyD1Wh4a3NE1gySalqzTOQwVOI7j3aWE7eZXPd9j7uMub+vO29bi7PJkZwy0KASaoyMMnoWuE6F1EuKxK13Lx1HTevX0NU7zSrDgliwnIYsVwPyJk1QQt0fS9thHSOxaAe55hBUaK9cZDIi5tNIGt3Dttc2ee9RYwiwO+bRvOU9vQ1erRxZ1iVoVRySpJ4GAYEImctmWNisK/R3KbrTq5U35m8d9usDL4AAvKY8an/ef8pal8ST/gv8mFG+Lm3fRNmfef4Yk3QyYCbN2yKSW3jQtm9q/fQQhjR5CLVAAEyR8wTbRkXQLugClDYqTyWiDBtVgvn2ski3qqcwzQw5NoBw7BGCISkk9AMJ5eCrpNqIoMWbAEHCsqeqB7xZkl3Kdmz1m15rHM+1f1hBICilK2a5GeTkJwvFnjDlSfQ9TOHP/wgAtFUSNuMVWj/NCMsX4IZwLv6nmFqUEWLrTQZ7yayMVJ/Y1DrbTUetI6XGWU37HoOCtJp2harIANTw1sjG0x+vnkYTHUvI9yeqyYQm/llPh8DlhsAVdwzM2vz1ow7t2/h6tVXQIqJJ4XZxgKUELFcj1iuR4zjqB4qAd4hY6oJrA8sRngclWNsDgWc/TPxcpkn803OIN54wLRRaTtWbGJNGnISWDYWMFLqBErxKNW+X4WWjMNMqE1G4YbYolct4lBdb4kO5WRihP/nfTfCD792hE4cw3dy0bY/QerDzeBtqvADQkezrgntpPAQs7C2yZFpyAyfXAYdTHZdzmIWCmsBiO73od5r29HVvltl/arHXnL2MFXKS8Wg55xBLCJEVqVUGxgyxnFAQakGW425bcUVRwVajwC+WIzFYSEcO1siRAJZdl3PvVoucfWaUA7NI/Zd3xYwVThDrgo3vKbWxUTedsrUK4sFiOoNVQ/RTlL837TBVABanWDUa1MN72tySY0jNiEE83wLoCW3HhXdk8I2jX7uaZgVPpjGYG0UVv9sDXDVdNDzk70t9nEaxxF37tzBzRs3PGllm7TcQ0Qea2GP8NADxpwlegoBySIxh++kCAYMUAxIiDAGDMAIKZg722zywR2K9meFc41EmCc5i/o0TaRhWhhFxa+4wk2tIl9hFugv6JwqWdpzOcOpNO89Vq67XUv/twkf3a/jL4ERlqVYbIeEAfeWcLMEWeO1ABN+KxcGxaasGHWhlCzetPEizYiZMR5zRlKj5AUKsMuRKmLLjwpXSo1BA56JL0IfSinCSkFNnSpwcFyyCsczhmFE6hKyeijWRYA82BMDUlC0+o3EmDQcYSsT8ZbjxdeU4o3yX9QiFKu9bxNyZ2dneOWVV3DlyhNYzLdUtMjfUPUg0ZiYJjy3xVCgsA5Dn0Gfx0NVrgbc/10X8CThyiRNNHxzM7hBFzZs8+XpO7OP+A/Ix8ASlSYL1G5s7TNZjz0078Enq0+NGiZPj+lIsd2jhfOtE6/PQgUYhgG3btzAyckxAgGUIpTsUHncFo3pA4rBKrrRsKBHkHZXIJ1rQZ7HQv9Aqp1RalGL8OGrB82MiSKawUJkIYx6rQA2osIavQbUMupARkejJtptHCqu794cHSM02flsrM3zZ38FNn++eATzpR4PvRGGJuKEcUq+KM3jYCWRp9QI71gBRdHJEdueZ2Z9dKcuVfHJtUwdY5I5NREF0RDNQY4sGhSks34SojWLP3YJKSbHsEBaLKHQQ9ZrxmZ2FQBn5+dyXq5LQn5nSSmrFipA0RZPQTvswiakLnOzuphOyBCDS4Oyjh/RtMJovV5jGAuuXLmC7e1t98w1qHTRlsp6bjZEP0sdV4aCDbq5pmDeDABUTQfx0quRrkf1W9Gcky2UZyhEVH/vRpEh4jPuxcM3cuLSXGvDGMCMOia/NxjCFvqUs20J0ekYOITirVosQmueUXfL1XKJV7/wBa1czZIPIGESVP1d0W0QymWo0Zae271MZu8UIh6xiVJlM98ardhcmDIKSuNl2iZZ35uMh0Wlm4lyezRWD8DGK4QgFY9sCoh136zNF6qOBUNodqWwetP6ZmzoDEpBxf3bnMj9Ph56I6zzy/FDCtLQUZS5anWOqY+FtmxTsURCrbgxpbOYErKK3kjyTbwvBkPqMaxGyST7zHazFnzUzLFnv9XrcPiExciEGBBDnNDFzEsrJU87OWgHh1yyYnqWHJHnkfNklKITFLb4NYRU/Mx6dvnGZF4aWdl3EJ5xkHszCMA9U1TPz6CSs7MTfO5zf4pHH30Mly9dRup0+jWFCYINKqUQ1QOp/kg1aLUXbrNA7EtBeRVq8NqFuZnwMljItB/UKVSDQvUqzRhUr9aMQzXQNu9ar/UuL6q6WhNoov694rl3Y8A1gWRJxHo/5mEWcBlxenKCV195FcvzM1BQlkKT92AtIhIalrBugOwQVs7TZx7HESEQxmKaDUbxgpaAT8uB20q2qnFSE2f+PmwDaaIVz82E1gmSY7N6LQQCZ9IkrDlTxddcxbIJYyF7ZQJPos4znSH+Eomie9IPyBF++I0wYIup6O7JyEpIl8IH+UxLWQPgHo7GYt71eLSJpJ6PnFcwQbZqNfVArFTTemk5ZmuhF6CehSyOsW2xopKFFvquB2myWZowzkJA1h5iRASE4CwICoQysk84AI53AXYdqKiNjgNM10KSWuYB1LBfS2jB3qmEmYEYPIxkfVZLIsbmfodhwPUb17BeLfHoo49hvpghxqAQg5o+c/ZRDbB5xvJuTOO29dU1iHT7qn9xuzhdtNX7gsMZhv7Zp81P58YDt7GvITxXr9u82Y0doZ1TsrG1C7reY4tHy+ebz9jzm73ye+XJ8xHkfY7jGgd37uDw4DaG9dK9QTczOs/MGGbOKIWQtRINIfo92Ps0mI1CRB+jV4/qrodALEUcOkdbI2se8vQd1I3RD/dGp4nwTcPtMqZ+Xp4YSeaaHLejtmoi4UVrUVKNdnSMDEqjGmiQ6ow8iOPhN8JkyQN5YTkX4fXC2pno32P0lyYllo3mqIdMwooYyigSjCliGEVIJFoC3UpmjVp21+1U4xtQW3Tb7ywcZQZiknBxzJL1TTEJvl2m2shg417qDaBCKtjwJHLJKhhEcH3ixgsDIJsBZNw8saN/d83jxiAW1dKoEoA08T7smcXjEkN9fHyM5fk5Hnn0Eezt7yPGpANI1eu+57tsvUS9dTLIhCqc2xpCNcqbCbLmVTQ/azdIu+bUM/O/hxZdb4ysm/DpYfPJIyKe3pMYgvrspEbARJtK81nb7OpVSHUgMk5PT3DzxjWsVyuJlEIEaadom9cxGo1SmTAKSxHYm7sKVsqu9e3MlSDvfdBICwxVsyvuObfPHHQ9VaNYx8egB5+/VL3YovQ20ko6e1GToirP3UyGETXhpuMVQlNB17BdmndXowk16j6ymEQ59/t46I1w0mofMYp56jVCJ0/j4diubcLZBs4nTb5ZSSa48nINtLDwvVg8y2gyzzRZ4J7w8wkhxSRjkQkvDAMp0xSvoMIRBl9YkgHNtT0s1oNVfJ2bkNwMhxk00wDwxIWG0pGsdDVLmTZaQ2LPE7ws2/qnGSvDk4qokQa46kKs1iu88sorODo6wiOPPoqtrR2E0Hok1ei041Z/uPGyxSHz+58kmTYM6PS8FZJpDfjdEMH0UuYpT4y7/9eOU/VXJwb7rjPaYwkDxPBS6Dk6qqyTesZqWNerFW7euIY7B3cALuhSwqD0xkmPNYOemCv8o4dBWCUPYqC9h2HxohuneJWMGBJGSBWn3W8Li9j4cfPea0VQ6wlrgk5xu6ASpjY0ZcwuQWqbHUGV7O7F7zfni6oQjx2tlrcMw+bGUXyMQ0hyb1zu8c7uz/HQG2EiCaxHhQRMQtKw0hQTYpACg/ZlbbafH5WgbuW1lrwAxNDlMrqhCSG4EQcwWTi2awu+TKKmRpL8y+odMUvlk3EWRSSllhD7ju1/1yXMLEYb8MVb2DpJGztDA/hJaA+Y59vec2ZWzDdp2K0pLxcrkvsTaUMGa4WencMoSHljHLxslWVBHB0f4fTsFBcuXMLly49gNptPBLTr4jKPFGpbK3OYWSlrtrEBvvG0i+0uY8rm9cPftxtuNwK+bdWxBZT2xJNFTLapW3KXa8JNQmGGKILV9+abuG4cNkeDhsspRoHRsslyGgAhCmjr9RqHhwe4c+c2hmENhyaCYbUFjAgKSvlTLeb6vDqWJEyBGEa5jgZp1jGjMhq8tk+1ieHvs45Pm8yy/IfmOKBOBsT9NZjFIlMyJ6BhFUnORJ+ZpQipTbi167b9t49VY6Sjz4Gpkb57flT6Wgjka+t+Hw+9EbYSTgPfW1k+e/kxRlAu3sfLqFyRQlNQAYcYWqK50c7MCzBDmfXn5hHZIX26LBSL6pmawYtAkWyzCb17622qEw4aQhoeF4PqWaDeR1bMrOpTGBWsGsTK5tDFwmgWjngsgg+KeDoCTSa/PL+UsYY4xeyCUZfUUzQt59Lghe3CKKXg1u1bOD4+waWLF3HhwkWp0mq5nPAo0r1Q29DMgFUdDUu3wYX4N4+pd9yEnWrVyUJW5yBPPelJo0s7pwxinX7Ubnht2Gs2q3YLtvMFnZNdH6UIYcwYtBrNtDVQpMXV0eEhbty8geXyHCGQwgzqRPi9Ssv31OCqKSUN80mdiKqBTCGAs2iFWKFDxV3JHsyjOWs5XmGW1tNXKqduiOCCEJMzW0IMYDaoS5OG+l7vgod0czMC0FgyUColtJ1/PuYb75vZ3sO0m840Sq0JPXOWKARQeTCg8ENvhMUghuppNDueea3MFq7XREDh2pEZukiYmp0bDTYVGm1TBjLXtu/gdvnWRVipaKEms1CD0nGU5OGYs3ieDa5rhQDmhdqab/80Q4SmCEW+Y8z66gEQkZengiozwu465wLEgKAJpQmWrCFlYatuk8WQdcPy5AdVbrbBH/7vGKXcOTOW56e4tlrh9u3b2N/fx4X9C5gv5mpUdLHomBaCU6HMN4RzcCt+LGNin9ooAGiMS2bx8hvqtnuycp06Jm1fiRrp2CDL/20Gr3Y77WbiZe0eVdhGF5BHxnp97qL3RTHf1XqFgzsHODg4QMkjxnFAUuNrR0zCD7fNFGQbbXO/jNpxJpIYNIPPdFtvDW8bUVhDWk86T+a3JYwrJpv081xkwhSyYhCTDVA4S98vHCueQgUSZWaA2CEVOWWpAgIbm2Mb9bkhntxvdQYm84XFAaFwr7d5/46H3ggDsmAoEEpRChlNO2J40irnmkwzzCjUBWkJhs2DzRPWHV+MbK4JMWbtGCv8xGmICjXyyojIeZJdlkSdeNZZVc38XtQTNcNYzws3erkYbmxestHnqkFnle+rLA59LvkATLjHZC09RGwSLRbGJ62YK+uVD1wBS42/uGg+3tF4ziyJInDVElitznHjxhK3bt3EhQsXcGH/Ara2t0UlSxd09bUwWXREthGxTwC7v3tBEv6s6oFlPZ+fn1k40maG2/en/f3aM7VeWH3PVhzUGmCdNertW6umUcILl/HkIk0Azs7OcHh0hIM7dzCOI2IgdF1yjrtJT3o+I0gnDFYPWrD6Ov9NAU+iB9lgXEnMDCla2MuiPUZKtgGzazeAVbdai4jEYckARdWjzqhCHgwm2+zZzxNj8gQaMM2ptJtHNs0U0g0MUKijspoIlrNgv+/W0MKjPvl88U1DvXbSopFS7opw7ufx0BvhdrkFla6zxWSTtkooVnnDe2k/SBv6JgEBErWmXHtgOY5s3qAZbZbsfc7Z9RDkvMrYUB2Dwlw9abtTJmk1D9bCjpa3KwbY1KxEBIk8o21es11PvAD9u64B02e1Y+IZ6MKzqqKcxzqWauRjitUzhhiNpO1loF7wWmUPAcWU9RwmAlRy1TuejjtwfHSIw8ND9P0Me/v72NnbxWw+RwrSvtxijbbyMRC54RSPxmZDTWjZOFDzLoC66Rpdz+5HoAK5VsV4LbIZa/QDoPrnej+h9aXIN8rW0NsGbt6q6eMeHx7h8OgIZ6cn3kXF8PHVaiUUvw1D6Yp7AEABpnAiUqxGm2RwEdEpu6c69tqKi9kZP/V3JOLsnvQWoyW613myZqAVqjKuzTmYAbYCD4VnTBkw1OjJnqXNwcQYQUWksqDCREwC4wQti7f3UqPHOp/EgAdf4wJn1EjEHJwYTbReE+QPyBt+6I0wmhdBUbpamOdqnW8B6GSq/24nkoUwpkfMpuKklVFsIZFekhhuKFtjBqjBC6TlnrXzhRl+L9PUndcqwqBqUG4sS1NW3YRbIrJd6VfmfU0rl5TrrDWEJvRSM8bsSRIRdzfIRCUHzdNmwZhF1TLK54JSl2B6sHL7xi6pxqz1XAXLJZji1fR3Yiwy1sMK169fw61bN7C1vY3d3T0sFluYqUZyYc2qK4Zn42QghDOBDS5Qr8cWl0dA9hN9b5tk/jq2zTzxf+v79wjKfm8JN2qw2hptQOdXLhnr1QonJyc4PDjAarkUxbx5r4m1DIJEDln53USEvu8xDCJBaSF7azjFSYDPKXnHEYHYO3SUUsefKMLaUE3Xgc0TKcSwzf6LHeSwA5w9E4lc2c/ut2UfsHroBoUIXFGrTmHnKsWjB49QlYrqkexdmwKU5UHNv0VHxiNL/ayNp82IVn/kfh4PvREmaNEAjJoCwLEh8QZiSgoJ1FB7E4uyON8pYvZDNWA1EdcYqHuEvqxeSB4akRye9rYzKMESH7JBkHvh1uGjGuCa3LIkBWDJseoNO4atYZrBD3bNNonUMj1kgXJjpDRTTIQUO3/WUkTfYtb3vjgtMWfdlj2J5k4jNc9rRp4mGs9QT7wobMHM2pLnGEQBfd9hZ2cHu7t7mM8XSF0HKqEmzkhxRlgIDb2Hig+boUbzDuSFsRtmx1AtXG0BETaDWseotrdvPEAy6EjehSncrVYrnJ+d4ezkBKv1CnnIal7EKozjukYJBgM097AZvbXJJptLKYnam0Ro0DkihTIVnoLkOLiAOCCo02Abfm2KCQxD7U9oHVtsYzJta2+qq+NrVfd2/5b8k+/p+w86bs18BGqyzt+BwUsgT9bZGrZ1sUlPa9cFTP6zZgBgm5FDbrohmWf+II6H3gh3XWqMm2G0BVZdRS5CYh7ElE5Vk1b6grLhWgWW7BBvTSayYIYaxloNvnFkm/P64pxMtHsc5hkG8qaJ5ilL2bF8yDoHyHPKVydUHb1/wd5sM2jxS3Lctk1aAhVjNEiiAAhK7QMRuCHzMwucwiwwRWDrbQakLkzG0nJozCI2FBrmBQCf+LbJWYLKh4bkuU9PVzg7PcXNGzcRU8JiscBiscD29jb6vkfXdd7fr+1c0oaq5GGsDroaXzckMGwbbnvFAITqjXHRBB5hzD4b9Lu1WGK9XuP8/Bzn5+c4PT3DerVCziO6lJzFEM2bZ2iuQPIFfd+DQBjy4FFRjIyuMbpuUvS5YkwY10OVJLWx0zlIysSRyabt3knvPARwrsVBnsCFRoOoG5Z1WTZPOVCG50dAjVGrEV0L28iGWUv3K6PF8Fx7sGpsrZcduN4H9JrCE64C862jwc17NXmCQAbroDG6EqWB2jL5+3s89EYYqCEtmB2OANjJ54ZV2o499QpFzT97+3QGWWscjdaISER+CoAspPKsYXCdoCYuUsOgcczeJaCFFnyyQvVNbcKG4IaIiFTvtKVAtTSblv41pQ2ZR2EhuXlotngrzln5k60nYEa6lAIU7YhrGJ7eT7IEpJ7bpnMMwXvfme4AM6PrOq+eEu9w8Hu2e7HGooRptWMd4xHjasQwCG9WKFsR89kcqUuY9TPMFwvMZnN0XULX9ei7ZCAFAFVWUyMiMbRGUGouNN/k2MM0a86+4HPR/oBjFvGiYcDp6YknXq1rMLExc2RDiSRX8iiINFlF4jhItBHRdb3i8+T3Q/bedUxMNxpEoqanyeKYVA9BNwVAntEq8gorbsrQDsSbHrZsYjDP1PRRNLKU3EqjO8zmLYf63EQeDdi5a4TAG86JetYFNvgweMDWRjQYirR3I1vvOjis1kamQel5Fk20FEbJ/ZTJ/Gwjyvt9/KUwwm50lClgCJNNSgD+MqYHIY/Fz1HPx82fJBxfW5RKparM5CasIkkciACPhmPZJDA3cFsn+wOMXL0vtOGWet9tyKw/a41/YdGCbUPX9lz25K0RZQityBaMwAHSbTkmY3hYEqVuJBb+ZhRf4BRDRV41W20G1s4vhskMbsPNbp41mOfd3L/T3NQDTMZhRj3/6dmplErDwm92mKrrxEPuuh7z2dyxZOHRBveeYwzK8dYRZpVZ1DlVchb2CgsTYRi1/18I7mWOefDeaQTSezWjBscziJQyppuwtPURofGgc0ZKwDvp/ZcZA4/ouiThfc4VzgEhc0bfd87CgL4HcEHsRAw9UlLqHyFyAQJhHLKwIHQsa3MAM4MVzoDKWGY1yr5WDCLTqE822vqnvuQKNagxFmMu4+IVn3plJo00S9VrIRDA5vjoPCNldqhzEqjmCmqFKNBKcVq375b5YxGkSwHc5+OhN8K5FEQrQ2yMIWtMkxtxHd3aYYkiL0s2uGHTUKLh2mplEyCLhMTmy6KVWeLesXl2gE0eeAcCx90aPEsEbqbXNUjDuLNTr1C8BPlR0yGEpACFSIo52GY5oEwCwECV2HgGLTZsyTYThSdiRATvSgI15BLiBcQQHBt1byRGjIPQ7cyQst4fMIUhzJAHIl/aBu0QERCt+oxqlAC4p1ppfg1HHBI6D0PBer0CQUTwz+KJv+O+7xsFu4AQk2CgSo0ybQWBccyIsrT6Me+dCBFRjGgImPWdeqbQGiI1LgyPHKw4wjbM0hQntOMyjgNCiGpsR/25fI6ClSXDyEAYhhF93ykzQDzBECNil8Cjoi8FAIkHb9rWo3rbNuc8GpIfNLkMXVdUC4YAIOpGMllLAKDqZLY7G+/Yu3VRm8/QTZYYnNWLVsNrG1jViagRra2lVhq0ZBOOtw2k0XexcdP1b1GiRGfjxBG7n8fDb4QZyG4NUQ0QAEBCvhijLq5KiclDLWQwCotsy9PzOwTAUM6uGDwrMJXPBBQtazbPQC7PKKyhb3M+q4SzpSihYdHFVQs1zIDq/AdguJz7nbCkGFMVIgKAgACmDAuozTszDz3EoNSkaXVhKx5khlkw3cG7UUMnb6XiKbbuOhgSHkv7ctOX0ARLsAhC7mkCa5QmxPXnreMRu847RLvBJcO4JZsfg6h/jcPg5a9c5FkCCIiElCRPkKiG4XlcizfKBawLuRDQdx0CqJb0ZhZeMkH6/xGp1GdByQK7eNWg8YABv869krk5F3/BFbaC09BS6pzG1Wo4g6C/Tx7NEIkUqmDKjKA4rsEPDEIeR+RxdK3t1luVW7P3qRua5lhIN2N7FioKTehztbmPrA1m3TAHgwgCQCxccy5AlrXkTTsJGpHZXK3RhEeJDWRnDUZrF+a2pZKtwbqh1vfRODFRNtO/Klt+nYeHMCwTqgASFhEw5gIwSWtvfRGlFORRIYIm/ChGB2rwsTqxqqdrYY1aIvlyFuqRlzibcWdJYonUo94vsZeTqisAENfQ20uGNYEBmXQ1aadulnr3pkAVWgxPvS2rnjOD7lgi9HdBMWf1/IQTLePYemWtoc5Z2ymlej00/0nijny8PfnXelTueaHJsAdfSPZ5O8zQiO5B8OrCYViDQOj6Hl0S3vJ6GDCMo+Cq2rk659GTMlD8PPUdUpfARVTEvPeg350Y2UIZISUXyzFIIefqnbNFH7lgDBld6jAO6wafrHzV9mdeHk/TeVg3wOrpC6zSVIBy0yhWP2MbZdZ28DU5qcYK0kDWaF0yLQSSaw2XRDoAaZWd3Z5Fm5NuGKUyJOyYJqIt4aYVgWZYCSIf6VBTTUKDGGXMk3NNICp99piiu0Jtdaht3MYOsjxEVRusycN203tAjvDDb4SBShcLUQxSigmDtp23gW1fjhg4w6v0P5L6+jZJZUZHDGrFlmw/rRNEs8RNaaV7PZpE8ASawiTVYAf3wiV0rVCGLUIAjvvGJHxZO1cgLfPUakGbTRScmev3Y5O09SKnyTXSgieNGNSwElfcLEZICyZmQIViisIUQcNIVsPHnGuHbQ3Nc56WfBftDSac1ur9W/a8Vg5WWhGPoxjmKIa5U2rWOAwo4+j3C313VapUz0kAjxlLF+1XDFO+1YgiyZFzRgbQpa7qH/DYJCxRvbJRuhqnmBqxf/XWU0LSjabljNu1bNMySlebhLV7XC7XPjbuaW7M727WY71e66bYVMzpfyFEMXRe6GNrROctGFwssUW+OTIXbRklh7CNxorPNgaTSRpnGuxV4TaDobJDJ6TJafdT9Z20xrw1kHfLEdRxslpR09Yg8F1zSPByrcokiQ5sDTyI4y+BETaPUDtAwOCAKRuiip3DF3u2nmzq/lhVF5iVbG6TSsJpw9F8NwcaowzFDuuEsMM4mmbg7J7bMBWbEEN9Ovl9DChEFv1PJhZQOdAKt9lXfUL7NdVAt/dYb1UnMFXMWvDsPKlwMsNp1zIPO2sHZ+8qAmDMWb1X/SD0+6gbnon/SFl5hZXs6Pvef08EkInzK/TBylRYrVce3UgIXFXKiMQwWqRiXhzFoF1ZKnTgnr+OkdD3AnIekULnTI+iXrHgx/4yNZyOGlmRV3lRs9BbaCKE4BKT5o0b5AMIzhliRN/PkFLyxC8MatIIj0iT0ZnFc1+tNZHM2lSguBdpxs7KmMEsCS5IFCk5D51b+mdp7rfOJ6UXcq1QtQhBoIKMKpFZjTFptGSOyOaY2DyzadA6DmjmiHm4fpjXTvBkIaHmGeSQ91HAGM04BwI/GDTi4TfCBEm6eOILNEmCtYvZXpp5jO3vDTd055hrFt+SDmjOVbgow2EDD6Nq7EqpdfGeQGo2A1Lv1xYTUS1XbhkDtpmYGDz0SS00i0bn0vuLSrivTREbo2ZzuL3n5nqWFHERZg1rJxzcQIipA5GU1dqm1Wou2OS3sbHwOgR2dRzLZrvAPupia8fUkoSCh8uYRMV485hBilXHELVEF0ipqxV+w6iiPfK+utTBJB8NagpUl4pxSc0xNz5q0OQks7IXCqNwhiXm7DDNDADCQNBCFC4F3GxmLVyzOTc8GjM8O1QBnxSjCKITEJPh5LkaqBRFUyLLmBUItGKepiVKwaxsDoXpDA5Tp0MkNi2sV/qcwmJm/ARrhWtgtx5sm7uQM9ZxYtbWWwSpemzgAftG2yHd5ouNo22S7flsDrnR1/GY9vCrzopHgGbg8WCOh94Id6kT2IEE4y0QWMEybGYMJVNfPcDskzKoSEuFDKyd9yYRvDXEbcdihyZ0IhRm96onoVKDW21OWFOMajcNK6Dwtkx+zXZy6+YfCMRaWaTusvXdazFJAChUKUEWKTjOCEaipNBcULyZqgdECh0EwjiYnoJ6txRExMVSIES12wHIeb3t/YzjKFDMBl5aoaNmEYao4wbtxis6z4GkrHdrawuG/xnuB8CNcTCMPAREDWMtSpkcGvq3kZT+WO8Z6v2KFrO/S5hnOa3iIhsHfR4xrISiYxpC8ORaKQUlF4x58NccqCadci6Yz3pwUO+dG4ZJA2tIfkErK0tBhu3TwuU2w+T8bMC7hrdQlaynJtGsUEikAGK4xkT7XbsXw51r1JZhS8jhFt9w4Y0FJE9g91gm52WuTUZbx6G+I1NUFIcsaKhWqz51Lau3Jcwkc6AejBl+6I2wTULDZA1rBeDZX5aYvDGMaBaY/h8Hze7XMKhtatgSuQ3LazP3bqgtnAqEgOjeTesRTrBYTA3NFztao23JOfva5LyNUWk3jwkf2BaVTbrm3qKqpLFuJK0BtvvMXLBeDsqUINVvkGv1fe+eFSBdmAszxlwwm4nXZnS3oq1yxLAps6LZtNqxBZqogAhJPWiiWgJtc4B1EStaUPFk5dcWEz+CRguNly/XMU9eFvEkSakZ/hCCyJAWK0QJCErJA9e3YOPtm7RTIqMnVCVZLB5zjBHzRY9S+powhMAM9q6WqxXm/QyCFunc0gThbD4DM7AeBjWWhBIIMXYouaAMjJhIkslN8rPdNNrCnXaeurGzuc8CaXAuyFQ/Uz13ckgLAIzaaAwfK4PGxBjXOVv/HUBUk+RsL7bZLDbXiPvfbPwgnmz0tsF5nuYBHg+9EaZA6tnpv0kxTyaYBGGLlZp3axq/RUV9DKaY4qT68tR7qde4F3bbYqxG7ZmG1RPsSj7pO3N7jk1vxK4VG+zPQjGj6JRS3CsTnFr67rUbRP2+8iQDlGYF8ZoU5zXCvicMFd6JMXoPvJSkQs2UrUTsR/43jo1OhtLQhmHEMGTElFTAfNo1RAofMFmM7RgbQ0IqtASntt+NDR3KYR+aQjFW1GKv1byfwozQelkhiOdpYx6ihQvgLNBD6gRbNknMzJa8lHO0hrfdcIPqeci7EO8xBgIydPMgrNdZsOeU0Pe9JAVLAWUZS45q+PQ6WTeWYRw9tF6t176JwqIGcYMVnhkqxIbGeKFCZKYNATe8AnONOTvGPlmHjQG0IhK7n6hee6HQODji2EyjjAYTRjXo1o3GoiCo52x9C6eRBzcISF3LFcuWZLU5YqwbSRt93e/joTfCTAVd12McshZLmJG625OysN7yGhWzbDSG9bwVn6uLddMwAq3HVrPaXq2zgSNvfrc1wJtYaBtiAXfrrpqXZockJdnhAxiuS4aLTb1Zy2aHBgNsSe+TUDwQKAb0sxm80ISF7pTbTSZqxp8qVahLnZxLRX8Cq9ENBM71WWOsHndRw2NGUbzrajwAaDa9jqV7yDG5V775O0kAAoDAH5asMQNNqN2yDe9kfZ/EtUGpbcymraEWRYoj5IR+TemIXTwslkhD4I5SMsZRchpZYZKUJGE8jIMb2i5piy59XtaEciC5x2EY/HmNdSGSqrrpu/PBLnJFE5igYvK5ifh8bgKqAKdrQpksRRbAZBxTShJxluKJ7czZjbmcMjaeLTxyiMGayVZ1N4Amz2ZrofpId/Ou0WweOos9QjVKJGty2jne/xvR6Os9HnojbPXxFICIgFIq9umZZDcoFs4afCFGVipr8sRA2mE7tuOpxXQGuAmPbdNl/Tz7RLj3OadenpzXhFPkaGvhW6/FqDdurIPya1E3lfa61vqnNexR+bPmsVtlUTsRPXpgiEQoEYZxqMwH9bAMrrANS3iv9m9ddGReZjVybqh0jBFIvEIY60CrvnThtQYYet8xVBaD9Taz7HvUbh7y+cpT1uGZhM2jSXPa+2g2LDZsVT1Z78LgGz3Di8D0nQbtvQdIlJChurrBNmihFaYQRUuCxDgDZtiDQAecwWNGzqKQlrokXOgi6malSClziMFbegHArEsYNcEl/G/huI/jKB6ztohqvXWDGVS6SgpZSPvPqcPiBkyZD4EIGRWeA4SWVrggkkVpKu/E7FTGad9DiVi9swkrbOFOiEEQ04iSdL4RyTwwZk3dOOpcqpEvu8NhVlyE6IXGmO6KVO/P8dAbYRtQT/IAbrgmGJH/CQCEUkwcRZNxTSLI9k/Z+TOA4Of0bLZ/t5a51g4Xlca2ef3NpE1dCPD7cUhBPzfBnlHFbfzfusNDjXLrFW9CGkl1ISzsJFI9XQ7S7aFMPaEQpduyeYmtZnCBdKlOjYfiwi9QDxJRBVPYn7/rOtkQUTcZCQkj8mikeuEjt3hvyxsOvpDgRtmfE3X8ojWctHGj+nvzNC1CsnMbZa7+XaMcNcCtQdAdxt9T5gKoNKJtmMKnFTGfmKrgDYUo1VoxYByrTKlDIe5hjhPZz67rdJO0HUXoc8Z1LtDnLgyEyq8OFNDFhCHXoiJABdzNc2RCIinsMI0Hx3SbOYwmqpqsR93lxoYZVOe5wmRka0O+E1W4v5S2StN0PABLzrVrwu9XxyDGqNhiPZy+aTuvOgW1OGYjsn1A3vBDb4TzmIHQhF0gT7RIH7QpLGFG0zLtYliy/949OvWCxJhO5e9aD1U+P9UnrrZhw7M0LxXw/lytQW7DutYotOcwQ9viz8WMdghAKc59bJNKdl43VuoREAijntd0bIGKN4sEZAGU8C/dNOrnLHsufdNsMUmon7lt+1RxUq9eKtwsSFLVuoAQgIAoBkvvt74fObdrM0wM4hSuAaoHHKN2AtHNLsaWCiWG0ZI0wnzQZ+cmdCeDAhSfzdILzf0uIi34EfzTSm+dXmZ/h3U9rjq2pGNvVZNZw/nWY5f5PqqGR8XSk+plmyb0mDOQiycpEZpy45GBQbu4cE3utsUxvkFpWyUuDG70sCfzcTLuxavU7Jw+voCyaio7Rp43O7RSnZZ6fnn+6j2TbXoMUC3lg2HPvt4tHcdwpgtQ5zzrPaO51gOywQ+/ES5gRGaEFNHHmWeUN5kLlrSpRokbz9moOobpAhbqaDNnrT5TIwijC2UYFc4moU0cO3wX1gIHf9P+mWl1Vhsetue6y+uVDwNq6CREg+NklrDzydd4a6jTUJIw9/AwBCtT795KozdaE/l9E02vw/UerA1FS5tyOCNUQylGrvjPZMjEezOc07WI9Z3YNc27ZxtPhMl92ndssRumSEQILAmezFX4xSADZimNFWiAUUg5p4VRNIS2jTKzFEgkxZBjij6f5HNRKFqaSAtqeHMenLrn87ZUKUagVsLFFEGlVtuJAJFi732H5XKpGyVJcY+WI5NCEpYoS51EF9bTUOZ8jcBs3gQiV9QbB/JNz6NG4K65YxWmrUGvXm/LfBD4TWifAAf2ZgqWvBP4EHB8mOEcc9kYimqF182tCkGRQ20OeakkJwVdjwSfj23+534fD70RphC8uk0SHVJ55EB8kazz1JuS7xoeaAbV+4R5OyCryIK8MGGWV+PUHIbpmqfZhjn+gsvUi7XQy47N8K4Nvy3JIIuswYR1sTBzxUBRPZuKHU8perDzgd24AqwJr2oIqSmptsKBVkaRCIpr1k3IvSwFAggNBY7ghgnAJKnUjkthuZcRotdgiz9SAMjU6uqzGgTEgRCYHWJwjrXCi62XBWZAoQcTvxEDIEau6zqdZOpxRoFahjyIxwzyjh6j6kwTEbpu5uNsuD2zaDAD8v7MxIbYlDKDcL5aqbj7TDfXyhHGCMm1hoCgUYF7raHzjTOqx89BYRqYbkRtMktE6FKHgQdYRaDNFU/YqVoghYCu71TOs8J85hi089ZEeox9FLTgw3yRcVyj3VislJy0lVittpS1JhCCRhtN8UaB8eArla3YRuyeFPnPHblpHKQWgpSN7a+M8Os6PGQn8xpYEzpR9WHroOfccg+biaSL1PmL7c9hLe4lcWIhcbO9uvdnIakdNgHc+65utk/gFsu0yWBHe64asrMbJfuOeR5mqI1TWknoQGDjEkxDM+cBa6kyAG+syM3522uIh9Q5P1ugBRPJlnfhGWjdeCQkrBoENpZJk1gSfKihClA6WB0DMaaESLXxpFtWGx/18i38nIbLYoyMvhhsUUOw/xArDQoongjLCj+kmBBiFE0GqJemYyj2vEw2vpyzaFp44CPqZV2XEKlGJCCRoRzH0ROXMSZ95lQZB8q0cXw3RqTQYxglYTkOgyq4FVAMGNZrgICo+DGM2cBQ3rZAEq0OwzgWWMdmZoEg2uo0MAvWywDFKh5v78D2Ouu1V9TbtXlv8ImNnxhgY6zAx7/1rH19Ac62sDVs0Yxtdtm8cla4kS2q0TVAJB2gdfxDDAiQpHQItXT9fh8PvRFmBlLsZT53wSesTKS1v1Sb5KWg+UxxT4kKawjK7iV5iMulGijAjRPaUF4NkR1WZdQWGIBZ6Fs+qeqkaT2QFpebwgh3e8ut97JZYVcM02aohgF5SNoafIkadIIbH7rxKuz71tZJWBEB2WlGPAkL1Tms4j0gJC0VHlRdzDxhCmIwbCMtXJDX2b33qMLpckuV3RCMSVHMoMNZDC2UY+9ZvNrK122VywQnDDo/KgtGBN1VE6EwOLDir1YRCY8GoEyO+XyO1Uo0jFsBm0CE+WIu5+HcwA+s0IvAKPLuZMwCEYZsCoCKubNWnjEwDms3okPOGMYRXdchhoCBlJZlHqIauBCFeUGazLMooO14bBtuNXbyfRfHIWjSz4xgUx2nYLXNU0tgm6l2rx5t4VKF9GwO27+rs2FJtsY50fls12O2CIugaQz5P660VYKIyW/md9o8wv0+XhPn4hd/8Rfx1re+FXt7e9jb28MzzzyDD37wg/775XKJF154AZcvX8bOzg6+4zu+A9euXZuc43Of+xyee+45bG1t4bHHHsOP/diP3dWx9WMf+xi+5mu+BrPZDG9+85vxK7/yK6/7AefzGVIKSMpHlZ21ehI2mVwKscE1HUcCeZLEfmdCJ14TD/UQmWvdPeCZdDPOWfmUdrQJPzunnFfVr1B3/xoWVw/grgXReMC2Gdh/rQpXi8VZotKev4U59GQykUUHdBLqG6NAdB5kw7BkprhEbSiqVKmGMlXLY3P9DNXko4mg12vVsbBqMvlPC28g+OuYrZhAPFz1SSV5Zu83JaW7TWGoVkA/a4g9jiY0I28lBKmgJNR34kwW7e0s45nkv5CQYnI4zKiThvPmXFBG9bA3uhgLNCBzto3Csr9DiaJqnmMqZzmWjNRJcUegiFLqPCzMjoEzCEKMYN8AZCPLrqVr8802UJvvNl+DbjxWMi3v1salRljO3NB1wc27t/MCBvlphLWxPut5DR+ulXiW17GrhSA60VFL6mtLJv0M109PIlbb1EoVj7rfx2sywk8++SR+5md+Bn/wB3+A//7f/zv+wT/4B/j2b/92/OEf/iEA4N3vfjf+03/6T/j1X/91fPzjH8crr7yCf/yP/7F/P+eM5557Duv1Gr/7u7+LX/3VX8Wv/Mqv4F/8i3/hn/mTP/kTPPfcc/j7f//v4xOf+AR+5Ed+BD/wAz+AD33oQ6/rAXPOGHPGMKy9KktCw4oRW7LOFqOR2fM4SgeInGF9p8zwwe2TeqDFPAJ5lUW92tZTpeYlTkNhnZ7MPnFb42zG18pfgYoHt+H2ZNemmniwRWMev18XjSfSbgIan4lnRaAQwRB+Ksxw0rTnnHnEU+hEcEUz7m35r3hqI4ZBMMdhGMRAeUKzhta2WRo2bx64vA+7ZkPxCyTcV+/mIPcYQ0QXU7NIq1HxyKCUyd9bj6gUSajZhj1hIcRpJxZ7VxYKJ9MwyRmxgXzAlfLIXDAMa4QYMZvNmrlYqxZJk26jzlGLagxK26QQ2nuVeS7hf9d16LTAxcaiZSpYwiulqHOmVpbafVsVom/cMSKFqeBQyw+2ue1RqMJm8jv5vUUP+nrvWgNtorO9dmtw6302EZ3OWeMl56y6z2SQm9ICde7bJt3KaxLqmr/fB/Fm/Poaj0uXLuHnfu7n8J3f+Z149NFH8YEPfADf+Z3fCQD4n//zf+Jv/a2/hZdffhlf//Vfjw9+8IP4tm/7Nrzyyit4/PHHAQC/9Eu/hBdffBE3btxA3/d48cUX8dJLL+HTn/60X+O7vuu7cHBwgN/5nd/5ovexWq1EsUuPo6MjPPXUU/hH3/atSF2PUswrLG5QJQlQk2aCew0w18sqe8CmQ1onuHeEsIUP1N+W0rQLgsMJ/vnG0AKVAtV6su3it4ltWf6seB1tGMLWgNg53IvY8JYrVss1udZ4K5Z5N4xY00z2QPoz1nLhqGugshXahdDS4JgZg6ud3e29edJNbgLG0TbjNqE1+Tjqh8FumNuoJYZQPSBWOUfoM3v2vvJQqyGz+xFalTXYNKgk54wYEkynsmzMiRAjAglrIYaAcTUgpiYSkZQkkkUeqFSsGESik9WAEpGKlEvvN/GEsxfWhCAtmECqx6FYMaNWuYm2s0EmwJgl6bler2H988zAtfCDJadbCKxlFzXuq38uJdUH4ZqMk7k7wpoJVDqY/sm1MMicG8uZVFYMTQ2nrzHpoNPmSapRlvsglTVlTfLJrVfeMQiwHNGketK8fTD++yf+DxweHmJvbw/363jdJSA5Z/yH//AfcHp6imeeeQZ/8Ad/gGEY8La3vc0/8zf/5t/EG9/4Rrz88ssAgJdffhlvectb3AADwLPPPoujoyP3pl9++eXJOewzdo4vdvz0T/809vf3/b+nnnpK7xMYx+IVVSl1E5DdPOFxHFwSEczQUhoP62FhD1CTEVwzrlKiKZ/zkBkyAUzqz3BNCynbxJ9NptZgApgYT2u0aBO9LThpjxCCt3mPIXrI324EflDl/KbYSCJSa+Ask18ASFECiJU6pXKFwXA92eiE8qTPimqYzEjWZ6mJvVZzgPX/7MnalvWlFO8VBlRNAYCwtbWF2azX+5JxiClJWxw9YSACxegnt/EQzyo4iyGE6H0BJSrwO4MVN2RuCoGazdXef9JIYBjWaBUTBZoR79y9Oa4qa+NYsF6PiCmhn820tRVUolIpXsossKakjBqi2z2aEXO9XDXS69UaXAjjWBBjh5Q6nzMpJcxms0nhjkVT5n3a37lZK/ZcQdlIxvl1GIKnKoE2mnXDhxvuqKyP2qDWqvgYJWvkyeoo5aboYnJQ/ZPqeNjlbL6aYrzdb3Ua9J4059NGGffzeM1G+FOf+hR2dnYwm83w/PPP47d+67fw9NNP4+rVq+j7HhcuXJh8/vHHH8fVq1cBAFevXp0YYPu9/e7P+8zR0RHOz8+/6H295z3vweHhof/3+c9/HgB8wZqxE2M8IufR/96gAKj4YDUarYezib1iYtjYPUr7fs7qedvZ6e7J0p6v9Y5bb86epX6X7vndVsgmkHhHQi+yZ6v30fKEAfMkoAI11aO1xFSMaVL+6tikhuGFJalhTGOpzNMqLRVzbxtuGm5qWJ54ps3YW5Shi9GMVdd10F1h4mmnmGBtnCQX1hRdMHv2PIaIPiV0MTlsIPujha1TDdpiGhZqUOx6WbFo9xB140mdGLG+74WuN6p05kayx7y6ie4HmvwAM87PVwhBPHCZXrWAw8ZEOW0TT5WhbJ8McIZ0DWd4pAIdn7b/nD+Xvk+DimzDtH+3cy6lNKE+YjIHLQdTizGqQ2Cbp77KBlar9zelVBpuL44Qayd08cQnvfU21prR1Or64Y31U3H+uuYqQ4eazz6I4zWzI77yK78Sn/jEJ3B4eIjf+I3fwDvf+U58/OMffxD39pqO2WyG2Wx2189l940eOo/jgJyLhmxZFimJRyDei4QmuUkUAQCogEutsGp3RTeSjTXnUu6asO5RG4baGGRG9aCAqbFuDaXt0BPMK0wXtn+ODaKo3US4OZ8seqVg6bNLHkzZDN6FAu49gGsCa5IcsWuTI2j+uxCCNMjkmviontS0D5pNevc8mD38N9gHajzcQKuX13UJYx7lfvX1GG8VarTGYUAXhcc8mJxjrM0wbTG3CVsGNMFmJdpVoyNnw6GlAjNGGdO+70EwxTnSZF1N1E6wW9scCcpthfNyCxecnp5hsZhjsVhgtVrXDd6iOe3rl8eMrutBsUgFG7QLBhFGK1xQKCnFqMVFVSGs3QxaB8SM06R3XDPvW9wWuhmZQE9skoSbCV8podfKSzbDd3cRktDZ2HFZM/pBOfmijyJQgicFm83OYI52vVY8PLgTVhSnrlTCou+isokexPGajXDf93jzm98MAPjar/1a/P7v/z7e97734Z/8k3+C9XqNg4ODiTd87do1XLlyBQBw5coV/N7v/d7kfMaeaD+zyai4du0a9vb2sFgsXuvtOlwlpP+sod4AoGLEUg1kuYOpcfVJ03iQ9buN8eEG0G+z1tjAfJsQq4Uc2u/UydN43Giub61fUCdZe19EpDSj6IbWgp6g3rpdt2VqwCAHvUdWTKx2U9AP6f+3eLNeXDBNGz+ILSw6mdvebOM4TBb4xDvkahyAWrjRmaFsIpM8jt49ZBhGx/AlvFTaXJBEYgiaGIvRYZ12o7OMPxGBlONqycYQQ4VW0JSQc0YerSy7KaMOVipOorBWCnJWSCtaia0aWhRQYcROIBJvy07QIgXg/HwJIqG45ZxxvjyXd6JY51iynDcQkEWmFEXR1gbWsfuzsPzupFxN1BkE0RrRcZxS1Xy8qMX32+hKNbibpKV5/PV913nbRqJy08ZQ0hmla60WXticDYjRcIbYqBoGpDiVs9SroAp2KUbMktCdSLo2zs5fGDhi8yilYLVa4Wu/9mvRdR0+8pGP+O8+85nP4HOf+xyeeeYZAMAzzzyDT33qU7h+/bp/5sMf/jD29vbw9NNP+2fac9hn7Byv+f64Cnmv1ysMwwolj+qRVoK3PIsxJQaAaoKilOxNPivMYMmR4iWiADxMNBUpC4UZ0ERNa6Jab7YaavltDYXajDORicygmdAbz1uKCKxAvGDJKgMpCYE+aDbbDGAIAVE90xSTVlaZU08+VmR4anNtolqRB/VKooaoLdVLTiObHvS5iAjz+fyujQ2YGgRSb9WNPtXMeOo6Z7OUorQriIErpQgDYz2IZ6heTUxJWzAl90zNANs1xmHAmLMLzST1lg1JJCKkrtPxJLdylnNYLpdYLldwGqPCLWORfndDzhiz3m+BF78Yc2DQrtBVuzl48jmEgNl84RimGTQuWmTBgglZq6Gi7wUQo+hwhWLZIqBf35XT6PwdB+/e0VIk7eB7zFv7eRWOqviqs+qZtedgZSy5Z+ssDanmI4J78rkUN76maQ2Fdey+jX8t66biy06TDMLbF2dAoQyqlDiZb3VeGMvmQRyvyRN+z3veg2/5lm/BG9/4RhwfH+MDH/gAPvaxj+FDH/oQ9vf38f3f//340R/9UVy6dAl7e3v4Z//sn+GZZ57B13/91wMA3v72t+Ppp5/G93zP9+Bnf/ZncfXqVbz3ve/FCy+84FDC888/j1/4hV/Aj//4j+P7vu/78NGPfhS/9mu/hpdeeul1PWCBVPGMeYDxSSVMHPyFWBjGKq5iRs4mLIDJC9gMxza9V2p+ZuyDTQhA3depN9F63RbyKwTQGj0zRLa5uCGPEbENDQE3DqS4myzWyuZoPXmCKJ/Zs7mHqj+zid3i4ka+T031V/FxnG4gIdT7tcVTSkHX9wAz1k2Jso2PfT9vGIaiHrAlSUspzuWOQfUbTI9Dx8FKh0fTNNbu26ZpLPiuhNPWQZg0VC5m3KBJOohRSDHC+LRRhdZXq2XDtKhJQ5doZACQSMOKZSJFdfTkPOJ1sUMaxpTIOWO1WqGfzbC1tYXVaoXBSvELg6jqNrTzlqK0ujc4VKQhyVkGmYurlbWdYezexUO1SIBAVMfdoiHbcFp81w6ZW3Ve1oSz/NZgKnn1xT18yS3UZqTyp8xBYzC00SKrkTd9kFLES3YvolmzRecy2Zy2f1OFv8whlwKevwBSltevX8c73vEOvPrqq9jf38db3/pWfOhDH8I3f/M3AwD+9b/+1wgh4Du+4zuwWq3w7LPP4t/8m3/j348x4rd/+7fxrne9C8888wy2t7fxzne+Ez/5kz/pn3nTm96El156Ce9+97vxvve9D08++SR++Zd/Gc8+++zrekDBeUaw7rZE7KWmhRnBsNsipcemCeseBczgVuNTIQp2iJ+oCkLbMcVpq8conw+NgZzyH21i2TnNQNuZa0hetQ/cG/HFaJ+1n8Oz86ywQQjV2IBVs8Q3lKY6sPFUW6waMM+nIFJyz9M84UDS3dpw982iBvNmLQnXdd1didBxHJGisBSS4rAt1jzamGu7IhubkmsVownN5FK0c8fgHmXf91gNayRlQggdTOaAK3qpNoE9vyRzGT3NNFIh0WboEnIeJYFJMi5j1g7QDH+H4uk1m6Ua6poYraIyQQ1NCxuJ0V8jpIZBUAqQFPUPAcTCnoDnCyQhKVobDKCquLFuBCEQhiFXnW1MDZy971rEUSbzYZOK2K4VLrVJ6OQgVB40WLx3xXzsvi3L4BCNeueMVnLTbxBgxsijev/6/EwoqOLxdWM0zLlexaFGVDiLwSgPyAh/yTzhv6jH0dER9vf38bZ/8I1SmplHnzTSaDN76G4eCMCSmCKtRmrO12K9QOWEAqhJNcj8KRZmmTfYeMotVtx+1w6fxKVUA7wBATh8galXapCI6B40v9f7stLk1hupOJkkKHKr9KZecwzW6aFseLbGPIDyosmNh9DW1LOLWrJdGLFLXqV1Nx4JVyTzceGqAheJVNPAzhsVpxSqWx5rQqV644IX930P6HdMTWw2mwktMYvKWGHGOOj51QsjInRd8nfrz23jqFoTqetxvjxHYdG3qJuZbLhehm2evFPRRn+/plcB5cmGEDUZaJukGW1JTtkmE5OM6VopljFF5LEaRuNHywYlzVerh65855RQuDIiAGAclfViFDTvW1gjwYmh/XPWiCvhNVGkG1RYoZC1jzKsWueAGmVr/WTPM+Wu1wVlCTYoBVA2+doxhGuCAw4s6yqQadys1yY3MeaM31dSwv3kCT/02hF5GCdGBxAgnEMAaxhXZQO5wYc2PD73Ai35oJJ6pqw2CaignYOqd2vnsJ3Yd2P7rzGORASmhk8MgUpaz9nONxE0b4z5pB24LsDNJpkt1GKawF4kUhhjFm9IGm5aY0aDLWTR9apV62LuqpFQuKBTJgGYXWe3qH5BHkbXZzCusSWyAlsDSPl90t8NearQRUQTo2Gi5TXpoz3bGo/I1N3m8znW67WEz+ptMRfPvIuQPKHrhWplmhkGU0E3ulk/Axfh3sYYEcxzcm44wFR1q0MMjk22lKh6zYo/Ch9bx7yhj3EpCBxq9+FSRXwym6BO8XNJ0GS4LLsQf2XNiPc5DqPDCqVYBBPAXPvT2TjZ/GkhudaZmLA/mo2rXUvwzKK8Y0SbkzbRK5TTXlPObam6BvpgBlFEjHAY0PnRbJ/WYh67X1/i5PkhTzyiRqUyt/FAjofeCAM1pDHYAU1IDED+zeo56bbbVsS5YWLLqAqnFtQokRn8uGG8W7zY8WDcHeaZwREDNjXcFoa137GqOQup7Fo2Ge27+uh+r6xUNatoEkNgco6G6QVfdIY1gkQ7IVCcTNJcxOxZAsOSJgStPiyK3ek4x0AoY3ZMe7G1QAhBq7ayU6F8UPX5zMgnOxc2F3oQb1yfTxpOEyyxKfhtj5wHFdFZTqOTQIgk+G7JRZ+VkKIk/gIBFCTrbuIvgYIwI1JClwjDOHpCNuuCbkN21zuGRRb1fToNrBTQKKLxrYcZmnsVD0+YEAIbMEoImPU9hqIl9tlEcOx9ZaAQmBhlnRE7TURG6YC9HgakFD2KacfX8V4GmLMaaGXelDJBGKYOj0B/+uoB1HO7NjKRM1AE/42a0FXlPp/ztvE2OYdmzlv+JZCx1MUQj6jddEyzOcQWkoBGBsVCnQ2Wh3jNIUhS9UEcD70RZhaiur3ktlGh/L5iqe2uDtytJ2r2jQhiTEpBqXME1sfMQum6g4bGOFdjDmzIYpLyRdXqbhpq8yiyT5K6NU9CZNTFT5PrlskiqEa2qQCUciRAjRcDIjTFBeCaDDTqko3POI5IfYe+65QjCvW+JKGXNGRmsPQxY0kGSam5GKXNJpq1Eo3Qd71vRmZgCFKgkVU/2ENPg5j0U7JBKB4Lwnq5UtlCoU5x864EOxcPKcQoXn6G4KzGfiMpgwZEpDxGeATAnDGoRwllp9hrLAzkIWMkUhlLhVcaz77vOlBKmkC2SKFVDTO6X0HkCAPBHCePEbFPAIThYWXfFg2Ij5yxXg8IFDCbz7AeBuVMC4Q0nTsCz1WPsK4bM7IThbyJA6LSpbpANmE3SXAW0SYpWddQqQsXqNi4wUzq0U4gPW7GiWukggbTdUelYZNYtCivXaNJxYftXjzKDIT4pZPJ7nn8JTDCjZFFxbSslNiO1oBthllAJXC34QlvXsxefOO1TqhWaDHgmpWFecgampnbYNKa9nM3irrYWo93888JBMLi8RCiU3Pq84rhNW/YvUO0uDEgcESY9Olr/4RCA+s1Y9b3uiCEPRGNXE/AqJ5zCIRAtT+cFW8A8Co3u5cQxMutUqAyzklLkrP2RGvZLiyWCkRA6iIIEethqU07CUWTa4BBBBFjHlWPIWg1mmw0Ld3JPx9rF+NhHDCfzRBiwTgItCPGOWjEofPFklOw8lwzXPJEpUgX5a7rxBOm+k5qwUDDkIkBEUrZKvoeCiN1HWZ9jxTjpLgDqB4ssVQemkpc0gIWAK4MaB5wXSOAxe8+xmT3zsoJrnN+E5Kw+djCYEELqaoBN0ekcoXrZtDMgSbiE6NcO+LEGMDu7Ng9V2693bMVI8mGDFgyMNh6bO55U/3wfh4PvREWvQhdCM71IxUxV7qLJsEq7lsNcGuYsmW6KUxKah3VgFHFSBJdBm2Y0TZcSo0TQk3mTbyEADDX5EmtnlIjC/hE4VKVvmzRbC6CEMgTTYEImYxtYZOs6DiZWhmDC5pz1vbo0oSzGhLxhAIoF0Cjg7Ozc3R9wmLWI8Xg4yLnEWU1K9awTamtvCulVqp13Uy8Yv9dqQsTpILlynxRetrK8Fki9Vi1W3RWaKQjROpFJW8c3dhCR9fgCQpAGeuGS7r5UQgNLUyeIadSiz0ynHsbtIU9F/I5Jh5YQdLrdikqzg7VY9bELrJ2hana1l3XSRm0zqeUOiBLA9WcR9noxgFdP0NKPYgE6qnRl8zNrhfO+OnZGTjrJq9zWZTqquKZrAWZk+27Ny8RgNPIvO19c7QOSbvGmKHGT8banGDStRFs3qgxnpy2OaclLRnstrk0fSEl19GErO6M2b+1RJoI1RG3/IY5G/grI/y6D2bVO+UJ79d2WgvxNvG5dseWj2+8xGayGU4sHqRMRvv0Jm7ZYrYA3Bgayd080jZJYB60ugMTA2sJm2mVT4MTw7i2au71XmM0XFc+5SWqFCBBgoxN13XIHMG5uLhLYZaFDdbmn2IEop5b7lUkEK2VTu3QkKVEPAQE1Iw4oFoGXSdAAxH6KCF/a/BMqyHG6kXbYsvDgEJBMd0Aaxw5jgNKLppoZFAGWI30fD4XyKFMK8S8pJo1CafhcwgBa6W3WfeMQNYUoBeNY+0YjFChAIsufGPU8TANk6hiPDF1Xk5dirRrYkLVYiDCwKbuV4AMxVWnxi2PGYgyf+bzOVbrtTBBhIeIrp85BGHaukUZPeaoNIvIjW4b+vvPoJoU7hjwpAzsrjnZzF9xSKqUqCVU7R1Uo8mwrix+V+4VN1ElZAJOI1uFMbiuX5lylVshgv8EUpU79UIE/iq1o/qDOP5SGGFbuB5CTcJ2+6e8QDPAbYVMHXvxZnKjySuThWo4j6nM3xSjrdew79o5LSlYPT2T76sT3wzxZslom2G3zcA3jQk0Qpq8kGIGzrqAGmZFKTVkLKUInQusHqWekIQGVQZW3LIqZ/mGBIFSzKtiRPdKYyKgGAe6TER0ALhWrvGGSR+MiND3M/f27bk8aQTxrFKXvINELlnpbTKepgnCWcTObbFbIXGKwvYIFFCIMZuJmJFtFsMwIKnwz+Y7HpX5kRIQi+hlqM69Qg7kGhQWJXOzCck5hvpvBjKkj9toiTObO6hmsozZOepBi0dyKei6XrDjEJFSB9Z7jDFgtVwrDBVUAVB41cQWBQSPwESEKkOabypbg2q+QaIBcWslipgsGh8ne0kGP5RiSTSF8YK9a6WeEWBCOtYpBS0mHG3sldkSoro/rFADNc6RlUqbp2yJ3mQL2SNL5uphMxffPDc9/Pt1PPRGeMwZiTQ77OGgFVvUUklAxE7uVmOy6T7NyNp5CrddKip8sRl6VU5t9ZLlsyaKAjdkICkxBiyBxuoES3LCPDBbhQyp6JLCAv0J2QahLAgS3YSgJck5j0JQZ6kq9KWgngjQskfkPpbLJebzgILiAuGyWMxTJJ+0AKTxJZF6ihkZo4h/dx1CUw5qRko6cth4BqzXowiLoyZJpMll79oP90pE9r0ojnkVXZTNJ3PTripE5GFEAdApHhxCwGI+V8WwUTxEkiaWXS9MiPl87sbYogfDtCtUI8aewcjqfZE8hLwba23MtduJzzvz3gzqMoOVja87FdSxQpdW4LywafbK5XLOSKE2BRiGAWOWAibiqgzHWUJwE4Fve7zZxiARWk022nqo05G8VLlWXBp8BNm0G8/YcyVsidUMUnaOVTCayl7R8asQoMGERVdo8TVBJM4CqTGd3CfVDh6uFU0CZVkik5WKSaRwUh4mm+X9PB56I5zLCIyV9mUJkM3EGwwSmAhD19387trxOiHs31WyrxY0+L8d/gCs2ah5v9aaReExTchlN06WLDDAKpqxNSOvdLEWvwbgBRu1bxjVdjYs14Eu9PqoJJ69ykKO46h0L5m84zgKw4StHNuKI0aEINoTScNIyZpnx/S61CEk8cpyUbFyNFxMqFgPieShMXNDJJVilAXcRisi1WkQhvBNuQhLxbPiRUT/jYdszxJSJ/cOKVIpRUrcQ4xSSclasQZgvVr7fAgbqmvtmFuyjtX4hhCcbw1IZVoxNkcDLbXzkcTqqfdYIxnbVE2AyqIgIq6bUggYc0aMksRLKamwUVYxd8IA5V6H5trmKOhmbXNBns8U1VoDWJ2R1qBO6V3NZ5gdJnJoTZ+JQIhEKMYtEvdV76k6SRWWEC9deP21yhQwJ2UKx4lBnyajeYJsmONhHrV513aPOvfXSzyI46E3wo4V6ai3WC6jAO4V1xdTiuFRNlearDzXSh9QxaXGzOj0hQN18rW4s93GpHLI77TFqismC0/vtfiwTly/R54+Q6o0OC+isA1Iu4UwgGIemV9X9B2IOvFMTexFcUQxlNkF3OueZNgb3BDbQgtJMv2Fiwica1+3YVjBdDxKsTJqMaApkW8gdo0uJWQ3zDVqsIRK38/ECGRRUmNNnhnOK1VrEC+sSfpQaDjRIAxjRgzArO+l8CFE/5wk2irFL2wYHXuvIQR0/Qzr9aoJvXOjMwHXuE66ERivFqjVhOYJey6jECJkjIvygU0/ISWhX2Ytt+fCKDRCaLpyXmtsYGp0pWTH9iWxl914VUiqtpoy49WG5S1O6tCEz12bufDvm+dpZ3CKpr5sieZqwnMTbrPkcV1XtnH5NPbftQnq+n7Q3E/1xAFMPjfJ5ShsMcYOD+J46I1wLqUKzzRsgVaZH4BuiyJKfi9qjlUY1ZdGXpdf8SOeTDD5rnBRyUJ1m3BADRkbkxa0xLd6GJhMnnuFRC0EQk2VkU8ogw0AgKRiymCEtthkYtSLdfCtItxtcpLVaLcQjVCrIoaxJkwWsznAjFkvAk05m6RlbYAaU30fCJZIrdzuqFAHWHBWy/Z3XUIIogUhRjI7vJTH0RdXzln4tyR0MvU5HdJhAMzSPgggLR5QHqt6ZakLCDORciSq5dXBy2Lhpe6WWRdhImFI2P2kaDh1BIqcq+v6iQFrN+kp9q9OgG52/q4DSfFFLiCMMO3iXAo6nRNEpAUqQFSPmovQ78Yxw7izMhal8fLNY6wFJq3Xa/M0xqji9fWefW7Y3LZ5rJueQSkWCZFukrJR1vpTc3jqRmVrgWAytFbKbHziFqayRO50PG2dGIpc1Puu/fNqhEbABnZ/P4+H3gijsCcKpqE6IRQgc3YRGzeibJ4nKTzhJRiwSeBgv+NewcN9v07jMbixD/LCPfwptUpP2r4X9drqrrw5odrJsLloAfXy3GOMiKRMCK7XZjaFKShm1/CZSf5tzVDtPDZJ/d82DqRtjkKUTSwlMGuPMw5YrVdYaZhuYxmDtB4SeKQ4hQ4AEKtcKEGLJlCAwCCWiirpvJuwHkakrhOvW3m5ALvHH0KNTsRD7d14mOdTSkHqOkl8+ftWLwtinKfdj4NvHO1mJ/q9xedRCIRxZFF80xA/hkp3s43NvE3jhRsLhKi2FLL5ZvOANKw3WlvRTYljkOa06nyIYH2Ho8NjxfGLMjkC+q4TLDSKwcxZJr/pqrReocMKPu8ah4YLxAFvqvpQN20pxNHPk3Jy0Wz6lqCbwHb1HI79kyXPNP+BjVZhCmVYtFGvbxWvNUIFTAtF16clKZkNe1OboesF9dnv9/HQG+EaFtWXYl5FYUIMSXd/uAE2A2qVV7ZLb4ZqAFS+kSrBG82LZvaQyvBj58wqrcsMk0AH2WETWDJHLujehnu9G3BHG3bJHal3CSnTtCOEKCW3ypOOMXqXW3kk9mvLea0KrcInpO6jtxgHwbp4hFD7koGB1Wo58WKowZ/HUb0UDTFrIUjwst0QguCobAUdAi2QLpjZbK4LT4yOeHCSVBKoQ/mi8ioRQ83oAzURS7p4rbdZ0XQlmJFHbcsEVI8qEJiKdzn2Ba/FB8MwVJ1lcaRQcsaIUYwjkW8+AZpEjBEhKQtlhCaKijZSNSBKpnPOWTpjqIEruWDFKxk7GWVJKKYO67UVpqg2hjJ1csnouwSEgJLhm64VkogDqnQullL9TdYB1IM0rQZuDODEizcvG9N1ZM6BVGXyZCzb+ayzV+7FIIxCgFHjbJ2iTBygmoSXqCoEjUqIVAS+0ZGQq+r7lchK77Q+6wM4HnojzIpDkf7PlJgMc7JMvHxWv6OGsArewA1ya1D9GqWAA2oJc3NYUg56fZuwYu8Ec7QwTaplNfRqoAJLZlSbzFULWH9n4fum5wxUrC9rYsToZoa7yj0VHxurcPPQEU1ZqqeaNYQDqTcVfdGawQtkRtISbgZziLfadZ16Z0J509Soe5KA6AijFMSkYXPrBWpRiY2PLHCFj0iSgnXjjZoBT47Tbm5srnEBuBee5eXL+9N5IvoRFQJyr103LXtGp00Zo8F0R/KI3HyXVKozg0GlGi52b17wbKNglSyG2ch/DmWBkEcRoje2xDCOGIa1SHauTdBdEmV5HDB6hBgw6zssV9YOSrxd24hsQhgEULdk+O9t7A2yqHNVWQYEF36ygwCnxLUQBzCFx/RCMEeVAyaCVK1jYgGbRRBZi1lIqYFkjJsmH2Jj3pbjbyYX77W+78fx0Bthcyd9gVakCcyqWtVgSPYya+JNJ6PupPW7clbzCCx91las+U6PauBJS2ktJrOFaOdnsLdjdw+Xq3exmY2Wa5gRmVbKiZGAbzaAYK0lw7sXlGLhWkDRcBUw0RdZ8GQC7FypUMza4WKCV0voXkuJNVHXSQnzmEcwS0cJAmMcVs4m6Kw9e84aDhdPCBIzaG2bAtUNiuAZ8lpkwM2zb7JUIkJI9go1IhLjP+aMra0tGB0sKr0u6nelIFCz8swqMl/D2Ok7IcUV4dctucCKCqRCT/uXhYAxS7BrVYk2p2xejMMoUAssYmPBtrWaK6VOrlUYqY9ObwtEWK20ejBE5NJLVxmDNdSgQce/6MaYi5RPW4QhOY2qWmbKgaxuua0tOy955AOnq5keB9X9FZbwNAepfVft/G7/RBNJlXatbhyVUqdj4Y5FcSfLnZrmfiZ5EbB2cUm+zh7E8fAbYX3R3EADNSmAu16yLCh4mFT5s9WwhuZ88om7KW/tdaDhUWUpiLHy+zBvnOruLveOCRa1GcpVCKFWxY3ZcBXAhM79vpoNSTys6M+bc0HsouJzBUYDCkFCc8NJHSLQyjoiUUYTyEFgnkAAJelzN47nyNmqErNABUWeqXh5LOO8iJEaVqLfMObBcfZxHF1lzTB3hoWz0LEzX7gupopXN63MoaW7QQsn1ICmlHB+duYt6EMMvlGFEJD6HrkAfUqCszfYcosX26Jvr5lzFiPARbxgLd+uobu8Q4BEyKbpxCKJt2ZeBYHQEhEG3axCIIxDpSnGGJH6Duv1WgtfCrqO0aWEEYysG37X1aKhnOW6XSe6y2FF3unEsFo3qDK1HKKxAoe2EYBDCQYvePEREFg2zdxAFjbJJxFm43wkbcllhpvBqroXhfWi0ZNpurXj18ITbTTcJuBbLNt+F1zDpOqFPIjj4TfC5h155FEnyb28Sp80etTwm3zxW2LEDav+vcVp7VqtRyz0m/qyN+vv27BYL6kPYB5hg8k2v79r4oPufq4NT8O8diLrfKxhvVWUoUIpsTHAWTuUhCDdJ2IKSEGwOuMLj+NKPe62uaMkJ8/OzrQnnAixxxRxcnyi3a+lM0cu1RO2iMN4vGZ867MpXLKxEbawTN04SJ8j+cZmnn2IAV2S5JyUZxunOenvBULpux5d16HvpaW9GXCgCpd7AcXGhrxejw4JZUuUMgAEL5IACGUUeMvevm3UuXkmcTd1o1cdYmujlVJCHjPKmLWjhyQMY+zQUSeRhWekxLhmZFAhjyj6XlTrrN8cT9aFTkRmbeRa/y2AWmNIFfoRI98klI1lYfMed2tLBEuIEfl7V7QOIKguiV1FoJJEtTGpjUW7Sd6Tw48aOVkEIe8DMN4wM3uD1vt9PPxGGPbyqlaCL1L7nf5pL9eSDwYDiM5Cs6M2u7WdY5OxAKAxgPXvrZdmx6Y+qn3Gwi0Lw7nIZGeFVWRycjPp7PYCjIVghkd+3gIjQT3X6MbMQmso/9aNXQiYzcTzS4haLCJQRN91GIc1hvVKCjlKxnq18g1quVphGNYouWC5XGE9rDEOI0aDHJjdM3ZPxta0bT62gRomaJtYs9gcE9HxlyQn3TWu8mZDs5FJE1SLlsSTD4hR6Hb9fKZVcnIfMXZI2kuu6zrMZiIw1CVt+hk6ZSSkyUYNEp2KWj4vhjOb1GRpKFEMcC5TmdNmxpl4vXmiwlwRi55SUlGjUWGEqIL6hEI1seQYtkwFdSZIhZUE2uo6bWzKjDJxMGp8Zkk8mZcFRo8zo8kebWIKzamL6cnpSN7WKWuSUhGWKdyg0aG0IavQnidGFX8ztToAXv5uh+UB7PNmD3wd+XubanaH5pz383jojXDJGSF1sAo0C2VsMU9KjgFnOwD2siwpUzGsSbmsGoH2JbeHTyD97qaHVo1nLftlZm/jE1LSMFNuy7x1T5zo+rQqH5ksm5xI81KLh8k6OkgxakVX0UVAQLSKPNFh6PsOIRDW6wGLmWg3CESQcX5yJNVoWtZ8fn4unYK1MGBQelbWbF1Vu2oXlv47SGJLOk/I80hH5Oq9WIKLgQkjZRqBFN9UJJknRtmFxEGTsRszgCw83uJGbwUggE5OJBRNFV4QA9Wj7zr0sxnm/Rzz+Vz1lEXGczaboZ/NpaGonTME7fQcwEUEezCOkw3cytZZwHofG+sIInq/8PZGFETgBzEon1q6MjOLpKWMiyQ1EQJSJ9hvLgV938lmWCrDo8JjNq8iQmAMzrCgyfsyZkfLjPDIhy1TMoXUdGUoAmNmW95bbmiD7RqqHmvLpRZnZMy1CaxtSL5OMY2cjE/crr82Kjbv2+4DrHALwaPE+3089EZYduTqAbdehWkq2Avzz8NKSgFrl916zJshjf38rms3oZUZDcImLcqMQSXCt9ABF8XVgInB8VY+fDeeZTBFG961+sP69AqPQL8XtBy3Uw85+J/zWYdhPSCAsTw7wzgMOD091bbuS8EeVRbS2s8zM3K75DTEC7pRSEjfYzaboUsJs9lMDFc/c7W2+XyOlBKWy6XQu3I1WKHxZghQ9TLh5ALSy24cs4f/gBiy9XqNwgWr5Qrn5+dy/6sVxvXgMIJXCqr6Vh5GDAO04AY6L2R8UkroUoetrW3M5wt06iX3fYf5Ygup79GlTvUsah6BTYHNu1iwUtKmQvOCvQvLReD6rJ5CB4rkYy6eeY9hvZ7MLYNIQhRjRUH64TEPWKu3HCzR1jATTBujS4oRU/B2VCCNUgq7QbYoMrjxbRNrupkEhdI0giNIwkxYIFaa3yTK2wiTjdbZcJcBV78r/lnN4nCzJhViqOuhJrNtfbTOmK+n6j9JApru7Wh9qcdDb4RLYWmhRRWra7FSm0AtXBAMa1PBkNZjNczLzEtriNtzAkbLmsIf0OtV7VYr1jAD6meW64cmcYBmMyjQ5JJUCFk1k0wuSVhk69pgRpxZE1H1fsdxVNwzqAANJouyDCOW44jz8zOcnp7i5PQEp6dnWK/WGEZJ/FhnYwu7LRyMIaLrO8zncywWC2xtbWGxWKg3KYIyxFIebQlQG+u+67CztYXlcgkqoiqQSL4zjAO61AEk4bPxYhMRSpKSXjBjNuvUCIuCGxcA29siXwlgvRJDbLjrarXCar3GarnE6ekZVus1zldL96Cze4wAkJGzCBOtacBquUYId5BSj9QlzOZzLOZzdLrRzGZzLBZbsvH0M6Qu6XnlXcQYFE7ogBgdT2dmMBFSUBcYAAVosUoEZTGQORecDmeCVbEViggG7Y1TQxR5UdP2cP+UfJ6DyDFUZsYwriUiShE8WsRRqX/mXDSLQI1g45kyhJuOhnWkkBMUBrFEZSByNq5EH1qpauuOK+PI7hcU0CXjCttdVScrxKjdXkqzxsPEGYoNHGFHUF5yRpGNIv+VEX59BwkGaAZC7Cj7wDsetFEV5Mk31MSBhSXm0Zoxnhjexvu0IxiM0Bh8MbL2UsXTAWyBm7c1pej4+ex+0XgaMA9ZxG8ET6sLRaEtTURpB2H1NjrtNxY17OVRqGnHJ8c4OTnBarXCMI44OztTBa6smKtNd7nHru/F2C620M96LOYL9LMefd8jJWNiSDEFoIbFvAwbM5ZOGIvFAsNa8OOghSxRYYF+MYdJh1piihRXlnZH4mlNNEBUnWvWzxCgnt44es+6olDD1tbWZPoM44hhHHB8corjkxOcnZ1jtVq5YI4YBcZQBhDJecM64uzsFIdBkn3z+QLzxRzz+UKM82KBxWKO2Xyu9LKaFJb3m2TTcIjACk6sCwp5jiOGCAoRyAUFhKwGchwzgs4pCsk91DyOyGP2DbhCbnotvrtgQrjYspFJlBMQSCIF69RiGh4qMC1zOJgH2pbDy4ZrsJTPUEl0eBJMoDVLvEl0F2xh6LlMCAsasVoDVdGOHmz5Oy5vuuFt1NhGuFY74MwWqLAQq5rbPczL/TgeeiMsOJpNDH2BEIpMaGhn7hOYl6y7bTHvDoCpnwLVMOtFJru0/d4SWIVNjMRePPxzRuvZZEi02VozoC3sAMdNje9bJQ5dk8KJ/jXE6vsOfd+h66IsJE2ssLIRzs/OcHR8hOOjE5ydnWG1WlWJSQHhfJy6vlMPd4GtrW3x/LrOx5xYBVeiVbhJhVkpjBSDLlw5nzVgBQGp6zAM0odOoWkxpqRJzDGj66o3jZREU9crvYpGIcoWUF4uTFNBm6QGIhRibzUfqDYkZX3GGCPm8xm2d3bwVP9Gx7zPzs5xenqK4+MjnJ7Kz8BC/+Is/GwRbB+xWg84Oz9z+GW+mKtnLOPWz+bo+xm6FF3TwbqdEKwkXJgrOet8ygVMhK6L3uXFMVaKALRtkM1dEw4qDbfdvsfSxcOgOTvcGDdRYovPCgWyoKjqXfBzGlsh+BozKh1DDHKgiiVXQyvOieRSFZrQ52KwF+qYE+UQnRlk9YalY7fK1+ociMZIaZ69PWw9WsQazEkikoIq1tzBAzj+EhjhumvCPFKNh0zG0EpvjWNomWKghikAzAq7B2zYqx28cWGNwmAUKnilUTW+EjqGidGtiQfW+6ywx1SgxBaYTFQRJK9iQrYAROs2ou8TUhB9BZKBASJheb7CweEh7ty5g5PjEyxXa6m8GqXVuZXydilhvphja2sL29vbmM9nmClNiwAVAk+wOwshousSClvDzoaLDUt26rMReWnyMAyCVTbYHBuGVwqgiRjBk3sVjxmxWp6Di7QNgnqQKQpGy6UmncQ4RFAXgBHe0ibnQQ2vCtOHRoUrSIdqSVIGbG1t48qVKxgG4TAfHx/h9u1bODo6VrpdkygcB5SSsV6tcXp6htRJ4s6M8GIhME3fz7C1teVCSDElp58FxYsRCKSbYkjaM1AxeKmGNCH2ajG8gWqDiwJwJkVRjjbF2ijVvlOdioYC6KG7lGkHqjPfEmMMmTc60VTPpCbp7DpAC2dYVGc7/cZabilssDUh69nyF1xUJdCrP6MzLUjHabMcWlozSXVkIEIwVK0UFJ2bhjc/iOOhN8KALj4WI+iTsKGbVGx4+jOQGZPqaYoXbR4h3bWD1qwsTSZLQC0WKDAjXz3raWLNPA8ptwxabWQTyrQRmIEYAYrmiQQM2lk6gpD6hK7v0XWESIxEjBQZXEYM44iTk1Pcvn0Ldw4OsFyupJAgyzgBQu3p+h5b21vY2d3Fzs6uV+XFGNB3CX3XubcsXqfigSG6zgO7RCILBNHMZU9yxqiiNlWzwyIQ804DBdH7pcbYaHPJmCK6lLBer2E6CwHK89XETdZyXgnXBzEiIQontBTE+QyWLCpFBIE8LNd77vsZQhjc84sxYnd3B3t7O3jiiStYrVa4ffsO7ty5jaOjoyZZKRV2VAIKZ6zXK5yenmLWH2N7exsLxcuX51tYbG1hNpt7YpIAxX1lSKy6cCy5CgNZolI3LRT4xuOhdpOAts4lnlTTucx5g8ZJ2uuw1NZbVv5r+G8byYHC3YZOHZ/aXNdYChVKsw3ZknK2xMg6uhBAVPFgWdl2n7WIKCukYnkH6ZoiYxI1em2FgwS/lqhC8OtptMdFekbamn4Qx18CI2zYaIURLLkGqruwEegNmmhVvSp9ZQpFmDdQ6S+yExsMAWBiiOU7RpFixcCm4V81wFMxHtJa+ZpcUJZBP0Np7iWl6EaqiwGi15ORSEKs87Ml7hzcxu3bd3BycorlaoVRdRKslXtKHfb29rC3t6cGYYbUSeg/Nga17zoEBKwgPdesyMEihHHIYCq+QRXPQcpYdZ0wB2z8rCpOCj2kO0Rb0GL80flcqF/jekBIEfOZGE90M8FnRzGSBVUz4/T0FMyMnZ1dyfp3HYb1GiVr2BytfTwjhgQiqRLMLF531N9HjSKydp6IUb4VQ0KKUnG2s7ODxx9/HOfnZzg+OsLt27dxdHiIlRakZOtKQhl5yFguV5gdn2CxtcDOzg5mp9Kkc3dnVz3luVDi+plsIKqZTCSMg8BFNiOF3biweNFjTaCZZkPrARpzwsT1y1i1pi1sJwAIkiwTaqFh+1WxzsV72GI/OI1RZDSB0ahvth4JboCFeUAKjcmGAytzpuqgtMpuQTcH0ijNcGczuoVHqX4kcsfJmuva/dpmAFQqqpXbO884kPPZy19R1F7fkUtBCnHD2EGxIodbYVVgjsdasYN7qsbIYd8hxUuVHZqgLASbMHkqkWkehXvcVL1nAJVdgMYz12jLuiUDjJIrFt11HWbzOdbDIG3LVbchhYBIEK9Un/D05BS3b9/GnTt3cL48x1opWVC8LKUOu7s7uHTpEnZ2dtH3vU70XDUpctXoJR1Ipso/NpbFOGaUMkpCx0o9FcATT4S0b9qoOg3RQ2AZPFl9xu2OSYoGSD301bBWqEeUrlbLc6Qg1LrUJd1MM1bLNQbFis/OzrC3u+vJKIoRXd/DklgE2zdNmpTcIxKesDY5LdbOqPP7tdJv+TshxR5dN1PIYQsXLl7EarnE4eEhDg4OcHJyIpWBOaMUSXKWPGK5WuL46AjzrQXm8wXOz04dqtje2sbW9jZms5l0/ogBlIMmCBld36nhE1nMYrobhqEahxeVjiXPXqvJBAsnqKAEwKTGG432Qo3IbDNmqrkQO49tyjmPoBDQheDc8boNkE8Ng69sPOU0QfVDNGGXG2aTfr+lmLlxJkzyDYGkZJxUDyQXoealDQlb6dY9zfdYTQE4IP2VqPvrP8TIVtzKkl3ijcpyNqaAHYWLZ87lZReVADSMCEqpgUxeZt8x2+tO6HDmXVuoBcO32AXnXXAk1ITCJDTXc4QQsFgs0MUOXdcjDwN0jiGFgK6TBM/h4QFeffUqDg8OcL5c6k7PoCjdKGazGS5evIjdnR1sbW95+EtBKufIEmiobAtrkbRcLsGFMazX2NndFe9TNWnZUA3bTMgEaqrea9CNZxgkkw1fDIRACamXsDtEadsefFxVijQX5JHBuWBNa2myGRO2t3ekgGRBuHrtVayHAY9cviy83ZTE8FnFWAgI0RJQ2t1DMcxRN4EAUV8LIWgkEDxszuOIk5MTNwJEhFk/Qz+boZ/1CEGSlYv5Aru7u3j8scdwdnaGg4MD3Lx5E8vlCmUU+huYkcuI4XjA2dkpzk4X2N7exvbWNtarFU7PTrHYWmB7exfz+cxLqnOOLkQu7YAANR8qVM8IJTi7oj1KKaL/rDoMpF07YiC9J/H+qxethpc9LaavTqNCMBDYISsgun5xl8SImbe9kUXxKLQtkDKoIRfTp6jLtHBBpOiQCAoLHY2Ue8+q82c5Fln08t416VedM3OOppx7eW5LBv2VdsTrOlpstg3H6m4KpVaye2rmCVdaSs0Sm4gJMysFCCAWGku7K5vegOFbZu+JoGwBm01tmNgkrjwEg2oQiGeWkjTK7JJwUVOMWK+WSCSbRpcSSmEc3D7EtevXcevWLaxW52J0QkDs5DO7e/u4sL8vHNb5TPq/kSb51PDbZiVj0cgg6ko4PDzCsBpw8eI+5rO5PKditmjw1Gp4q38hvFjykNhXloaps75HgRj4wE3RiY0NEZhY9YEFqlmv1+AEpG6Grp8jdj3++l/fwfnyHIGk+SaN3HizqYGS4kSrAJCNqItROkvrxiMeWxCopRQcHBxgsVhgf39Puhur131wcAd7+zIuXRcx0lpxyg5b29u4fPkRPPbYY7h165Z4x8cnUl1YGJwzMjPOyzmG9RrnZ2c4OzvB7t4eVqsVTk/PsLW1hd3dXcznCy2wib6ZEbFqc7SVcDK4optR5Sqh8zFYyyMuABLGYd0Y3WqQ6hyv3SfkT4WOUMC54q5yzViTgyFiLFVGwhOvgBtdO3IZ4Q29WKKbceTG6w2NQQ/Suy8UBLaFLHNNNJfHJgFeq0oBc3zMwRHnZRwHTTRHhaOECfIgjoffCFNAYfLBtwXmmd7GGENx35ZmZhgW0bRrhoerzJJoM2yZCMU8LMebpWOC67PCKry0f5d7v7WdkBk6hmwSJizT9T1STLCKtmF9jkQCTaxWK1y/eg0HBwc4PTt3Xm+MUfipijnu7OxgPpNM/KgshKCUshis5TsABgokqWmbT0pRw1bg4oWLCBSwvbXl3RgAIFIEo3qGxRgLegg2WFXK2vdh31kN68l7FI8vT8LOoPoaBhXZfa2HETEl6UuXC+azuTbHNC52rpsMVDkM0LZKcEqhWgWMtos295diQuYBFy9cwNHRIcrWAmmRwCVjb2cHAOP46BDponiXKSYRdFdNYQTGzs4OthZbeMMTT+Dw4BDXrl3D0fGxaG0URh7XyFmKClbrJU5OT7Czs4ud3V2s1yssl+fouh6z2Rz7+/veTdn639UGo3CDJDBaB0tY27y3dxDdUEn7Jd98yHIi7BS6ls7ma61YI1kRfjKqms0Do8i1Y4kikIZAXtnhicLsDBFWY181L6pLXEpBpI21DOiGAiRKnlRs2SF2TxKlVt1tc7hsbnoiD8143sfjoTfC0O4PJuQCwHd895CLlNjapGihCZPbc84kGgNu/9csUGat0rGXCfGUSYBo3VlROYuo4tPteZlZeolFMTiLxaIawHEAM6FkeYHn5+d49c4d3L59G6enp1ivR88Qz+dzbO/s4NKlC9jb28Vs1ks3XoUzVuu1q0BaciXnjAz23nzy0NVoiVcbMJ/32qiSHVfzj6tHbeNpE9nlB9UAs37Wfm96EKTet8l8WkSRi2rbavIRpTanJMMxuaAMA5gliTabzWSTWq8ALkjdTBdY5dGySmIyMgpUl4AIpgyaTHNDGQeBCJQSEoArj1/B0dGRs0kyZ8xmM1WMGxHAIN04S84YR9mUur4HZ4DLiO6RDhcuXsDx4RGu37iBO3fuqCBSwTCuMYzAajVgtVrj/PwM2zu72NvdU0619Nzb29vzpqaZshstkGQ25O+1S4jIlSas14PTuJhVQxpWNZq1KSpcShSY8tprg1CtSgMB4wgOslFafmWq/1A1jznUNcXQUmQiN5L2CzfuyrwxfEIEkOBQS4wEzvB8h7RXqs1TgakWsWDRVWTfIEG7tjOjpujJfTsefiNsEL5OBA//UT1NG3ynRwENPizGU3bftqKteg9EUPFzS9aRe272GRCQQpLMLVmtkKpIcS3QaEV2rODADAYXwaUjAYEJy+USN27ewMGdA5ydn2O9Wsvk0gKDne0dXL58Wau1ZhrqFZEAJCn5tYVnzA4LH0vO7vEAVskEAIRZP8dYCnpLVBWG0cJsoptnYc/TFqfMZr1sQo0RpgZz5JI1QSQ6B4LjBRBJw8aQEsDAbDaH8a2lik1kNhMV6bemNL71eo2un2F7ew/DsEaKKp7DEvovz88BaKcFpfqVyaateHguwsmFJO9JvdqUEra3t3Hn9h1s7+6IR0qE/f193ciMQSDKX2fHx2CGVM31c4AiAkSr+OLlDrv7+zg+OsLVq1dxdHKMcT1gUIO8Wq6Rx4Ll+RonRwJRbO9sYxhGLJdL7GxvY3t7R0PuToxXES0K8xrMyZA5X8XkC0shDDPUM9Xy52H0d0PEWsFYqWI+xw0YsGcmOOZPhCaZZxBdpX2aCH5Qz5iUdbJJdTPMWKIZUxHMGmnaPLOedlLuHLRzxzgOCCVM9DqqYVVNCmVHGCzl67s8GCgC+MtghLliwU5Po5ohbkMjoAXjCS1NbDOJMLmEhjPyffW0c91F7Rf6E0DLMc0AtuGReYhWDdZ1HVCKaPZCynZXqxXuHN3BrVu3cXBwgLWWElMIWGxt4cLFC7h06RL6rpcig5hAVJ/dPN8sqjSTUJV1YxCakOYt7RkpIISkDAxyTmkIUdoAMZrxMgUuskFBP5956WtmrqR4HV1p507oZwuMSnEyXJ4Lo0sz9PNeSftSIjyOa6W7dT5+Ai9kF/CRBJqUnsYkLIKYkpSkhhFdLhgG4w0nHyPhxorxLzmDtAdcySJQv1otMZvNfHbs7O7g1s1bePSRR9DNqmCPnJcBLp6AHYY1YiQsz889QZpSD3QArVfYu3ABO3t7OD48xNWrV3F8fIxxlAq8PArLYMwj1sMaJ6cn2Nvbw+7OLvKwxmq5ws7urgshRUSn/kENinT6IMBhAG2KCi19LtklXd1JAbSZq3y3XU+GHccUwCAU73ZdHZ+iZGuRhq36FBOsn6dFS+1nfG75+hXefYhpcg/GbrAgVeYAdG0pla0pC2/XaVAFwWnUWx2xB3E89EbYiyIcO5h6bC0eJtFNm7zTyilIiJuVx1hpMjrBULE0SzSMG1lo8zZYPXCa3Bt51Q4gntVisUCK0nIoahICzLh9+wCvXr2Kk5NjrFdr9SgJ88UC+xcv4JHLj2B3b9f1gHPJ6FJUz6EIJhmsdDd62xmwFCfktvgkBtROXASiiJCS8oWr2Ld5OU59gpHldXFEw9aSh8aB2Rd5jJJBZwCdSmXGGBFDEqOTs8tZ5swSCehC7TopnbZQlUtBHkXKsbAwVmJI6LokojdGLWPR8s2F0c1mCF2P1Wpwz5+IgGCNLzO6qLii4qyWCBzH0T3eQB0euXwZx8fH2A/7IlsJ6H1IVCHcZPHYDw8PcfniJaHYxQSijJQ6T7SlmLC92MKFCxdw69YtvPLKF3B2di7GOI9Yr6U91Gq1wnq1xPL8DBf2LkCoZwPm8wW2t3ekb1oQrvOYjaNc5z8lAg+DzM4QAC3acUEdhmtEWKFR0PVktMs2jxKIhHHhCW144YjNLaOvCUwAnS/aXJMqDHYvGNDaXsk6Ni+hEeSi4B67LapqtO82pi1cZtexhHHXJYUobFO6/0f4X3+kHr/4i7+It771rU7kf+aZZ/DBD37Qf/+N3/iNNRzV/55//vnJOT73uc/hueeew9bWFh577DH82I/92JSrCOBjH/sYvuZrvgaz2QxvfvOb8Su/8iuv+wHNzrb9oYgUq7IdV39uIufw3a9W21UYws4ixpepqjnZ7m3XapkY5iH7v0MAs4V48n0zvjs724gEYMzoFGe+ffs2PvP/+wz+zz/+P3Hz1k2cnZ+DCVhsLXDlyhV8xV//63jyy96AC/t7IkykScguJsxmneJfUEhFMLlSMvIohoch9Ch5ZkuOiJQmI6gBFkx6UHUuVujDWAaeXCP15Pse3az3DLNgqeJNg41qJ/eZUgJpY8oxZ+WU1uKM2HUg1T62c6WUAAJOT09xfn6uoSQ7DGKhKDNjuV4LZLNeOe435oL1kHG+HoWFkqLwo2NdvMKa0Pfqymay8e3s7OLs7Aw3btzA+bkax2EQ6CIQKAYUgakFjiiMPiXs7ezi8UcfxWOXHxFPuDNmyohxHLTMvJdChBgxW8zx+BOP42/+rb+FL/uyN2Bra4F5P0MfraPHiPOzc9y6eQuvvvoKrl27irOzExwfH+LmzRs4OTmGIC2kCV6BY4rqU7g+A8P1I2JMUkkI9lZcVjZsTkxlKDQwBFtirmr5GuGhnXc+tqr+Z3xtywe0TAzPkbhdgUJ+AaaTbZ7qZhKQSZLLQNOhJGfvGAJgci37M2kHcAAq9NUweO7z8Zo84SeffBI/8zM/g7/xN/4GmBm/+qu/im//9m/H//gf/wN/+2//bQDAD/7gD+Inf/In/TutKlXOGc899xyuXLmC3/3d38Wrr76Kd7zjHei6Dj/1Uz8FAPiTP/kTPPfcc3j++efx/ve/Hx/5yEfwAz/wA3jiiSfw7LPPvuYHnGK4YlraoZRa9iktDCoDaApQpHFNDEZRqzivvahSapIHgLe8aV+wmjXHGG3XlrBRdAkARrCJGwinp6e4fv26lhYvsdb72tqSIoALFy5g1vcIKagebUZMXb1f3cFJvRJG01VBAwNCgaxDqcyz6igbN6LgnKKs1WjiKUtCo+hmhEBifEJA6jrELrnAO6DKZVm8GC5ZNY7lfRiGXpOTQYRu+pkbNysBXq6WWhWmoj5ZC0aChJLi2VQdWkC8OGnZvhLMu1h2X6zPOEoVGhse30viJ48jwKrFawL7xTZfwiOPPIL1ei2bgFbi7e7t6oZRqWA2/biIZObJ8Rpb21vYU9rZ2dkpuq7DfG5ME/gGJXMzYXdnB4u/9tdw+fJlfOELX8DBnTtIY8ZatSnGseD05BTr1Qqr5TkuXLyM+XyOYVhhNayws7PrXiiBpJmnztGYksMltUDJ8FvxkCOgTJfgpd5m3GsizwxerTI1p6SF3Wz+lTLt7m0brGH2IcjcCzwtCLG1ahu8TNHaQTvmgDEPCJTcE7PNwO7TnD8z9DIvK/PDipSs4/UEBL+PBzF/aeb90qVL+Lmf+zl8//d/P77xG78RX/3VX42f//mfv+dnP/jBD+Lbvu3b8Morr+Dxxx8HAPzSL/0SXnzxRdy4cQN93+PFF1/ESy+9hE9/+tP+ve/6ru/CwcEBfud3fud/+76Ojo6wv7+P/9ffflo5qRr+uzdaM/EAJgbUkhYOO/C0jNjSarZAADWvzJj1M/EyjPricIddR7OsGsJ3XULf9QhBMvCkxnO5PMeNGzdx8+YtLbLQJFeKuHTpshQfzOcSYmsGmIJwYbuuA48yMZkLuhSFouWShVU0yDz4pFgp2f0hwNokpU76dq3Xgr8mxV+tmqnF6oigUYYYVevyYInRQFEns8E/aMaY3SDksbgiGyDJNWO1ZBUnD0SK+1Y9YjgUouwL3/zEm4+x03GqRH3rqC1ZcFKWSFEqICn0sHL6lCxKFWcKAonYOx69C7Jcw2VCQ8C4XuH89ByHR4fy3Bxw8fIlrFYrgLR6jQN2d/cRlHUyDCukTnUqguriMmO9WuHmjRuCF5+caGv7UTQjgnj1i8UWLlzYx/7eBfSzGba2d1woKIRawWbwjLwi8SDHPCp0UTypaBWUtglbRSMA1cZotSCqUZbNvTIUWofHPGuLMHwOhDBZX22vOwDuqVrZNUBIsfOfyX0yUojQNK9XYqZQtYRzKcqRJ2QrtlJbEWPVwQBE1vSDH/04Dg8Psbe3979ti/5Xx+vGhHPO+PVf/3Wcnp7imWee8Z+///3vx7//9/8eV65cwT/8h/8QP/ETP+He8Msvv4y3vOUtboAB4Nlnn8W73vUu/OEf/iH+zt/5O3j55Zfxtre9bXKtZ599Fj/yIz/y597ParWSyazH0dGR/k0NCwBNe2r/NlWOgu3A8pfNHdYMMNB4tSGoEW/QHMOOS8HQwCvGdGC1vKwvuO87dL30KwOL9xtZSPY3bx3g1u1bODw8wvlK6EcpRly4IAm37e0tpC4hal+urut08WR0MSIRYaTsWC8sLC9i7NoWR23oJ89mpa6hGQeZgExA6JKE2YUxm88mMMR6GETHthQwrytkodlyqBG1pGXO42Sx2hjHLOpnaxVYB9Tbopp0SZSwXK+FAmaFAqWgjCJTmbpOudjyboS7TIipc1HzEMUrlNsXj3U9rGGGnFh6q1l4yuqtZ1KjrfDOahxgOs5srJYQEaIsZJlr8jpKzlivlogxoYsdTo6O0XUJ62Ht9K7Dw0Ns7+4rNjni+PgUFy7sW/4UIKHdPfGGN2B3bxd/9oU/w60bt0RusRSMJWvkIHmD87MzXLh4CQAwrNeYL7awtbU1Kdt1Q0w1WSeRRfSxFc80KowlRhNk6myEUuiuDRVsSbKNPm6oORVAtVuy4bvFvVOHCJTeV7hSJyvjxnDgpjoV5HkRAlfIhKDaG1V1LliE26x7m5Ot9/4l+qtf9HjNRvhTn/oUnnnmGaHD7Ozgt37rt/D0008DAL77u78bX/7lX443vOEN+OQnP4kXX3wRn/nMZ/Cbv/mbAICrV69ODDAA//fVq1f/3M8cHR3h/Pwci8Xinvf10z/90/iX//Jf3vVz93gdFy7IKr0qxPKaFRZ/qYY1rRzgJJSC53wnQL8Rwu1zANzDtAmYUsJiPkffd6q7MGCWxOM7OT7B9evXcXBwgPPlCoMyHrbnW3j0UUm4dSlW8RYixERgzkghQpIRRj2T+6rdIAwWCZPJralCeZKYEFMvoTBIFeEyiCXE61UPAVCxnVGKArJ6iAbPVI+mahwXlxFkzczDWQ0egjb3OQwDVobfNp072mVgFWwtnpdzljJV1Ygw0R8rmUazUccUVQBG3p1xZVMnRRcjZ004MnIOXiYs80ESnBXikghCO/4ovTAgc1B61AolFywWCzz6yKMYVOvj+PgIZ+cj+r53ylQpwDgoxMEZ87n8rrDqeHDVAu66hDc+9RQu7O3hlVdewfHRCWIJWI8jMhcsV9JcdblaY7lcYW9/D7lkDMMK29u7KrifQCSiRJPmqQo5sCbjHIYDgZE1iUtAJCkfDtp9xBJ6ZviidoRuos1amdiwIIo4SSEkgccILuTk78zpZZXySAhSZINpB3UYzMBAIS3ZLkHvyYqpmroA9YKttL3FHwTSezB4xGs2wl/5lV+JT3ziEzg8PMRv/MZv4J3vfCc+/vGP4+mnn8YP/dAP+efe8pa34IknnsA3fdM34bOf/Sy+4iu+4r7e+Obxnve8Bz/6oz/q/z46OsJTTz2lCQhbuoZNCebklWJUJ5gJ8FRiuHJZm4WO0DYebAD9MN3x+74H1HgkbdWyvbWlmqUF4BGBgWG1xM2bt3Dz1m2cnJ5jPYqubdf3ePTRR/HoI48gRkK0Mjal2RCx6u5GIGoVXh7lI0kaP8ZIPuFJoQsPudRbKGZwmbAeJUNvYXfJkgBLfadVeWsfmxACIiq+Zx03WMn2hqlm0nJfgyRiNZgmkg4i39SiYcoxop/NNNFYF2ItP21wX90MLHlmRt2U2WoSyswmIYwV5yOyqIXQDUlD7ojt7S30XcKwHrFarVT8R2lwzMh63kAmQCMnLCxiOFFaGYtaW4yICUjdNpgZp6ci9B5jxNZigfUwYDab43y5Rj/rdTMo6PskMFPfV22dXFBYMN0QAy5euoTtrW288uoruHHjJgD1inPGqF1S1us1VusVLl/OWCwWGMcR29vbmM3m2mlFwv7CohkSuVaJCo+cnWInuQwTa2IUHiGa2YoHt8UcYxaIp5Qq6NQctn6CyU7Cgjj1RKnCG/b+wVU03jqotL8vrN46yWabEGFmtYU5uEhTXYOuGPaM5BEw7Htf3Px8ScdrNsJ93+PNb34zAOBrv/Zr8fu///t43/veh3/7b//tXZ/9uq/7OgDAH//xH+MrvuIrcOXKFfze7/3e5DPXrl0DAFy5csX/tJ+1n9nb2/uiXjAAbxS5eZScMaJCBxVgb3wqlkxxUAWtQfGnoJ6kvTST4ouu7lTlLUNF/SdcX9FtTb7YuhjBZQR4RM6i6Sv6AUdYrkeMRWrcL1y4gEceeRQX9vdAxABn0ZFtNGKJNRcWxBsWzNS8ioIRMgHReDcyr2Rjsaw3AKkSQ/XmcxbvGhDcrGgDyUABHKa0IYaEeJLcYuHfaqxgDThX67WLnRsFre979LOZyDfOZg3XWiu81KuyhWglr21Ya/fgOKMmnQBtbUQEIJjKozMvrDO0ZcoB6DMLLVDzdTg7O8NisXDetuCEwGq19BlkHn5KwnhhnQdcirRaUsufXFOalZ4lHvDZ2ZmWKw91jEsWhkJKoABEqsUOOWcUAlLfyyirut18PseXf/lfw/b2Dr7w+S/gfLlEIMKQM8Yi+PztW7cwDGuFtbaR84j5fIW9vT3MZr2kWTMLNECNTrHyta0sWmQ8o3J+rZchAUEEpCy/YOMDIr//FrJo4QPTfa6xpsam3HrOqAY42JquiURSVUOMUlkXNGnIYIcM6+YNrxJtq+M2mRKA8of/onbWKJrtvdfxiU98AgDwxBNPAACeeeYZ/Kt/9a9w/fp1PPbYYwCAD3/4w9jb23NI45lnnsF//s//eXKeD3/4wxPc+bUcRAF91yuwLywAkFGX6qBLN1lUHYcmceAJhmYChcYzg3rSIBOrrrxD47ESySIfhyUCCVPg5q0bOLh9iOPTM8GRA2FraxuPXL6MCxf2MZ/NEMDWKcxLPEtp6HTyFgBU79A8BYNbiFQLgLUe34yT6sSO2SqUTA2Oq4cbgi8JgVXiBMMFSLsZmF6rXMOMHKPynnd395w14o0+Qe6lDqsVcinIo4jOZ2VDWDKqsHopVLHtlvYn5eJarYUaulIQWpYoggnPeX9/H5cuXQIRYRgGnJ+f4fxcOi+vB2FKxCT6ymdnp81ckvZQRNHbNgmPOaKUJEUoWpZdxlF5xaRiT+ahC8YaKWC1XIpYUc6YdR3yOGCxtY2SpcqMUgdLkBpNKit3mkKQbsixA6s8Iwi4dFE0Pe7ckYIeA89yKVgPaxwcHGAYBlHP2911j3N//4I4MsMIwuil9SlI5xLb/CQ6VBBLN0XJhwhEEUOSeYF2Ay1NdFjhPWDKMALDuzq3SfGJ0QYEr9f5KB9WmhoMG87aQDaIgE/J2jhgWoUXKSJzrZKzw6KppApyDwYNluM1GeH3vOc9+JZv+Ra88Y1vxPHxMT7wgQ/gYx/7GD70oQ/hs5/9LD7wgQ/gW7/1W3H58mV88pOfxLvf/W58wzd8A9761rcCAN7+9rfj6aefxvd8z/fgZ3/2Z3H16lW8973vxQsvvOBe7PPPP49f+IVfwI//+I/j+77v+/DRj34Uv/Zrv4aXXnrp9T1hkKwnAiHAvEIxZOY9mgFt8dwYahcCA/6txpz1HEatCSFqJlyoYaRAf9/32NrawmzeY3l+Ds4iAn52eoZr167j9p07WK7WzlG9dOkSHn30USwWM/G4A0uBHTVda6EeBVgTKHdzN0VSsuj9aUcEHQ7pvis2eDUMSsTvELkaNgAIGvaisJQJQzaoYcyeCEvNRpTHjNOTYyxXK4QYsdha4PIjj2I26xVLTc7RNErX4eEhlsulUO+08MHoaOaNT/BivdYEn7f/ULF8+8+MdOo69KnTRFyHmDqtUEuug9z3PRaLLc+2r9crnJ6cSteQItV0ADAOA5ar0BjkGRisFWwShqckSb8YRfEOYJRhAPLg1VsMoJ/NEGLAyckJ+k60I9ajdOAIMSHGTo2dsAvWS0lEAqzzU4Rv+r5HCDOUkrFcniEmxsVLF3Hx4kVs72zhlVdexXIl2sqj6nGcnJxgHEeslkvs7UsScBxH7O7uYrG9jRWAvF5Dc4rVI5aBV2isGldhJwRgzI6fesk7SaNWIqlctAhsUpKsh3mmNhcntMUGbjDMt3gSr1HXU8+7lCxQmxp3MAtmr9e16j9SGAL+eFV1zQ4uNbl3v4/XZISvX7+Od7zjHXj11Vexv7+Pt771rfjQhz6Eb/7mb8bnP/95/Jf/8l/w8z//8zg9PcVTTz2F7/iO78B73/te/36MEb/927+Nd73rXXjmmWewvb2Nd77znRNe8Zve9Ca89NJLePe73433ve99ePLJJ/HLv/zLr4sjDOjg5enLlj5YVjtuvceqF+nQg3m+qH3Wsr1QDVcBVXHSxI4Z7r7rdGNhrM5OpcUPA9eu38CNGzdwenKG9TgixoSt+RyPPPoILl64gBSDsB5YsrrSZEA5vX5dC60AMCufUVuDq4GUbLxlkkmz1HrDgVR1zLDlCtWYnrDAHQUlC64rmKnwmYN6Oedn596Nue977Oxs49Lly0Jh03ON44jT0zOcn5/j7OwU5+dLqfBar7HWrh7C4Z22JG+hBs9UN/9ujzZ0pCbEbFWwrGebGGczvh1SiuhSJ3KealRTSpjP57h4+bImOkUz+Hx5jvVqBWR2TvYwDkKHiwlBuc/DOCCsEyhE9E21nRiFoqXgEfOtOXIeEVOP+WKBGAPSTOcnk+LNjPV6ELEfUnGbAHR9J8UNpWA9DNI3EISUenABZN8kPP7YFcwXC3zhz17B0dExKMs5MrPDMWulql26dBnn5+egQNja3hYoYxiQh0ontKNwwz1n+OQkCtJwM8u9E8ENnr5QlbwzY5fdw26NrPy7oVLaem42Av+OrgdG8USxreHMBYENR4ZDGbKWTXiowZpRBXtqUlkcGhofjIral8wT/ot6GE/4rU9/5USlLHh4rbtltiqemoCwLHnrWQmVqTZQjNqbzDzpkCoVre97zOdzEBhlHBACcH56jlu3b+HGzVs4P19KiKxh8eOPP46+75ACCZ6Fog1C5ahJieCTz6DtQFGNDiBJr6jerpWXBu9DJpV5QTYS3VRAtSBAFK9IlMK0hQ5IGB2dqnOdny9xcHCIk5MTSH+1Xezt7UlVmxZvrFcDjk6OcXx6gpOTE5yeikdpvNSS84S1AdTFZdxjeRy+a9FVvK4KxbTft8QLmj8BwJp2Gt/V3nPFeqNKQYregnRFXog4ey8dTGZ9j3EYcHBwB2enp85d7vseBMmXIEgRS4qdRBhR4A/ZpAnWuglgzGa982j7rkNh0aPI44gUE2aLOVarNYb1iK7vJPk4Zvm9aliEJplk4usiMqTcdgDL1RLL83NcffUqrl+/oRixMhkgDWLniwUuXriIRx97BIutLezt7mO+WGDUYhvpxJIbz7c0f5eRF1gvuhys4e1BKZ0yoWnyPtqjNcTmnQYSGKWl0LWQW8s5bis3WZvLGg0NpdWP0Q1AjW3SKK0V8pLO0FANEfndmDN++yP/9S8OT/j/KYdp1rJ7gjWsNfpNu5jd8AKTsKite7eX5AZRF7MZap8gJN8/vHOIq1ev4ej4BKthRNf32NnexiOPPor9/X1ZvCWDuKiCmGFpjREh0hlRn03wVQ3LNGwjhVZk0gcJv5wvCZTMYiSU7G6Slzog3m8uhYA062VhF9ZuzAfIOWNnZwdf9mVf5gmrUop6umc4OjrCwdERzpfnWC6XKsIz9XKdBoUpNgh9Cxb2WZhrEJAMBCDgvQjp0+Sbd29c9r3AdUxbdoXBNzEEYSQoBhuCdanusZjPMVtoi/rFFi5duoTHHnsMZ6enuHPnDk5PT3w+dX0CcgCHjNRDk7syH4gSQkwqvF8wFiBGKRRYDQOG9QqAVGuNJaOcnbtzAIXEKBQ1YAH9rFNMewVAPMComwBzBiDtpmIK6BRymS/meOWVV3G+XAFk+hkZ52dnKCpk9MijjwIsXrAllPueFCpa67NOvWBhGWkSzdYakXSE1rEJqJV24R7vyIypGeNAVlkakGIrrDX9/L3+zlw3aiKSzCbEaWb14q3oqK2YdbqqwxoSmaSY8IDQiIffCKNI2+pqAIQmI3Qay8RWqpphiYZHkbplzDXstcSWaUZYpt9aiIcg2bPleok7d27j1q07ODo+RmFgtlhgd2cXjz/+GHZ2VFxFWRfGk6xE9go52P2n1MMSO5OeddDJHaUnW86ijTBpTx5kEVjiyBOQLCph0OqhLoluwGq1wrWr13F6eop+1uPixYvY29uTJCYkuXj7tii5HR0d4fzsDOtxEI5qzlN4AY2323it06SHLWpMPie/kSOXRrWLbXzqpwy2mbZRJ79Om1SsmKS890GLLqyljnnHZ2cniDGhnwnGv7W1hbm2rL9y5QrGPODw4FAw7tVyMhdyNyLmHl3XyxzK2bslBzUWg2opJE3QGZY6jtkdhPV6dBYAQ2hVMl6qk6HPHJPQEIdRvG4KEYGAmArmgXDliSvo+x6f/7M/w8nJGUBFhdsLllqZOeaMcRhw4eJFzOey+SwWc2kZpa2hJolpmHGrTIXAEaDifQGtuWeMESlGdRCK5znsJVpkIO8tqMQp/N1VmKnOFfv5Jozgojy6Vi2BKnmUogU8rWwBwFznRztvCngib3o/j4feCHuwZCG9zhbWl29RUlCQ3l5k/Y6KKcC8URMegYfwi8ViSmED4/zsDNeuXcOdgzs4W64QY8J8PsOFi5fw2KOPeNFDzll6remLzyUjxSq551gnmVC8fE6q/hjEjBStLVGE1qNIPl49HYJVFGlG384LAlj6iXVdRApzlFLEmz04wHoYsbu7gy978knMF3OEELBcLnFycorDw0McHx/j5OQEy+VSPDutaGonq3m99iz3+nPyvrguSDSbUc1ekxr0KsrfHibTaX3KxEtDBfAxNc6mlGebGJAF6tGFu1wuawJvmWRD6mdYzKUzydb2Fra3t3H5kUdw6dIlHB4e4ODgEKenp0hdj1nOiOOIPGZ0vYqFZynUQBAqWlHYoOsFR86jzAlpdClOQy4MRgC8erMt17bCEXlOJoHHeBTebwwCdwDCRnnkUUmY/v//9HM4ODiSxgVqZFarFfjwEFmFlC5duuhGyaRVBcttedHs1ZFAg+mWahyNgy2qdrG+K/VwzBa7F11UwdAwYa5woRlpo63Z+Q2OEFy4+GbKzXc9cV0w6UDdRr4tRu3O2z2SiPfreOiNsE+ADQ9I8CsCDFbQ5TwtM64esA2/e5MxoO/7OjFRea0Hdw5w/cZ1HB4fYcgZs9kcs/kcly5dxv7+voZBAcbvRRE9hKRaEhJKVmysnRRieGoygYKU5oaYxAAWW1Dsnyna7QMQjnPQoo8UgRh6dF3Ccile+507hyhcsLe/j8f39oS/C8LpmRjeOwcHODo+xunpmYvemNEt5jWUqcdgnngNX/03+qfAJ6aGVXFeXWxk0YF8x85lR5uYY92YzEWW6If8+83ZtWNEe68WhponJmM2DIMv4JQSVt0Kq2WHk5MT6YS8s60933awf+ECLuxfwPHxMW7euo3j4yPRC55LlVqIEWPqMO9n4rWm6AaMobhmCBA6nQoIAYjaoJDVszTcPGvvP9OvQFTR8lKwhkBB1qyTwA6ZbW1v46+/6U34/Be+gNu3bmMcJYmVC2M9DMDZGcr168g548KFCyAS0ajZbFajm8Y4TZKjNtei9WeTpDGz7CEGC9paJCLl6TfVc7aZa74GdcYrhFYNpHXIgM35QL7XmlNlSUD5Tp7AjiLmVMuU7V37pqKVlw9K1v2hN8Is6iywTsiGYXrY5LtbTU7BjVzd2fVsLgzTdR3mWmDARToujOs1bt68iRs3buDk7BQMwmJrGzs7O7h48SIWC01esfB6CQywhZ+aSGnKQw1nm2JlerdBcFHHuGIAa7tzwfkUdnHIBdLBo4wKn4hmbc4FN27cwJ3bd8AM7O7vY18bgOZxxOHBAY6Pj3Hr9m3xetdrlZssGlXADZ5UTk07TlvEYX+veJ79rjW4dydn5NuMyu0g91wJdbH59erl6u+lQZksaA9ApaDkLugHwDhKV4Y26WMGRyh2A9ba888U0OaLBU5PTrC9s6PdLaTDhWDpt3FyfIDZXNrVDxQxrgWymC/mCFGwfaHpSSY+pSSer7JbWIV2+k4Kdsahlhhb6C0YdlXvy+OIcT04NFJ4hPGyY4jY2trCG598ErO+w7Wr17EeWBQEs+geC2slS0cKNUpG7QOgHTOmzoIdd22MVHMp1cDWd2bJViJSKVEGK0xTcf7gRlXeEbu3HGPtllLzN9HnF0E3BeGogQoja1m6vNuaI6J2LgbTEs9NNun+Hg+9EQ7WBx628JvwPtReYpLAiz7wLd9UBLflBPO5lHjOZoLNlnFEXg8YhxHXb1wT7+fkBCF12Npa4PLly7hw8SLMfw2s3YjLqAwBS1JoY0IVQbFEBmhqnFjDVDSTlpkgajKaiGs8FfPsuxgQA9B3kv0fhxF3Du/g1s3bYAb29y9gd28Ps/kceRxx+/Zt3L59G4cHBzg9O1MDoTxTNpgBAFjhmuI/awN/2W6qZ2MeS7XT9d1UT9QMcPXmCb5tNXaVvAegVBUaZUn+TSBVAKsbrSHoMqjkxt3OWRO4UD6pbhCNRxVJ+pmZTOdyGbBcrnB+eobT0zOcqWe8vb2NCxf3sbe/i4ODA9y5c4Dx/Bzz+RzggnGUBFrX9ZCOJR2AIq15zKujmmS1GpvCTdVkCFoST27gZDwJy7NzTcBKEjOGyvrJJFV2Ke0iqeD9tatXsV5Lwcc4Fu/YkseMkgsee/xxlFIwm8+wmEv16noYHEpr2QW+/kKFv1qaIVjGO5LcG5hU58MIkDyJXoMKSVmkJevXGglEGPoUyGoBrPJNppGVJktCXeh8KBIZllCrAWWj1XcdgyrJyYX/ChP+ko6K/TQuWDXIwSqSqnE2Q2FlzjEZdalH33UY84BxvUIZC85OTnD7zh3cvHULy9UK/XyG2WyOJ5980kut8zjATBJFSBGESiqyYpMyedXIQsuFx1FEbvR+AxMYklQQYRT2Zo65WYAhiWhNcuMrvd0A4PTkFFevXsewHrG/r62Q+h7r9Ro3rt/AwcEBDg4PtGnoWuUOtQcfQT1e+Bjafdu/q+8qQ50JugnR1FW1RJrDPZub5dQztk/UDLkYRAkegn/ePCsAGk203waU4QrjxDRmeOLtMKCEFLEYRaUYC7Fn9+3+pMBjjdV6jbPTU+zu7GC1WmNrucT29g4effQx7O/t48aNW7h9+xZSStja2gKRFND0Xa+FDNoAVMPm2qhWrjeMoudhIbnxYqn5k4gQQNjd3QFLuYKH+hZ6R5KEMErGbNbj8ccfRYwBr77yKsazJWIQ7YmcC86XBTdu3kBhxiOPPII93gNBJDyj0t2+yLJzuYDiXi0BJPMIsM3UOh3X3E07tmb8LAnts0ehCNYIjAKBo7zR2jlZO2qwilKxzslSk4vMBYMxS3TtC7SoM1mLcB4UPeKhN8KlsKoyyssKsSZzbCG7aIh6jlHDGJuwgCbhiDEOa4zDCpGAcb3G4eEhbt+6hcMj0XRdbG1hd28Xjz76KLa2toSHXLL6cEV3e1s8bUty4ytLa5bWo6l/D27YKBdttmj6v7Wks+uS9CvjEX2K6JIkQpbLJW7euInDo2Ps7u7hscevYHtrGyUX3L5zGzdv3sTt23dwdn4ujTOLCuJAjFHRsu9casfeuqmp+WJNnVH9u/uz/n8bhnUj4eEf03PLQmySKP55xQP9X7aBtXzi9jrVe7cNgQiIsM+V5g4MOZ7iz8yMAhMkMq+d3QgOw4D1aoXlaomT01Ps7e9hrS2Itre38dRTT2JvbxfXrl3D8fExtra20HUdch71Tynq6PseSRkV7f2ApNQ8pORxQSnFcWsAXlYcgrSrKiyl4IY51+Sz6EowVBrziStYLOb4v/6vP8XZ2TlA0b3u1WqN27dvwcTzjT3U971Hbq0cqZWXB6q8fIPHSq55GWbj/UY4RWxjIwWboiG5EL/xyaVMWiRHA6QfoBlzb4vFCkGRleYXX1cm7iTNGtgjHoM2bBqSjuuDOB56I2z4KhQOAOAesWRSTeTFzQjGPHpVmOj1JqQk1f/axhCcCw7u3MbNm7dwdHSMwoTtrW3sXtjDI488gpS6mlQJohtA2lVXDLqEuc4eINU60AkkGFdNQtmigT7JOot6ldCS5DXO+t6xu06VvggiUXh4eIjrN24idR2eeMMbcOHCBYQQcHx0ghvXb+LWrZs4PRP9hHEcMXKZho+wenoNCSGNOmWAbVzNs4TcK1UKnd25/bZ2b55mpz0Jap6Q8jYj6Ru0l6hCwV5NCA9wmqgCYDW81iuNfWGxQzp2u1B9BzcCtn2Iu954/XJ/0oG5dikxYXhrwjkMA5arJU5PT7F/4YLLv+7u7mJnZwfXr1/HzZs3sTw/x87Ojs4VabJZ8og5IHNGvXnSAe67GUoZxbtkdlzTO56oA1EzSYqdgnUjyiijiLN3fYeduTB1hmGNvu+Rc8HnP/dnOD09F82aEJCzGOKDOwfSMZwZFy9eFPZFo+FhYbt4lPi/23vzmFuv6jz82cM7nPGbv+8O9sVOnELIDA1w00FKseK2Vkf+qCIUoSRtBXWqEipC0iGdBUrVRlGmRk0L/aexmqqkbSCDw2CKaihxccAQubQQX2Pfbzzz8A57+P2x1t7vObZJf1Cbay5noYu/e773nmGfd6+91rOe9SyKUtHcP8FWD97IehB0Pf03TD5ZDUSa9Xe+GWkUbsOAC1PE7+OaWFsz1EgXGkvrFp2tWBHoEjFni1kqyboKOLOJhL8iW40kIUNlNXBkZZzcKwR1DgFNlV5rjVRr5l5SKUeBdBLOzs9xdnaOyWwOQKDTaRP+u73N0TZLLvmQEFI6pBXpARvW2hXBwfL7DO8h8FfDiUzdcjK2nAaaHMAcZ+ZBaq2Qao00UXDWYDyZ4PTkFFVdY3tnGwcHB0jTDFVV4fz8AqcnZ5hMpnFGmrX0/B4U8XqP2GrrODUPSm0UIYsYLYD/Sz8HVxxwbPo8jTUtrWuH48o1YsXBrjxIX6UK2G9zSKwWiWI6i4CpCl5rxiN5Q3oRqIXhPcoGE372nguHdcDcESALz2cOH9QSEVs0xqAsKyyXBba2tmDqGlVVodfr4dKlS+h0Orh58ybG43Hk5HpNz7dczOl71imUThpapAwz6MC6CA7G1A0eK7DWuq4DO8ha7rJj5TM4ZGkL3tEEj5q1O7a2+pB3XcNTT30R08mc9w5pVZRlgdF4BAdiWvT72xAiFJFFLCyvFjybQ3q963H18KVNSI44OFjnm0wndFjKMKIbHPEzREAFb/B0EN7bKtyPFMCUdUXfD9+kHogt7oHapqRC0FGmYIkpkaIJ0l5ou/2dsFzZ1BBRA2JV75SqyzpGkVprSCGQJCkLsDBvEx5FQWNlzs4vMF8UkIztHezvY6vfQxyPA8QutqDuFag6QHMQrDMBuJUyMiKaQpwDGNrwjCE3MIlSpPegEoF2niDRCmVR4Pj4BIPhEK12jpddfRn6/T6csRgOhzg9PcXF+RCz+YIEc1wz1LSBHBpZv6BiBo4Mgq8UvK7hLzI8GNZcAM8qhzWOUzQ8bSA4VKykoh7N/wNY3QR+XcLwS1HWAMT1D5OkSYyM6XQ+VO6bzwN+becpdQ2HT0irV18jdIYRNY4G6XgvIASzchxFkUG4qFgusbOzA2st8jxHp9PBXXfdhZOTEwwuLlCWJXq9HgDC8a3lDkO+Z9KUCsLWAtI1GYVSGlpRIVoIAZGk8M7EbMLU1VpBL00S5HnO3XLLOO06tKJ3Oh1kaYr//X++gMl4SjrbihpIlkWx0mkp0Ol0eEZiGu95kgNgeuQKVzc63/g900HeTAfn4i1Dcg6rIkGrNxcX3+BBqtaM6Lsg7ypRGzpovA+qgywuv1I/EJ73WnxdRNEscAQc88/nRAQvjN32TphOR1pMzRMWmhtCxsgtrG9IrbI0RZ7lMLYiB+yB6XSMwWCIwWCEeVFApxk6nQ729/fRabUAL8ATXyjqYmdF8EKj4aA4xW44lpJgEdlEcJDhubiwYS3z9AOGTU5LSQUFgXY7h9IS1tQ4H41wcnwMYywODg6wu7eDRCdYzhc4v7jA8ckpptMpiqJCVZvonDw3XFBKt5J+R4wOKw5WRFZH44RDqh8wTLHyc0infYSCwhcUgRYhYGOYGrr5wusyv4RCpUa3I7ybGPmCnTvtpvCWJVVm2GQjlwjftEkDMfwNfOu4McMFolmbVfpa/PchB2JHpELUzA7QGIOiJJnP3d1dWK4jXL58Ge1WCyfHJxiPx+h2uxBtUKZma3g46CSFNRJgcX4haeqHs0xvFB4KHonWkFojkSk35zhUtUZV0QgwIQR6/T6qqsRysUSgnoV0vihJ1S5NEly78w48aZ/CbL6A9QAUYFmXYTQaw3uPg4MD1iPOoILGimCc2Kwc5NZBKBF1sY0jppBUDaZMXaCcYwjGbxF43mGYLq2nNYZ58haA4n3UFNvI+RNGTJ1+JH7kQI46fKnONXuVHHNzwNM+5enQX95w+v/fdvs7YdAGDQ44RMGh+h6J3mjaHtMkQaJpbptwdHNPZxOcn11gMBpjUZTI8hY6nTb29/Zp4GZwAmGCABO+HUfQ4eQGAB8lNT2N1wFPCOCIxwvBYuaUQlMq1ji7QPLXSiHVCbIshZICs9kMp2enmI6nyFs5rl49Qq/fg3UO5xcXODs7x2g0wmw2R1XVMBxlhVQvONuQqoe1A4IfCmkj+cImtUMT1qBxmtGBrsEQzeVNUS3wn0kQ3nsBFVrAROBq+ib61mrlmcJBwKGVD/ADAKxPcVAEAPJnZPwXgOdoMjpwDwi1igEH3F5ABoyTD5QQtQvE8yUWLS0BpBQ1ek+z99jZ1LVBWVXY29uDcw6tNmlStFot3HzmGUzGkygtSfPVDGoiswJJBqE1ZWmcpdCt5WGch6sMEu1Q8fVSCh4skCFNc+R5RmODIJDnGeqaZv0ZY1AbmhMoPNHCup0OXvayO/DkDWpzps/aNJhMJjOqQbButmSGQWCSSC4ShgnGzWOevp146DmGMejmoMCAePleUIS/2tIdvhsZMHE4WOMBrqXQ0lu+t+iYXz00neWsxpMinvWItEchJITzXFRsYLVn38cvlN32TjikkfBUTBJhM0UH0Jx4StGU3jTV0AI03UACs9mcqUUjlMYgyzJ0um0c7h8izVKE4hO9oI8pIZ3slnAqxp/DXKumfVMQLslft+OT3zmiLgGMfZFPQ6IUpFJItEYrS5FoGis/ns1wcnqCZVFgd3cHhwcHyNIUVV3j/OKCq/HUYlzXBo4bOoKDj9Gc/1IcXn5kBT6Rqw46jF5aWdK11FOsrrSIKWdw0qtTerH6+iFDkQ2Nj0qjwZlTFNNUrnnTeY5yg3dEiLlXPiNrE3iQb+OLOBID4Jh+7QWcoM6zQFXyISqm8Akrnz48DT+FiAduOGRdzdCPtTAVMSd2dnYA79FqtXDXtZfh5skxzs/PUdc1+v0+RJoSTxgepioAn/K4JKJO1nXNbeyAk4DlQztoYdS1RWUqbG31iXpYlrGoSepxMtK6pDBN5C4Mut027rh6GU/e+CKWCxrgQDhyhTRrYTqd4+bN49hdl3JUbZ1FIKdQ/YN+XoXcSJyeV08Eh2xX/kiGBBtYQvEeiMU1fq8BamRggiJvIWBqQ0GL1gDCUFPOUFY6/uDJ2VsuegdBdwHKkGv34khZ3vZOONBpIi2HN2vQXVCa6DFa0ZdNDAPBztpjPJ7g7PQUg+EItXHQaYpev4fDw0PkrRbAYjoh4BMA03NW2h75hvM+4LmAEjqEXvCg1KxhQQRslPiJIchMOEKXWiLPUkhPEyxGoxGOj4+RpAmuXrmC/lYfiVaYzWc4OT3D+fkFptMZqpLF0lk5y0SaEPg1n1UwQRPdUXAro3MJmyoGrILYB2ELhGAxCMsENyhkE1XTeRSE5/l5gLV5cqE4Q5ioAIRGWVOxC0Ig1QoiYT3kEKKL0LzRROVhzFIMmH14NY6MVr2od6S6xo0GIbINlCr6Lleq/RGT5ugMIs4qDJ+KWm/p3/MsAFTeY2zHKOuKpD6rClv9PjrtDq5euYokSXHz5k1cXFyg3++zeDvdz1VdQiiF2lhunQ/TY2zs6oTlqLuqIeDR6/dpishiASFYQc1YBB3eVexdawUhSHnNGAMIgTvsFXzxi09jsaDpMM5aFMUSWZphPB5TdiQlur1elN5UcaAAUwb5nrPORCaEEJ5nIQpYQ+GI1grWCYZ1gu4vEHB8xGBlVUvFEVtlpVYghYRdoSbG752H4npBDj3wwX1kVdA+tiYIJfkXCYz4OnHCQCNpt1rAiekzAL2iASE5Nx2Nhji+eYzRaAxjPZKMHPDR4REVSJyL6TlhV03qBb6xxaoDDilULMrxhF7BfGYILrIJOENOJmw6KagzKs0SaG5Rni7mODs7x2w2R7vdwqXLVG133uPigotvgyGWiwK1qSnF877RkmV04dnOdy3yjThcI/PXFBtFaIejTCBgahAsRs9RLwJpP4ygaaJkDyLTh1lgTbGSswWhKeLRNLJ+UdQYTedxPLqSAv1ujnaeQfDGhAiv51c66hhLZwholW7mvSCWAQIsJeCd5L3qGD9sagnehcheRghjVZckCuI8B4Zhmpyz8bmc93DzeYyKLY916na6ODw4hNIKzzzzDIbDIbrdLqSkrk0hJBaLKZK0BWMsOh3SdaiqipwWwIcQBRdSS9TGoCpLksrktuTgYAz8yj3JGiRCxKkjBCNQZ92Tf/AUlsUSgmGD2pDk5Xg8iRKpbdGBVAo60XBcmAy8+KagSYeEcETJdDwQNNEpZQ0rsx6bgCZ0ulLTChgCDMGLcTWkX5GvFQQ3SKXiZwqH6Oq0EJ7nvfaN1bWBFMSa8t5vmjW+UmsyW9Hgd6LRD1VKIUmT2F4pBKU248kYpycnGI0nqC21am5tb2N/dw9JmsA6JoKLFSyVMU2EEzrK5nmOKFdZGByNscejEUm2kS8UlGJTyzFNtEgT0oetqxqz6RTHJyeoqgr7hwc42D9Au9VCURY4Pz/HzZsMP5QljDXEduAoJIxWX+O9hrXh/4abH4I5uhGSJbglLC59rnX+gwAlHBJiLQoGGhgoyjIKyfQ/EDywopUhY6MFvbixHpPZAsY279k6j9miRLvdIglOkM4CAIoKgz5swEOik28OZscNNM0hKRjzJLghEPk930fWWy4CiSZK5jRYCIGoWMDQiW8+Bb8vRO2EwBMvyxJDO2QKG41B6va6kc/9zDPPYDqdIjjNJEkYohLQOkNZVpQlSVIIE4LFgQwVXl1tEMZhKZ1AeA9T10wH8/H7c47lWxkjLsuKIlyWaz08OgSEwBc+/3kUZQXF11WgYQaDwSBKgKatnCdRMwzH2g1S0m6Jeg3ex2GrMrTkr3CK19udfWQyBN0WyZiv9yKyh+Cb+zgUC1cFegBSmAv3QiiSB/lLa4NWBGWuxti1vfJC2m3vhB2TvkWYjsyOTUhJxTopowSeVlSDnUwnOL55jPGEVdBaOfr9PnZ3d6ETDWtqrphqcpQh0oVvbhb+bzhtSfeVvlS6j/hawb3vCGpSDJPwe1OCRtcnzPVczBcYjUYYDAYQQuDy1avY39+DlgrT2QxnZ2c4PTvDdDJDWdc8NshHfV9gXV5ynSLXWDisVMDOo6MMk5RD2t/wLmXwyt6Thu2KQw2QRiiESla6CkNIAzYMjlLCwNRQPBNSYVktWbilMQ/SOSjLGq2szdF5GFFliPPsybmQYF7TAhysEZ5Z57E2rIZGiB7eQ7AgDoLD9cQudnHdghNunPjq/vWeBvdabyE9STPSmCePqZty8Y46uTrc3HHnnXfi5s2bmM1msNai1+tBJRpC1DAQUEJhKSQgia5G+KaCFZaVywRpa1uStlRKQ3lAsC6xlICpDeCJphi6Ogk6l0h0wgVAj52dbVR33IEbT95AWRnAA1VFtCCtFC4GAyRJgoPDA0idQmcaTgjUtYF1pvn+43o0bCUHC2+ae5CcdeCmE5zjPdddvIh0TYIbmxFWQgrqhORi6Bpe7xsseZXlstr1FzLQUGRPk4TU5V4Eu+2dsI8FivV25ZQ1YsNoG0lzTDAcjXB2dobJeIraOCRZhl6vTxNspWSMqBGiCc+3RjwXIelmKT2O8JQOZCxKzZ2nm4PnccI6H6Nf4n0SAzLRdOJP5jMMh0MMBgO02m1cuXIFW1tbkEpiMp7g5OQEZ2fnWCyWMIZGCK1Ntlj5/MHW2Q/0d83AbUCGA01r3U0j0sS8J1Uv4lgrwmnZERNnUzClCtBKxbXhpIH4m2DoQkraWOznpUioMUNKjGaLtc0Uv2Ne5zwjSCLIl3ooGK7iW9uktI61ZIOeQsP9lfEzkoSA4BTWQYe5fBxRgTdxwIODPkEjJxpWr6kLeKxnHmAHTYp0gKcWRExnM4I1DCmn9be20Ot1IeVVPP300xiPR3DOodvrAR5IIWFQkNqZTtBut6ESxVFdwvxv5j1DQLAmAiC46EXNHuHD03fquAYgCGbzBIMURYm6Mjg42EdVVfjiF5+GrS2EF6irCiJNURYFLi7OIaTA/t5e/P4l00TdChQmVtePbwgh1rUmQhYftdIAAEBISURBVFNTgNO893CGqJQ0JV2sOFK67+h+4u/bN8wnwQdLEIZvOuvWJ64oZnCEgjp18D17B7wwdts7YWC92NMIOxPeRU6BUpzRZIzT01OMxxNywGkaI+AkZb3WKBbOf13BeFcfC5m0c564UbyJA6vAecYrJc18887xJGCePABSmNJccJjP5zg7O8NisUCv18PlK1fQ6XYBAVxcXOD45gkuLgYoywLG2LXoN0Z2oVkBiDdjg8+CX5NvwFXRFNEcNJyg0e+Y2rUoeEy8B7QU6HVaaGcJJDy0pMGlQekrS1PCJC213QqOTgOVCYwNB4EcrRW0ljAI4eXzm1IK+3u7EMKjLMrYICGdAbyHdTSTzXpH1IGY4oYVQYy2wgEUMhnFmKDwTVSlRNAzCNE1HeJwIhZ/BQBYcgZCEKToVu6TyKCBh/WOcWhKg2eLOWpnUZma8E/XR7vdxtWrVwBQwdgYi52dHVYbs1A6BSxoGKlPkKiEh7NalHUNIRSEAhx31ymm/llTkxPGs7M44vCGSRrkIz2SRMFa4OBgH8vFAmfnA27uAeqqglMKk8mERkYlGlt9aoDSOgG8YjqmIxU1FbKWRn8DK4eVWFmneK/y901aylSLCW3SAfaCp0wjBEarGSq9nl35udGSCKOXwOJXHqzpLLAGkbyQdvs7YcbNBEd3OkmhlaLNzeLXUgqMJxOcnZ9jPJ6gqi2yPEev38fBwQE3cNANQSknR0yeItcG7F8vvAXIwfGNAoFm4yoJWxkADlqRjCFVuQnXVII2r7U06ff09BRFUWB3d5eI/Z0O6rrGxXCIm88cYzQaxfE0TeSzPkxzDXIQTe9CbDn1DVzA4EIAWFYiZsGOk65bLAsUFRdcABjnMZ0v0cpo2nSiBBKtuKAioLRkp6sBT5xOaigQKwUWwdNFOCvQEpWNoPTzWpJobG9vQSmB5WKJijvEDOvw0qb0EXenwaMmYn0xQhWNnoQQArV1mM2XrHSmkOdZw7aJkTU1BBA7w0edCudDRM3rzLFxIxMUVAr4sBb8mxDtLZfEjWVhIHig3WnjjjvugBBPk9rdaISdnW2kaQbJ8IuVCkYKVFWJVitHkqSoDcEbkApCejhXUfs7f8NBczg4Kzp0ucVZ2OigkiRhDJcG2B4eHcAYg+Fowmpm4PZ3gdFoFIOKbrdDXaNSEWaNEPVSRKoUVpxhc6+uTk/mr4eNnbZzXDzlHmUvGgjQ+ghN0JqG5+c42ZPDpsyJ7kHPgl/EEw/7g6Jt7zdwxFdkUmkafCgkd85JqCShNBmAUgLT6RRn52cYDkeoaoskTdHp9VhnIaVNzPwripxD1ISY+sbTOuJTAVNizqL3UEJF7qF1nrA7AFrqRgQldgyRaPZ4MsFgMEBVVTg6OiJqXE7zvs7Oz3By8xSj8RhlWcF50gYIHGCSwQ0RLUA3fRP1ruJhABqGQowOyQGHYpqQpF8hQRG880DF7a7BHMDTGSx2thIkgtZYCcGFS0AqQKsEkqdAhEKkVM1A0jRNuJGFHEIq1Poh8iwLLIA8TWCNpTqAlM3UD+4GtI4OtlarhaqqqGnFGNYGNnHOHzkTh8FogrKmg3NZAmVdo9/r8CTrQG1CoxrGARkrKgI+0NOAqB2NEK/xyR7wTnC2FGZUASiKJcae6XzOA34PrXYLV65cgXMOo9EI3jvs7+9DCEBrAWMKSC1hrIbhuWpJolGUJL2qtYaWBHNJKWBMCVtXNIWbp317RyOLkkQh0ym3sjeYqfNEG2u32zg42ENdGUznizXYq6wqDAcDpElCrIk2z2ZUigbOxtFSsZRAP7v11v5wGIfvkQqIdHGo60RmR9BijsylppYRIQuuuzRBE4n7JFLCBdkB5+EQ9jExdKxeb/55oezrwAlzk4RqCnEAR4HeYjFf4PzsDOPxGFVtkGQ5up0ODvb3qReeoxwhJXFHY8U0FJGaVKfBtxr8MeBJ4GtikcYT5UxpGWl0WpKDg3OorMV4PMbZ2Rk8PC5dvoTDw0NonWBZLHF2fo6T41OO3GuWKwxymOAoeDX6DZxM+luAAWgtqEMtUtGEiPGahCSmA9/kVN0i51kZF1t+V80BWCwLSLGNdktBK9bskCuMj5Q2ZiDiU8u2WhslFKIV6z2kSNaKW88xIUgrwBNHWynNmQdDCEFQnjdWkggW52+ivLqqsSxK1LWFtR7jyQBVbaNTsR5YlgZZVqPdyuiTekAJBWfpAPWseRDoTlpI2JBeBxwZIagPAvHhnqD/WkE0ORqmLYgLPhwRJQvAPvbQbrdw+fJlOOcxHlOhdpehCQGPslhwJuaJ0haCAkNtztScQfrWWdYC0gxzP4Vh/6UTcjjOeySS7gFTcaHX28gfNrWh4a9S48knn8J0sWAmF+2TsqwwHA6Zd0y4slTk+FYHagIWQjRNL6vFY/gGWotjh/j+DcqDYWQXJGWbqx17XBhqounn7FXO7KREqki7G1JAoaGXWmdje/8Lbbe9ExYcZRIVLYWSkm4qHvN9cXGG0XCEsqygkwzdTgd7e3tchKM0O0ARAGLlnp8dAag0hvDNQHGJKa4UkTLjvAcMUWbCEEKpNFF4QPKT3hmUZY3BaIjBcABA4PCQIuBEJyjKAienpzg9OcVkOkNdEbZqHWGgLgIIzTsM2G2g4UlBPF6KKsJARfpwHDDH64IDloLSvNg8ohS+FH3dA6iMhXVAv99FqgUPy9S8PozNKRo+6iAglYY1DgaWHnOIzSTGejhRsyDLl7ZlUaKUQS+AFbkcpahCaJADDuNvRPwOAh6ZZRnyVo6iqDEez7Asq+fA0M4DRWWQ5yljvXxgeAYaVv6BZEgqYMyrxc8w3NO5pmuQAzeKCsHvkx1JWZaYTCYILb/7Yh/9Xhe4egXeOYzHYzjnsLuziyxLoROBuiriwZu3WLc4FGkdYGwNXxgoJdHptNHrbaEqChTFkuAPUPRe1zXyPKc5dJaoblIqpGkGrROY2kBIhaNLh6ieegZlXYOKjsSZXiyWGA5HfL2GFtwgxdrGQTMiqpspD2dWMhimkkVIbKUZI9xwq3SzWOyIk0ZoYe2KY14txAFgWBFxnqEUNFWDlNUkByAb7Yiv0OjUTzTRybRSkHAoygUGA2pFXhYFlKbe952dHSRJQlGH44aL4Eg5UqRgSyCCe4w3eYhGrUAI6nkXLN7jPf87xakyb0rnIZRjDJhGygxGQ5yfn0NIiStXLuHw4BA60SiWBU5OTnFydkrsDcsYYLiRn1V9X5PtE4GfixhxRa40O5FYnFOheYKgESUltAxYLrfCJinSBNBKkkjLs1bdOo9lUaGVt5BlQKJoknCg4nnLClkgHNFZh7KqYRyomQFgBbEgzyh44vBzTQiqktswZQQk3h9GyitJ7aqRShd5qgHvDlkCHRZZLrE4OSdmxbNfi79vpSmlDpCNUJwpeAdr+SAMzQHh0OGoy8NDupB1eIDlNMP9GqIzy2r6lm+0siyjs1WShg/0e31cuXwZ1hpMplMoCLqHlYaAha0rmCSBsQaJTpAJ8Oh6BylZbMdazGdztPIMeauFdrtFY5oWi9hgIbnAKupA2yPIIE0zJDqFEAV2trexmC9xek5rR0vlUFU1ptMZ0jRFK2+ho4ixEbIgYwxF54LqAwFzD/WVOMQ0bjeSCV3FkdVKliu0gK0tC78LSB0OZDoQA+QVcpKIzTMMp6SmQ1LwPm62+YtiXxdOWEtBGJgUUMKhWNLJPByPMV8uoZIUeauN/f39OE1WNEOrEMJDjyZSAQt8BN3YYM4Tkb42pnHgEDSWiFN5KBmnHieS2li9s5jNl7g4H+BiMESSasKAj46QJgmWyyVuHh/j/IxakGtLI8mJCdEU4FYLGoFhQG98Bf8NkTyH+PSwj5CD4mIa0XRcEwnzIaaVgkpSOKGQpwkqUz5n1R2A8XSG2gJbWc7YOE+0tfSzdR4OEpCKGg+Y5+qZuie1jF1ZlTGw1sSosdlCTIMTxLyAIN1Y6zwMMxasCIeRBJiT7VnujgqNDt6TnoBzAqa2mE5mX3LTJVmCNM9QV/T83nkI5/i986QOB+7jCZgkuLDrKdzi9m3POhdgzNyjGXMFMK2MaVSKHSjQsHwODw6ws7vDjsxhMp1GZ9RutSCFRL1cRrGnNEkghURRLPl+FVGtr1iWpFXR62Fndw/dbg/z+Rx1Xa2xJhwBxACApTVoZS1aewBXr15BVVcYjMbET/a0KFVlMB5PkOc5tE6Qt9rwigp9pKPhIqOoiXq5fZjSDIiog0yHZaCvrUIN0XGnCRUa+Xugw1fFLJaCoiAaRP9LlI6TdwQCS6jhf9Pg0BfebnsnrKRmxyGZj+swnU4xGA4wnc8BqZDlOfb29khQ2weKCsMFQsZEMoD/HM7Ai9DmKFich26EZtYXV1wBLgyykhtfm2pBo8y9xWIxx8nZBUajMbTWuHzpMi5dPoKUCov5HMcnJzg7O8V0towVf8sjjp5NnRHRaYqG3YCAAzeFuTCWJ4hyB6aHCn8kt6oKSt0TrZGlCZQUSLIUQmXodTuYLsrnjVDnyxLTRYXtfhvOUnrrnEAcm+MFyz5ylJ0oooHxppNaUZppLMq6pAiUD7YgN0hdeVTIS3kScGLSOEXDWUNOhCvyQigYUz2rKNm0wToHTKdzLIvn/0xCAP1eF1maQAhHEBIzGKw1MLUl0R9B74u6wgJOTBh7gxj7WORdLTrSNIhmHQDAeAdvudGnLCCmzL9WCrt7O9jZ2UFdV3jmGYPxaAwpwkFKOHC1WEBJhbzFnYV5C2W5pIEDkWIHWOMwGAyRZim2ekTPXCwXmEzHsGHayuqBbz3KqkTeaiOXOaSSuHbtTlR1jcWi4JZ8uk+XiwKDiyHSNMeOkKhNTU0nSnFRdr1rzZiao1ZuEPJBwGcFjuDINt73DC+FtbGcR4SAJBwiUaM4OHd+HR8aQ6SIYj4hQ3qxQuHb3glrTq2VFBRtTme4uLigiRhC0g2xs4MWD12MtQAOIMPmCcC9YMwx0ls4hdFc91aSeL1UcFNxSoUQAmlCotcSVJlWkpzEcrnE2dkAw8EQkAKHlw5xdOkStNZYLhY4PjnFyckJCbBXFpU13IxhYzom0EQQSjQKZ03bcOOcKShsihZCgDYsE+ZDtCsFFwslkDA9K00SpGlCwyhVgr2dLRyfDZ63rb62DjdPL7C300dC4T5HohJOAF6zIL1UgJQ0pskTe0FJBcFpaVVVmC1ovLqW4Ehdx3mAWmt02i0aHy8UpE6iozDWQCcapuION60hjYSpa6ZSWcDxSHNHLRwXwxExBZ5lArROu9tbSDIFWYDWT0nAkt5ELWuY2sJY5gYLQQIygTYYnitGv+E78nDCM385jIZHdA5CgLWe6X0VRYkxJswDltjZ3cP+/gFMbXB8fIzpdIoszaCVRpZngDNYLuawjmQzpZYQNd0jYf5cojUdRt6hKJZwxqDT6UJphTTNSfJScgs4wypCCFhjUCwXyFttSCmxvb2NS4dHePrpZ1DVBpIPXescZrM5BhcXyNIU3V4fZVUh4XZ8a7md3gVmBHcxAfC2BiIfn/48e66d5jFfnvdk05xDbjhMo7YsPB87VMGROLiBx9HnM9YAccWbMU0vtN32TlgqgTRNAecwm85onM94Cu8F8jTH1tYWup1ubD2WUhIeHDcGY4sBSxICQlPXlw8FAICqz5IHFvqGMgNnYyokhIBKqDAhQA6gWC5xcnKOwcUQQgocHhzi0tElJIlGWZQ4PTvF2dkpZrM56toyBGEaulCo5ggRnXFwwE0nWGBlBknIFYet6H0nWjFljwTmCQcWSBOJPEugE4UsTZGmKdIsgdIJnNDY2elDawVTrxfNQoX7fDDCdFFip9+hzjfBjRoQRBXUGjrJSM+AswzHSlhCStRlBWsr2Nog1RpZkmC736VUW6mYhnc6HfQ6fUiWdSRhGQNpDbx3yDLGHr1DJjMopZE4B28NrKkJ6oDAdF6wRu7z30/ddhudVhs6pTUrigI+vF/voLWETR2qskJlPLwTEJYaMUwU5ucuQesjxQ2cfjumRzXZCoMYnKs751F7dsasoKe1QpKk6Pa6ODg8IB2KwQDD4YCxXAmdSThToSgIF223W0S/NDXVBCKsxtx251CYAtZY+o60ormJ2qw0L/gYBFjjUCyWUAkdjoeHB+RwBwNYG/aVRW0YlshyLiBKOCWagZwcNUspuHBnIw2U8Rs0lbdmjSKLgbUqgryI5OCJuMNh7RsZzaiHHZy2EBCS2qMpsaVpHHVdv0gu+OvACedpCnjPgufnGI5GsEGQZ2cL29vbVPUHoiOTmqvd7HSDYI1YcdTBYShQtEZRcsMXprHkAAJuBcIDc51CuhpVWaAqS5yfD3BxTiyI3Z09XL5yGXmeoSxLHJ8c4+TkFLPpjB2wQc1QROApx/fMkIEKcANvbillTMUIXhAMdQum7WnSEpCCutskPRYccJZq5FmCJEu5JTmFShQJq4sEO1t9dFo5ynoet0bcIgKYLwuMp3P0+z1iAkgPldBzSZ1AJSmUSiEEU6JcaGwRsXkAIKqZ1hqtPEO73Y5KYmlK2H0rzwFBI6mkJOeomRVjmOyf8XtylpS/lBAolkvUBeB1Au88zk6/iLp6rm6sAKAEsLvD94snOdJUJ6TdKySkcLBWQkoq6ijjYS2gDDk4IQWENdSsQFXb6HBjZyPooAyNPhS9BbUvEr0RgqLm2hjMF3OoEY090lqi1Wrj8PAQpjYYj4YYDgdIEo2tJKHJK5aiVqUE8ixHnrdResCbiu/ZRgFQKQmpyfmVVQXviNnjeLKGdezEfMPblUrQId1t4+jwAMvlAvP5EkoI6hb0lNmMx2O02gQfeUfdqUopOAGW5GyaoILWi+eMBcxsCR2NYW8G2CBkQWEisxMNnh0Kn+v7mbFez7PypOTCqI14PN2Dfzg75yu1294JSyFQLBa4uLjAcDBEVdXUjNHpoN/rE2bGvM5mthvif4EGcwI8VewdiYUI2eB5QRegtjQsEzI0PRAJXCgBBQ/hatTVEsVyicFghMHFCM557O7u4OrVK+h02qirCudn5zg9ISEe4q3aqAexyoBQUq691/A+IRr+b9DWJTUrhmhUw51WggqXUpA+b5rSoNA0S9DKU9LZSBNyLEJCKAmlFcl7aontrS7G0/laRAcELQ1gPJ7hrpelIL0MOkA0KOKmidPgluDA9CAtAu8tF0BFpCBZ57Gsa3g+MCIOCKAoSwTB4hjtwOPZ5ZQkSZBnxAH3xsDVFYQQGI9nuBgMmVb6bLyRcOednW1opSHgIKSHznIYKant1wskCc0RTBIFYyzq2kFKwpplTbq+FoGS5eFloLERLSqcOl5YnmocCqdgBwx2EMyxtcB8PsdwOECekSPr97uoq31UdYXZdIrBYEiNLK0WOVXnUXFhLktTZFkOpxVcXcPYGs5RC3q4R0Kd0HlHXGjXCJ1LQXoqkguqhjv7PIBOr43d3R3UdY2yqqFB7c7eOcyXCwyGQ+StFrZ6/diNFyY1B3U6ahl3jJFz91qMP0SEIEJEHmhnNmZTpKpnasP3hYh72nvfCMp7bkACQXCe25cNGke/rvvxwtlt74RtSafuaDhCUZVQOkHWymkCQJrEaNf7lWhENkTuwAENGzJoKjjvoKEjbxMIX5KIXEMHluaTxIYQzqA2FlVRYjAY4fx8AGcd+lt9XL16Bf1+D6auMRoOcXJyjOlkioqxy6quYSM1qIEcGt4jfd6IX8drWIJF0Q2YKAmt6U+gommlkDBjo5Vp5HnGqaKG0hrtLIclz0QTBqyFNTam19v9Hk7Sc/AIEZ6oQOsmAQzGY8wXJXrdNpRK4QXxo4UMfGATYSBqiTVU7PKOxr87EpixzgIGcEvCR0ljgz5rq93GfDHjidMJQiXScqQZJkeYukbNzkSAij/BuZ+cn6NgsfimmMkMEu+xvdVHmhLHmxUgIUCOLNGqYRHIAOlYSGmhpKP3Iek7MBWNELIAnJMQwlEGszq6J0BgawIzKzULz5NXBKCMxXQ6jYXTdEdjZ3cbRUVsh8lsiqyVQUiBtszpnrUWpqqQJSkP6FRwWkMaBWsr6jxzjuUuuVjlG0EqSOa7C4nSlZASUa/DWgvB0pe9Xhez2ZR4xxIAJIwnMfnZdIbBYIBUa+SiTfcnt0XXdU3ZFH/2Ro3QxSg4MERChBqaOUiAik8OT4JURgadD66VrLApnHOs0cIwUMCSY9TcBGEvht32Tngyo0LccllASoVWq4Xd3V1keb62pII3rXc+DicM/FkZcTnEDUqFBgsdes69h3EmRmfOkwCK1gmUVJCw8NagWC4xHk8xuBihqmv0el1cuXoZu3s0gXc2m+GYdYyrul6JgBupzACdBJUy4RtRdcmPUxGLu/EEyXRKkMZCmigkmsRxAh6caIk0kWjnGRV0khTgiERqRVV9ISCkooKiJ90BiAr7e7s4PT1DWVsY6+A8OUfHeFpRlDg7H6DX32JpSeosg6BuuLCJLDt3EW5+5+GcRV3T4eW5uOO5icYojn74eq0oCg6YubUWtbWxUh6mTYCLMYBnZ+8xnc1w8/gEhoQNADTKZ/CUZu/ubUNpjdparKoErzZ7UFekgbOGBm4KASno3lFKwiUahZSQVY26dqD5zFTMFV5wUU6SFhALyjfclxDF8SRjAN44CNB6jMcT5GmKLE3R6nSwu7ODsihxckbi/klCAwESkLCSswbLxRKtVs7deKSqphOFcjGn1/U+Fu6ED23vCsKT+LqQgnF5E7OPMKNOSYl2u4X9vT0YYzCf00QOeFKOK8oS08kEW90ukixDCLmVUrF4ttqBmiRJLKaSDICEMSY6YykEvHXcHNUU4MD7xKGBGGkLBb0XOtQJ5ggzF128L4PIz4tlt70TPj09wXRGqXKeExOi2+1EYF6IIHMpos4D0HxBcA6CcSais4iofCaEaDQDhARgYzWbGBmKHLEibdP5osRkOsX5+QWWxRJ5q4X9/X3s7e4CoLTy5PQUF8MRNS5Yy/Pg2GH4FUocf74wEPPZkARxJUn3wnoPOIE80UjTBN1WBqVAs/SUglZAogWyRENJhSThzjhWkrLcnEIiKRpCkXP1wkElKTodhd2dXcwXS3ghaIKDsSgril6NpUGjl69cQqvVBnjjWmMjhQkAwyHUWEHYuoe0gLMWaUJTHaI5Dw8Xqo3I0pTWgrMRqVjQhaMm7zwqU8EbywwVD+/pZ2MsnnnmGPPFMq5lKNgI5vR2Ozm2treQs1B5zd+P5AhWMpyjE4VEp4BnKUpnkdSGI3cJY0I3H/GTCbKwTNNrJnSEbzO0yzeNJTIW6cARqjEOAuTURuMROu0W0iRBp03c9+VyidF4hOFgQGI6ooM0A7yzqKoy6plQ4AEADkmawRnTFKW8hxCqKU4zRz40pIBFraJuiqS9kWcZTLeDXbOLqjxFVdVRJMpai9l8jrOLc+gsRZ5nMJbU3qKgEGtRhMxPcTGWFyBGyc45GlMkKcoWnLnSHgffO02zjwpZlACIJ95g84BboZk2wlzN676w9v/Uh/eud70LQgi89a1vjY8VRYEHHngAe3t76Ha7eMMb3oCTk5O1f3fjxg3cf//9aLepiPD2t7/9OR/wwx/+MF71qlchyzLcc889eM973vMVvcfZfAFjLJI0QafTQadDDriZrMrO1jfONwBvDcbJTAQRFMd4GsTKpiHlMgCgAp1WmgdyKkhvUZUlZrMZLi4GmM/n0EmC7e1tHBwcQCcaZVni/PycHHRZorYkY2icJX5mvPlX2i6ZhyxEgCUEO30aBLosK1yMZjgfTHFyMcHFmKr+WZqg22kjT1O0sgStnIRvtCItB80TGoSQEFJD6hQeCpX1KCqD+aJAUdQwXKhRWuHo6BBHR0fotDuUIjoSzCGyu8doPCKRfFPD1AbGEt1JK00RWmhr1pplCamII6VAmibY2tpCK89pZI7WFJ2zLsjW1hZ2eAIF8wopwnUWWiokmtTplBSss0sFqrIoMJ/PMRqTgl5oEyYcoinKai1x6eiI1MqyFFmeo9vrotXpQEgNJySsF8TwgIpYZZ5RVqFUQ/8jTF4hy1JkeYIkUUhSFSUdG7UwZrz4BhZ59vcfzHnAOIvKWMwXSwwHAxTLBYS36PW62N3dRZ7nWCwWmEzGNGmlruGdBTzNiQvPVxsDOn3UGh0zTIkOwUCA7ZIkQZZnSBLNcpUaSdLgtEmSIs9z9LpdVlILdEp673XtMBoTbj2bzeBt0yUXsxe/CveBGDpcgAVnnUJQYGC4mBumcwfoAr7RCQ6FuLCWUqrYUh+L7iJkHYh/f5Eg4a/cCX/iE5/AL/3SL+Hbv/3b1x7/0R/9UfzX//pf8au/+qt4+OGH8cwzz+Av/+W/HH9vrcX999+Pqqrw3//7f8e/+3f/Du95z3vwkz/5k/GaL3zhC7j//vvxvd/7vXjsscfw1re+FX/1r/5V/NZv/daX/T7ryiDLcvS6vTgqJrTChhuPxEFIrB1gHywQU9a6quMXGWg5gEAUnuZCDn2JjdCN1grCEaVsMpngYjDAdDaHShT6/T4uX76MTqcNUxtcXFzg/HyA5ZL0gIMYe9SgYAv4b9MR52NjRsB4pZSojMF4ukBpHIwHaucxmZc4HUxhLeGYrTxBokWc4Cz531PzgoTSKaTSqIxFZT2MQ+Q9O1BkENJDqRSKssSiKLAsShIVIiUhAEBRVRgOB6iqAt4ZZEnCU0nos2j+ORRJPIOuZVWhrCr0ej1cu/MOqq6XFYqiQFlW6Pf6+Ia77obWGmVRoK5Kci7Mx6a83ca5a0oQ57nVyomi5RxOz86xLEogXA/ONTg9buc5rl65jFbeYgeQIE0ztPI2Wu0O0rQFqRIYK6hwazwVFBXh7WlKnWpKKI4YPQ+glfEgCdHxqoNVTCykgzYEDLQuwYkJXjPniC1RVAajyRQX5xeoiwJSOPS3+9ja2obSCSaTKaaTCYrlEqYuAdC9b4xpeLb8qt6TM9PMyfY+3JckwVqVJYpiiaJYrk1uSdMErVYb7TbNvdNKI0sz7O3tIMtTvl+DA3VYliUGwyFGoxHKZQFnbHToADEvgtJd0zzB3W98QIRDywYVNQBK0wGfcjef8I3saNhShKNnJGW5ctCFoCDRSSxAPrcA/sLYVwRHzGYzvPGNb8S//tf/Gv/0n/7T+Ph4PMa/+Tf/Bv/+3/97/Kk/9acAAO9+97vxzd/8zfjYxz6G173udfjt3/5tfPazn8Xv/M7v4OjoCN/5nd+Jf/JP/gne8Y534B/+w3+INE3xr/7Vv8Ldd9+Nf/Ev/gUA4Ju/+Zvx0Y9+FD/90z+N++6778t6r0on6HS62NreRp7nDU0FKxXSUIgRAay3MUJ2zsL7wIm0SJJQMfYIgx5DChZOZg8aDqhAKd9iPsfFYIjxeArvPbqdLi5fuYytrT4AYDQe4fiUuMBVTcMeDQ85DBY63QgP5m4xgZjayRX6mdYKi1lFwjcra2EADKcLXIxn2N/tIWHqZZCTpA4zh8rV0EkOYx1MbWHRZA6hecUawwUoSu10qlFUBZbFkqdZsBZs8IOOhGCEcIxPe8BbUK8vaAPVhjdbhbIosFjMUVVlxIo9gF63i9l8Ae+BXq+LVp5hNBrB1RTZCimRpXlsKEmyFHmrRalmXbEIHIu18ESJwXAUq+941kaTAA7299DrdWm4q6C2ZlPXUIlAJujwM3WFuqYCKhxpVmS8tlmaI1EOECXsYgkv6V6S3Ma+LAvUtaEuRZY0jfoJIR0Hl4VkExGGho5QMHLOo7YWi7LEYDhAp9PG4dEheu0Wyr0dlEWB0XCI4WBIjhItZEpBaoWqKul71Al1qklmP/AklFDnkMI1s+y49ZuYK0T9Iy62JyhOayhNETNlBim6zP5xxnFGQvtruSwwHk/QznOiq7ED9d7DVxVDDCuZK3hQJ9ZZC2EYLRVbqeAZomDeSBzd0l8btTax4gsaISYPD+FFjM5fDPuKnPADDzyA+++/H/fee++aE3700UdR1zXuvffe+NgrXvEKXLt2DY888ghe97rX4ZFHHsG3fdu34ejoKF5z33334S1veQs+85nP4Lu+67vwyCOPrD1HuGYV9ni2lWUZ++oBYDKZAADyLEe/10eet+IpGMzzwEb6mWg/gSkRqrKecWDPBQOtdeycCe2RgtkQHmCeIpAmCtZUWM7nOD07x2A4QlUbVmnbx/7+LqQSmM1mODk9xWQ8QVlXrHnrnuOAAY62eeOtpnQBw1RBM1mQzu/zlRJq5/HUyQX2tvu4etjjSr+Dg4sV6NoYQFrUzsIJCaiE8W3i3FpjSLB8JTOQwqPdbmE4HhN+7Fj0SEkoT1yRPE2gIWDrGkbUsIYcc1FVKJYFptM55os5ioKiodqGtlXAC2pzXg0Bh4MRhoMRFWScowGfgjUutEbOG7rT6aDf7SJJFbEHsoy+U8tOn9tjEQ86JusLIM8SHOzvQmuJLM+RpsRPXi7mNO9NBxhIQScWVSEBZ1DVRKlLAMbZNaX4QqIsC5SVwXK2wNn5ELNlCedpWkenlSFP01iDoM/t4+m7PtEjBBIAFyxgnUVlgOmiwHAwxFavi7yn0Ou2sdzZRlEsMZ/PMZ1MiRVhDCAUpEpQFEvkeQ4gFKooDRdSQgFIw72muPHEgyhpQIQnvAeqqoaQFto5aOugdIJcKThn0Ot1URQl3GJJWZX0HOlaTKcztPIcWZZzhtAcSgFKCCylIACUrrAj4j5xPqr9uTBoVAZmB98+vGTUqu64g1RwVmBj8xUJyhN+/GLBEV+2E37wwQfxP//n/8QnPvGJ5/zu+PgYaZpie3t77fGjoyMcHx/Ha1YdcPh9+N0fds1kMsFyuUSr1XrOa7/zne/EP/pH/+g5j3d7PRoVLgSashZiy6PWxDuUspmqS3Gx5+IDR84cMUAI1tCldLIpAhGuF6rg8I5mbQ2HuBiNUFYV8izD7u4OLl0mUZ7FknSBh8MxirJCzTzg56vEUiodUlNOUrmrJwjrKEE8VaEbvYtnmwMwL2v87xs3sbPVwVYnA2DjxAnBbIrKGHgh4YWKY3JcKHpJKuZJRbQyU1MnWr/XwXDchpsXXOi08IbSxX6vjZ3tLRRFibKqsVjMMRlPMV8uUJRVZBgITseTPEM76Ua8WGvSB5ZKR5zPGEo9jbGoqwp1VaOqS3JypsZyQoeyGg6ID5tqZFmGTruNbreLLEuiuh6p2q3gsZ4Ecw72dtHrtIm5wDizFzRZOMtyFGXBhzFhh7KtqGhpalRVido4pKlEnigoSQ0SSmosihGOTweYLsgBh4huXlChTK92NoY7QKyLNAnv+b6NvAk4B1g4FFWN0WSC0XCIS60WWlmCne0+iuWC9IlHY/S6XWRZBlOXkKzw57wlBkjNKmZCwnoT18V7gudCIKKkhE5TmLqGVhqARL0iPem8g2D8nxgkKfr9LsqyQu1IXB1cGCuKErPJJL4vrRR0mhIrQtDahygXXI8JmWCkp2nddPR5H3VavCfWhFQqnFcAwKJQDCHyzg8wCK264vtCIE4gfYHty3LCTz31FP7W3/pbeOihh+KJ+VKxn/iJn8Db3va2+PfJZII777wTvW6Xo1PH7ZMBcG8mroZ2zaiVKwBIVvzylL4GJ7AqNu2954qsglbczCAlJBzKssRoPME5F+ISnaDb7eLo6JAaMozBxWCA8/MB5sslcW9dg6tRcMPV37DphGi6NpmEKwUVnjRPr0gSzU0YXxrutx44H0/xuSdv4lvuuRNZSs7P8LhzLyUM575pQqmhC1g4RyENjs5QiNBQHY3LR4e4eUJjooqKurB63TYuHR1iOp3i5nxGKnAhUkxTcoh5C61Wi6PNlNqpFfGwlZBMaWuoSADJZYIhlODErDEoqxJFUWC5XMbI2tQVRtMZ3GgCrRRaeY7dnS30t7bQ7/ewPLvge0Awu4UYEdfuuIwWC7jXVQXhaRae9RZaK7TQwtzO4J3n961QLAuIJEWS5ajKElVdwltLYj/eY1GUeOrpU0xnBaxfZ59a61HXBmkrQ5RfWtE1cDHrWsmUVn6ijM6jNhbzZYHziwv0+z10VA+ddoa93R3MZjOMRyMMh0PkrQwySeAckOVtSEFME+8dwjxFKR07NsoyQjHWO5JeDRhv0CBWK/BWsLosI1OBuh5zlJNZPESIfmgxWywwHo2Q53nUryaoj6CW2PcNquUQXivW/y6JuhaK6zWr7wkVBn1yluFX1SAowHiOHnhc3JcIRe3RRx/F6ekpXvWqV8XHrLX4yEc+gp/7uZ/Db/3Wb6GqKoxGo7Vo+OTkBJcuXQIAXLp0Cf/jf/yPtecN7InVa57NqDg5OUG/33/eKBgAsixDlmXPeTxhUJ7iWVKvCosbNm6T7oMIhd4TWR1AmCLBUDKcpMfoeyQal9Q6tgdL4WBdjclkiouLC8xmc8AD7XYbe/v72NregnMek8kEp2fnmM5mxAO2tlHc4uLUatNFiCwAin4lO+dEayRaIU0UFAvt6CRFr9vCdFHh+RotPQDjgBs3z9Bppbj72hGSJIFgcR2hNBQkPOjvVU38Z8d6DFVVcRbBbArecMJ7HO7vYavfx+npGZZFQVNyrcHZ2RnquoLSVCjpbW2vFW/y6HxVLFKFWWBhPiAH4pTJcLoeVDHC4ShWEIvaGpR1hbJYYrFYYj6bYTKZolguMCsKzJ9eoj0cI8lSdNotLJYFRcDKY3e7h2/8hruwu7MDnTBbBAKmrkmXQDTOJktzFOWSmmrKitulJQBmPSAjDRFnsFyU+OIzpzgfTdccsFj5blwMEGgYJzxVH5xzFJeFA3MFHw3PEgt1nkZMTWYLDC4u0Gq1oFKJdqeFXr+HxWKB8WSKdqeN7Z0tqESuFN6oe420FXhdE7o3raER9zQNmlguFRA75YTSKzUS6iQUoEAnYK9aK/S3+pgXBVxVw0PBOIIlyop0kTvdLvIsg1QSSZrFgrNjDi+8gFQiSgYgZAwrdR76zkjSVEimqIXOzNCCzDWcMNFFQMZipBDNiCPnbBSTf6Hty3LCr3/96/HpT3967bEf/MEfxCte8Qq84x3vwJ133okkSfCBD3wAb3jDGwAATzzxBG7cuIHr168DAK5fv45/9s/+GU5PT3F4eAgAeOihh9Dv9/HKV74yXvP+979/7XUeeuih+BxfjpHwDkVwMjrP0HpMYeXqKB0adilhhYuYr5SKmy9WeKpCcGqsIiYrYGFtjcVyiQEX4qwxaLdb6PX6ODigkUnLxRLn5wNMJrM444xkKS2aKceS31uIggG+jyg6ZmlJzU440Yq4v4lCkkkc7u1gOF5gXj6/8IgHUBqLzz35DKRWuPPqJXQ6PagkxbKs44wxsAi7YZjE8CZcpZVFzjULYScSaGcJZpMJTf+wFp12B1s7u+j1+mi1MrRaLdJ5YMxPhKwi4H8qaAA0GCBA+KqUImo4M0BCM+xWcXQFKBD+2+20sbNDXVhFUUaq1mw6w2I+w3Q6hdISOzt94tbubmN/b4veY5pAaU0HLiQdyhARvlGMHaZctCnMElVVo5VnkTPsnUWxNKgqgyefuYmnT86fI1DPfh0CiAerAQneS8ViPwGYCNmRIIgkFM0AxIPIOQcjBIrKYDAcodPtUHE6zbG3s4PljIrFw+EQnXaLGAJwsKaGCxi252Ya5t2GWYCAZyzYc6u55cjSE6+eT0FiM9SM2fv43iEEWq0MO1tbOL8Y0PMEMSbvsSxoiki7046ZplIJlHTsROmA9p41WYJ2iwgFORI4EpIK56mQEfKBR5wv1wQ2AZ+gE14JDZoNSMJJHqusqBfeviwn3Ov18K3f+q1rj3V4HFB4/Id/+Ifxtre9Dbu7u+j3+/ibf/Nv4vr163jd614HAPi+7/s+vPKVr8QP/MAP4Kd+6qdwfHyMv/f3/h4eeOCBGMm++c1vxs/93M/hx37sx/BDP/RD+OAHP4j/8B/+A973vvd92R9QQtDcL+Y3ykD3kU03lIVjngMZQRKSu3C5+y1U1NkxUKqk+TpqPfXGoCyWGI9GmIzHqKoKOk3Q6XZxdHSAVqsFawwGgwHOBxcoiyLS0OIECTRMCMn8C+qBp/cmJGIEnKVp7IpTUiJLJLJUIUkE0jTD1cv7+D83jmFWZBnXuoUEUBiH/3PjJtKsjTtbPSQyQ5JIuLLiJhbi3JL4DvF6wZMKqFosGFMnzidR7c4wnszgvUCn18PW9jZ2trfRylvQSQIh/YrSG0W6XgQ3RI+urgXQVMKBpl030MgER6VBqJu0JPgAFeQ8hSBOa57n6HY72N3expIZGLPpFMPBELP5DNPJFNIZCFdje3sHeluTxoUgLq6UXIByPvJSqXXW8KfxcKZCXQFp0oIx5PirusZTTx/jD546RmV4YMDzHI9KAK2c4CXhSVaRYAiOJlnvVkhBBagVB4yQrcFDCoIlispgPFuiMxzSqCkl0eu00ev3MZvNMZ8XmE2nyJMUgXteG1KeS5KEoA/W/k1141xXZwGSMLvlYiQXxAK90nlY7xpuu5SAp++q3c7RmmdwiwJQEt5RRlVWBrP5HJPJGFIK5PDIW7TPtEhgYGKLtJQkC0B7yK8Nt/XeRREnYV1UZLNBfZDfp9Iq1oKkCBKZ7D+CnxDrVNEX0l7wjrmf/umfhpQSb3jDG1CWJe677z78wi/8Qvy9Ugq//uu/jre85S24fv06Op0O3vSmN+Ef/+N/HK+5++678b73vQ8/+qM/ip/5mZ/BHXfcgV/+5V/+sulpACD0ugKaZ+cWWA+O6ShBG0ErzRMBAidXxmg3pDNCSHiOkJVWJMwOi9pUWCwoCl4slwA88izD3u4udnZ3AHjMZgtcDC6wnC+4I8hHKhK/UQDsgEX407gnxbPTlAS0pE63REtoARovrxRSJSG1xNWjLQzHU5wNZ3yw8HOzkyeMW2C2qPC5z99AkuY4OCA6UdAXcK65OSGColy4UcnR1XWNyWSCk5NTDEcjeO/R6fawt7uLbr+PdqeDVCkSNRKMaDL4Tu3WtPnD/LnwWnE9uEPMB20P/i7p8KToNBRdtFQQ7ICCoD0NP7XRySRaI1EaWUbaBnu7u9jd3cVgcIGzs3NcjGcYjSfo90fY3yOd3nanA6k0tWRbwJgKpqrgSUINMlGkUFYs0coyGGMwnVDh0XuB49ML3HjqaZ7cvPKFrkbvAHqdHNu9Nry1kKBiaPgOAAEnqSlBOFrHpnDXABvhkzuQA1yWNUbjKUajEQ7SHK0sw872NibjKYqywnA4QafVQqIVBDNhvKNp4N6SeA7VAQRDCzZG5Y672qQiSpqznoqXzsV2eg9wd6Fcg4vSNEG/10VV1YCxUFIx64K6/+bzBdrtDrKcqIRpSnUouvdW9F1WsqDYqgyGs1QzsDQGO8LxyCMZC/QAN2LxQRb4yJ7rBIE582KY8C+We7/FNplMsLW1he95zXcjTQjTNSzgLQQVOyyPuQkRHwASf/HknAioZ7Uv8GBGOBKfUQnytI00U4CtYOsK89kMp6dnODsfoKxKtNot7O3t4+67r2Frawt1VeH4mZu4efMYs/kCFbezNjcOYkdRnIBM4hCxUq4UTS/OswSZ1ki1RqoVMtaEaOXUAQcFVA44HVd47DOfx6Ks+UYSsSkiktO9h5ICO70uvumeb8DuLokbwYcUmZym5akXwXHCE7/z6WeOMR6PYYxFu9vG9s4uet0eWnmLGhYSFXVdI64N4nCDpzrE98JRnPM8/ydUwtkRhNf38ITHC8JlFUsuapnwa1B7r7E1vK8hEYaGmjjWJtQGHBeYqqrCaDzG+WCA8XAIU9VIlMJ2v4/dnS10u12kWU73inMoywKtvAPrLGpLAzPLqkSSJCiLJcqighQSw/EUN774NJaViXUlBg+augSATqbxR+66glSRFKe1jnUzDGpHIkDOe54qzRCRD7KN4AKXi9hGaORJlESvleDOS/u4duc15N0+lrXFF794jGdu3kS5XOLoYAeXD3bR6m5DpBkMQmeoZqishoSFszWcNYzbsza0dVBSr8lcBpF0z8XTAKvVtqa9x1FpVdY4OT3HYl6gsjTGilqQJbqdFi5fOsLe/i7yVgtpmkNKhbomDWjDDSIJBw0xmBHNNBMpJY0t4n3lPFMfAQT5VM9Qo3eexfYJow4O3nhL96J1+J2HP4LxeIx+v/+C+arbVjsiODVXmzhQ0lgeXcTprvUOcBZharCAQMVasgKEQxnjoLmqbawBFG0YLTRtisrCmRplucRoOMJoOEFRFhBSIE8zbPV6SNMMy2WJ6XiEs/MLzGZLlKaOGOvqOeg9KJrDSpFGNIeEDNEky/QpCWgtkWhWRJNErreGOLM7/S7uuuMKnvjCDWITiIas7kOURacLRtM5Pve//wDf9I0vw1a/Rzcvh+GW07jgxItiicFwhIuLERZLOnB2t7fR39pCq9OGlhrNXDvS1PCGaG0CElJqLkKygL5j8R5PKaaQmvA44bgNmtNBD4TUgAp2EoCEMwQVOGtYR18DXoNkGSuE7lfHVX/PURrpatAaa62xvb2NrN1Cp9PG8GKI2WSGi+EIy+UcvW4P7Xab2o5TUoATQtOYJmNgmFGzKBeYzqeQUJhO53jm5ARFZeAiDiy40Nh871oCd13dwx4XzUj2nxXghASFAeHeXeGKcx3D+lDxp2sE7wEHj9p6FJXHcDTBVncAr1M4naDX6yEfjjBfLjEcjbHVyiBUDukcvJAwTiBLW9CKZDkrZ2FtRdCbMwA0vw/SCzHMlrHWwrHesPfEGPJM4RROwgs6UIw1kALodrpYLCrS4BAKDqS7sVgWGI3HSPMUUnmU5QJCKCidx8aQUC+QXqC2NSTQ6E6AAhfrLYQEdKIhVYiKefgcwxjhgE+EahqUIKGEgmMVPBkO0Bc4br1tI+HPf/7z+MZv/MZb/TY2trGN3Wb21FNP4Y477njBnu+2jYR3WZnsxo0b2NrausXv5va1wMd+6qmnXtAUbWPrtlnnr559qbX23mM6neLKlSsv6Ovdtk449IpvbW1tbtqvgvX7/c06fxVss85fPXu+tX4xArr/JynLjW1sYxvb2P+bbZzwxja2sY3dQrttnXCWZfgH/+AfPG8r88ZeONus81fHNuv81bOv9lrftuyIjW1sYxv7WrDbNhLe2MY2trGvBds44Y1tbGMbu4W2ccIb29jGNnYLbeOEN7axjW3sFtrGCW9sYxvb2C2029IJ//zP/zzuuusu5HmO1772tc+Z5LGxdfvIRz6CP/fn/hyuXLkCIQR+7dd+be333nv85E/+JC5fvoxWq4V7770Xn/vc59auGQwGeOMb34h+v4/t7W388A//MGaz2do1n/rUp/An/sSfQJ7nuPPOO/FTP/VTL/ZHe0nZO9/5Tnz3d383er0eDg8P8Rf/4l/EE088sXZNURR44IEHsLe3h263ize84Q3PmTJz48YN3H///Wi32zg8PMTb3/52EttfsQ9/+MN41atehSzLcM899+A973nPi/3xXjL2i7/4i/j2b//22PF2/fp1/MZv/Eb8/Utujf1tZg8++KBP09T/23/7b/1nPvMZ/9f+2l/z29vb/uTk5Fa/tZesvf/97/d/9+/+Xf+f/tN/8gD8e9/73rXfv+td7/JbW1v+137t1/zv/d7v+T//5/+8v/vuu/1yuYzX/Ok//af9d3zHd/iPfexj/r/9t//m77nnHv/93//98ffj8dgfHR35N77xjf7xxx/3v/Irv+JbrZb/pV/6pa/Wx7zldt999/l3v/vd/vHHH/ePPfaY/7N/9s/6a9eu+dlsFq9585vf7O+8807/gQ98wP/u7/6uf93rXue/53u+J/7eGOO/9Vu/1d97773+k5/8pH//+9/v9/f3/U/8xE/Eaz7/+c/7drvt3/a2t/nPfvaz/md/9me9Usr/5m/+5lf1894q+y//5b/4973vff5//a//5Z944gn/d/7O3/FJkvjHH3/ce//SW+Pbzgm/5jWv8Q888ED8u7XWX7lyxb/zne+8he/qa8ee7YSdc/7SpUv+n//zfx4fG41GPssy/yu/8ivee+8/+9nPegD+E5/4RLzmN37jN7wQwj/99NPee+9/4Rd+we/s7PiyLOM173jHO/zLX/7yF/kTvXTt9PTUA/APP/yw957WNUkS/6u/+qvxmt///d/3APwjjzzivacDU0rpj4+P4zW/+Iu/6Pv9flzbH/uxH/Pf8i3fsvZaf+Wv/BV/3333vdgf6SVrOzs7/pd/+Zdfkmt8W8ERVVXh0Ucfxb333hsfk1Li3nvvxSOPPHIL39nXrn3hC1/A8fHx2ppubW3hta99bVzTRx55BNvb2/ijf/SPxmvuvfdeSCnx8Y9/PF7zJ//kn0TKg1cB4L777sMTTzyB4XD4Vfo0Ly0bj8cAGsW/Rx99FHVdr631K17xCly7dm1trb/t274NR0dH8Zr77rsPk8kEn/nMZ+I1q88Rrvl63APWWjz44IOYz+e4fv36S3KNbysnfH5+Dmvt2uIBwNHREY6Pj2/Ru/ratrBuf9iaHh8fx6GtwbTW2N3dXbvm+Z5j9TW+nsw5h7e+9a34Y3/sj8X5jMfHx0jTdG1SOfDctf6/reOXumYymWC5XL4YH+clZ5/+9KfR7XaRZRne/OY3473vfS9e+cpXviTX+LaVstzYxl7K9sADD+Dxxx/HRz/60Vv9Vm5Le/nLX47HHnsM4/EY//E//ke86U1vwsMPP3yr39bz2m0VCe/v70Mp9ZxK58nJCS5dunSL3tXXtoV1+8PW9NKlSzg9PV37veGp0qvXPN9zrL7G14v9yI/8CH79138dH/rQh9YmNFy6dInm3I1Ga9c/e63/b+v4pa7p9/totVov9Md5SVqaprjnnnvw6le/Gu985zvxHd/xHfiZn/mZl+Qa31ZOOE1TvPrVr8YHPvCB+JhzDh/4wAdw/fr1W/jOvnbt7rvvxqVLl9bWdDKZ4OMf/3hc0+vXr2M0GuHRRx+N13zwgx+Ecw6vfe1r4zUf+chHUNd1vOahhx7Cy1/+cuzs7HyVPs2tNe89fuRHfgTvfe978cEPfhB333332u9f/epXI0mStbV+4okncOPGjbW1/vSnP7126D300EPo9/t45StfGa9ZfY5wzdfzHnDOoSzLl+YafwWFxpe0Pfjggz7LMv+e97zHf/azn/V//a//db+9vb1W6dzYuk2nU//JT37Sf/KTn/QA/L/8l//Sf/KTn/RPPvmk954oatvb2/4//+f/7D/1qU/5v/AX/sLzUtS+67u+y3/84x/3H/3oR/03fdM3rVHURqORPzo68j/wAz/gH3/8cf/ggw/6drv9dUVRe8tb3uK3trb8hz/8YX/z5s34Z7FYxGve/OY3+2vXrvkPfvCD/nd/93f99evX/fXr1+PvA33q+77v+/xjjz3mf/M3f9MfHBw8L33q7W9/u//93/99//M///NfVxS1H//xH/cPP/yw/8IXvuA/9alP+R//8R/3Qgj/27/92977l94a33ZO2Hvvf/Znf9Zfu3bNp2nqX/Oa1/iPfexjt/otvaTtQx/6kAfNMV7786Y3vcl7TzS1v//3/74/OjryWZb517/+9f6JJ55Ye46Liwv//d///b7b7fp+v+9/8Ad/0E+n07Vrfu/3fs//8T/+x32WZf7q1av+Xe9611frI74k7PnWGIB/97vfHa9ZLpf+b/yNv+F3dnZ8u932f+kv/SV/8+bNtef5gz/4A/9n/syf8a1Wy+/v7/u//bf/tq/reu2aD33oQ/47v/M7fZqm/hu+4RvWXuN2tx/6oR/yL3vZy3yapv7g4MC//vWvjw7Y+5feGm/0hDe2sY1t7BbabYUJb2xjG9vY15ptnPDGNraxjd1C2zjhjW1sYxu7hbZxwhvb2MY2dgtt44Q3trGNbewW2sYJb2xjG9vYLbSNE97Yxja2sVtoGye8sY1tbGO30DZOeGMb29jGbqFtnPDGNraxjd1C2zjhjW1sYxu7hfb/ASmI8U+zg6RRAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Img[:, :, ::-1])  # Show image\n",
    "plt.show()\n",
    "Img = transformImg(Img)  # Transform to pytorch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T00:27:39.228318Z",
     "end_time": "2023-04-17T00:27:40.783316Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Video-gamer\\Projects\\Segmentation_cnn\\venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Img = torch.autograd.Variable(Img, requires_grad=False).to(device).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    Prd = Net(Img)['out']  # Run net\n",
    "Prd = tf.Resize((height_orgin, widh_orgin))(Prd[0])  # Resize to origninal size\n",
    "seg = torch.argmax(Prd, 0).cpu().detach().numpy()  # Get  prediction classes\n",
    "plt.imshow(seg)  # display image\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T00:27:43.691090Z",
     "end_time": "2023-04-17T00:27:44.978358Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
